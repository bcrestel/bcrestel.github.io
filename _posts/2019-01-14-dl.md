---
layout: post
title: Literature on Deep Learning
tags: ML deeplearning
---

# Regularization

One way Deep Learning avoids over-fitting is by building sparsity.
The former can be achieved with $L_1$ regularization, or with a post hoc
pruning, i.e., removing some neurons from the network after training is
complete. This pruning can be completely random, but most likely will be
targeted provided some sort of metric (magnitude of weights, gradient,...). In
[Targeted Dropout](https://openreview.net/pdf?id=HkghWScuoQ), the authors
observe that dropout also promote sparity (in the sense of small numbers of high
activations; see [here](https://wiki.tum.de/display/lfdv/Dropout#Dropout-EffectonSparsity)), 
and they propose to dropout, with a
higher probability, neurons that would be pruned in the post-training stage.

Dropout typically doesn't work as well for CNN. In [DropBlock: A regularization method for convolutional networks
](http://papers.nips.cc/paper/8271-dropblock-a-regularization-method-for-convolutional-networks),
the authors postulate that this is due to the spatial correlation between
neurons in a CNN, and propose to drop units in a spatially correlated manner
(DropBlock). They report better results.

# Optimization

In [Adaptive Methods for Nonconvex
Optimization](http://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization.pdf),
the authors study convergence properties of scaled gradient-based methods, and
highlight the benefit of gradually increasing the mini-batch size during
training.
