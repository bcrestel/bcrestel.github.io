---
layout: post
title: Intro to pytorch
tags: ML deeplearning pytorch
---

I want to use this post to summarize a few important facts about using pytorch
to build and train deep neural nets.

## Building a neural net
You build a neural net by inheriting the torch class `Module`. 

### Forward
At it most basic, you only need to define the method `forward(self, x)` which
defines the forward propagation of your neural net. Typically, all function(al)s
you need to build your deep net are defined in the constructor, i.e. `def
__init__(self)`.
To stack the layers of your neural net, you propagate the input variable
(typically `x`) from one layer to the next one, and return it in the end.
Torch provides default function(al)s to define each layer (convolutional, rnn,
lstm, pooling, activation functions,...). 
The main reference for all those commands is provided in the pytorch
[documentation](https://pytorch.org/docs/stable/nn.html).

Below is an example that defines the AlexNet,
```
import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
	# 2d convolutional layer that takes 3 input images (3-colors)
	# returns 6 images,
	# and apply a convolution kernel of size 5x5
        self.conv1 = nn.Conv2d(3, 6, 5)
	# max pooling layer which only keeps max value in 2x2 squares (subsampling)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
	# bunch of linear layers (y = Wx + b)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
	# apply convolution kernel,
	# apply pointwise ReLU function
	# and max pooling
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
	# change shape to go from multiple small images,
	# to one long vector
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


net = Net()
```

### Parameters of the network
The parameters are stored in the iterator `net.parameters()`. You can convert
that to a list and see that the length of the list is equal to the number of
layers x 2, since you have parameters for the weights ($W$) and the biases
($b$), and those parameters are stored separately. Typically, the parameters are
ordered, for each layer, as weights first, bias second.


### Loss function
The loss function is defined independently from the neural net. 
I think the separation is motivated by the fact that the loss function does not
have any parameters to be trained.
Pytorch comes with a large variety of loss functions, e.g., the cross-entropy,
```
criterion = nn.CrossEntropyLoss()
```

### An optimizer
Same as for the loss function, you can use one of the many optimizers provided
by pytorch, e.g., stochastic gradient descent with momentum,
```
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
```
`net`, here, is the convolutional neural network we defined above. So you only
need to pass the parameters of the net to the optimizer. But, these parameters
contain also the gradient information (once calculated).

Note that for some optimizers (e.g., BFGS), you need to do something more
complicated (see [here](https://pytorch.org/docs/stable/optim.html)).

## Train the network

### Basic procedure
Once we have defined a neural net, a loss function, and an optimizer, we can
start training the network. To do so, we need (1) training data, (2)
derivatives. You set up the iteration over the training data set. But once you
have a mini-batch, you need to 
* compute the loss
```
outputs = net(data)
loss = criterion(outputs, labels)
```
* calculate the gradient
```
loss.backward()
```
* apply one step of the optimizer:
```
optimizer.step()
```

### Looking at the gradient
You can look at the gradient for each layer, either directly
```
net.conv1.bias.grad
```
or through the parameters (here for the bias of the first layer),
```
ii = 0
for x in net.parameters():
    if ii == 1:
        print(x.grad.data)
    ii += 1
```

## Using CUDA
You need to explicitly transfer your data structures (neural net,...) onto the
GPU, using the `.to()`command. To transfer the neural net, you do
```
gpu0 = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
net.do(gpu0)
```
You also need to transfer the input data onto the GPU if you want to do that,
e.g.,
```
inputs, labels = inputs.to(gpu0), labels.to(gpu0)
```
This is described at the end of the [CIFAR10
tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py).

Another good reference for all CUDA matters is the
[documentation](https://pytorch.org/docs/stable/notes/cuda.html).

A first interesting observation with that CIFAR10 dataset is that for a given
number of epochs, smaller mini-batches (e.g., default of 4) lead to higher
accuracy, but will take longer to train than, for instance with mini-batches of
64 pics. However, since each epoch is faster with larger mini-batches, a more
fair comparision should be for a fixed run time, in which case it seems larger
mini-batches win (for that specific application).
