---
layout: post
title: Bro, do you even backprop?
tags: deeplearning ML optimization calculus
---

The goal of this post is to dive deeper into the derivation of the backpropagation
algorithm. This entry can be seen as a sub-post of that main
<a href="/2018/11/13/deeplearnDIY">post</a>
discussing a Python implementation of an artificial neural network, and all
notations will be exactly the same. For conventions and definitions about
vector/matrix calculus, see that <a href="/2018/11/12/MatrixCalculus">post</a>.

## Derivatives with respect to ${\bf b}^l$

First let's introduce the function $g: \mathbb{R}^{n_L} \rightarrow \mathbb{R}$
defined by $$g(x) = \frac12 \| x \|^2$$, with derivative $\nabla g(x) = x$.
Then the loss function can be written as

$$ L = \frac1N \sum_{i=1}^N g({\bf a}^L({\bf x}_i) - {\bf y}_i) $$

Applying the chain-rule, we get that

$$ \frac{\partial L}{\partial {\bf b}^l} = \frac1N \sum_{i=1}^N
\frac{\partial {\bf a}^L}{\partial {\bf b}^l} \cdotp 
({\bf a}^L({\bf x}_i) - {\bf y}_i) $$

**For the output layer**, i.e., $l=L$, since ${\bf a}^L = {\bf W}^L \cdotp {\bf
a}^{L-1} + {\bf b}^L$, applying the chain-rule again
with $z^L_k$, we have

$$ \frac{\partial {\bf a}^L}{\partial b^L_j} = 
\sum_{k=1}^{n_L} \frac{\partial {\bf a}^L}{\partial z_k^L}
\frac{\partial z_k^L}{\partial b^L_j} $$

Let ${\bf e}_k = [\dots,0,1,0,\dots]^T$ be the vector filled with all zeros
but for one 1 at the $k^\text{th}$ entry. Then, we have

$$ \frac{\partial {\bf a}^L}{\partial z_k^L} = f'(z_k^L) {\bf e}_k,
\quad \frac{\partial z_k^L}{\partial b^L_j} = \delta_{jk} $$

This means

$$ \frac{\partial {\bf a}^L}{\partial b^L_j} = f'(z_j^L) {\bf e}_j $$

Following the convention defined
<a href="/2018/11/12/MatrixCalculus">here</a>, we'll stack these vectors row by
row to get

$$ \frac{\partial {\bf a}^L}{\partial {\bf b}^L} = 
\begin{bmatrix}
f'( z_1^L )  & & 0 \\
  &   \ddots & \\
 0    &  &   f'(z_{n_L}^L)
\end{bmatrix} 
$$

In the end, with $$z_k^L = ({\bf w}^L_k)^T \cdotp {\bf a}^{L-1} + b^L_k$$, we
get

$$ \frac{\partial L}{\partial {\bf b}^L} = \frac1N \sum_{i=1}^N
\begin{bmatrix}
f'( z_1^L )  & & 0 \\
  &   \ddots & \\
 0    &  &   f'(z_{n_L}^L)
\end{bmatrix} \cdotp 
({\bf a}^L({\bf x}_i) - {\bf y}_i) $$

**For an interior layer $l$**, we need to write an induction between successive
layers. But first, let's make a few observations. In the derivatives, the
variables ${\bf z}^l$ and ${\bf b}^l$ are inter-changeable. Indeed, since
$$\frac{\partial {\bf z}^l}{\partial {\bf b}^l} = I_{n_l}$$, we have

$$ \frac{\partial \cdotp}{\partial {\bf b}^l} = 
\frac{\partial {\bf z}^l}{\partial {\bf b}^l}
\frac{\partial \cdotp}{\partial {\bf z}^l} = \frac{\partial \cdotp}{\partial
{\bf z}^l} $$

Next, the result for $$\frac{\partial {\bf a}^L}{\partial {\bf b}^L}$$ 
derived above
can be generalized to any layer $l$, i.e.,

$$ \frac{\partial {\bf a}^l}{\partial {\bf b}^l} = 
\begin{bmatrix}
f'( z_1^l )  & & 0 \\
  &   \ddots & \\
 0    &  &   f'(z_{n_l}^l)
\end{bmatrix} $$

We now apply the chain-rule,

$$ \begin{align*}
\frac{\partial L}{\partial {\bf b}^l} & = \frac{\partial L}{\partial {\bf z}^l} \\
 & = \frac{\partial {\bf a}^l}{\partial {\bf z}^l} 
 \frac{\partial {\bf z}^{l+1}}{\partial {\bf a}^l} 
\frac{\partial L}{\partial {\bf z}^{l+1}} \\
 & = \frac{\partial {\bf a}^l}{\partial {\bf b}^l} 
 \frac{\partial {\bf z}^{l+1}}{\partial {\bf a}^l} 
\frac{\partial L}{\partial {\bf b}^{l+1}} 
\end{align*} $$

And in the end,

$$ \frac{\partial L}{\partial {\bf b}^l} =
\begin{bmatrix}
f'( z_1^l )  & & 0 \\
  &   \ddots & \\
 0    &  &   f'(z_{n_l}^l)
\end{bmatrix} \cdotp
({\bf W}^{l+1})^T \cdotp
\frac{\partial L}{\partial {\bf b}^{l+1}} $$



## Derivatives with respect to ${\bf W}^l$

I am going to re-use the results in the previous section. In fact, this is yet
another application of the chain-rule,

$$ \begin{align*}
\frac{\partial L}{\partial {\bf w}_i^l} & = 
 \frac{\partial {\bf z}^l}{\partial {\bf w}_i^l}  
 \frac{\partial L}{\partial {\bf z}^l}  \\
& = \begin{bmatrix}  \dots & 0 & \bf{a}^{l-1} & 0 & \dots \end{bmatrix} \cdotp
 \frac{\partial L}{\partial {\bf b}^l} \\
 & = {\bf a}^{l-1} \frac{\partial L}{\partial b^l_i} 
\end{align*} $$

Since $${\bf W}^l = [{\bf w}_1^l,\dots,{\bf w}_{n_l}^l]^T$$, we stack all
$$\frac{\partial L}{\partial {\bf w}_i^l}$$ in columns then take the transpose
to get

$$ \frac{\partial L}{\partial {\bf W}^l} =
\frac{\partial L}{\partial {\bf b}^l} \cdotp ({\bf a}^{l-1})^T $$
