---
layout: post
title: A continuation scheme for RELU activation functions
tags: deeplearning relu activation optimization
---

One of the most popular activation function nowadays is the REctified Linear
Unit ([RELU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))), 
defined by $f(x) = \max(0,x)$. One of the first obvious criticism is its
non-differentiability at the origin. A smooth approximation is the 
[softplus](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#Softplus)
function, $f(x) = log(1 + e^x)$. However, the use of softplus is discouraged by
deep learning experts. I'm wondering if a continuation scheme on the softplus
could help during training. Instead of softplus, we could use a "smooth RELU"
defined as

$$ f_\alpha(x) = \frac1{\alpha} log(1 + e^{\alpha x}) $$

And starting with $\alpha = 1$, i.e., the softplus function, we increase
$\alpha$ after each epoch with the effect of smoothly converging toward RELU. In
the plot below, I show RELU, softplus, and 3 examples of smooth RELUs for
$\alpha=2,4,8$.
<img src="/code/2018-11-26/softplus.png" alt="softplus" height="350" width="500"/>
