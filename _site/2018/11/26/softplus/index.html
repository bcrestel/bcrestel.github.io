<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      A continuation scheme for RELU activation functions &middot; Fourre-tout
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Mathjax -->
   <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
   <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       tex2jax: {
         inlineMath: [ ['$','$'], ["\\(","\\)"] ],
         displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
         processEscapes: true
       }
     });
   </script>
   <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

   <!-- tags -->

  









</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="https://bcrestel.github.io">
          Fourre-tout
        </a>
      </h1>
      <p class="lead">Some notes, some thoughts, some ideas</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="https://bcrestel.github.io">Home</a>

      

      
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/archive/">Archive</a>
          
        
      
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/tags/">Tags</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      


    <a href="mailto:ben.crestel@zoho.com"><i class="fa fa-envelope"></i></a>
    <a href="https://linkedin.com/in/bcrestel"><i class="fa fa-linkedin"></i></a>
    <a href="https://github.com/bcrestel"><i class="fa fa-github"></i></a>
    </nav>

    <p>&copy; 2021. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">A continuation scheme for RELU activation functions</h1>
  <span class="post-date">26 Nov 2018</span>
  <p>One of the most popular activation function nowadays is the REctified Linear
Unit (<a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">RELU</a>), 
defined by $f(x) = \max(0,x)$. One of the first obvious criticism is its
non-differentiability at the origin. A smooth approximation is the 
<a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#Softplus">softplus</a>
function, $f(x) = log(1 + e^x)$. However, the use of softplus is discouraged by
deep learning experts. I’m wondering if a continuation scheme on the softplus
could help during training. Instead of softplus, we could use a “smooth RELU”
defined as</p>

\[f_\alpha(x) = \frac1{\alpha} log(1 + e^{\alpha x})\]

<p>And starting with $\alpha = 1$, i.e., the softplus function, we increase
$\alpha$ after each epoch with the effect of smoothly converging toward RELU. In
the plot below, I show RELU, softplus, and 3 examples of smooth RELUs for
$\alpha=2,4,8$.
<img src="/code/2018-11-26/softplus.png" alt="softplus" height="350" width="500" /></p>

<span>[
  
    
    <a href="/tag/deeplearning"><code class="highligher-rouge"><nobr>deeplearning</nobr></code>&nbsp;</a>
  
    
    <a href="/tag/relu"><code class="highligher-rouge"><nobr>relu</nobr></code>&nbsp;</a>
  
    
    <a href="/tag/activation"><code class="highligher-rouge"><nobr>activation</nobr></code>&nbsp;</a>
  
    
    <a href="/tag/optimization"><code class="highligher-rouge"><nobr>optimization</nobr></code>&nbsp;</a>
  
]</span>
</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2021/02/13/Sequence_Models_by_deeplearning.ai_week_1/">
            Sequence Models by deeplearning.ai
            <small>13 Feb 2021</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2021/02/08/Notes_for_CNN_course_by_deeplearning.ai/">
            Notes for CNN course by deeplearning.ai (week 4)
            <small>08 Feb 2021</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2021/02/02/Notes_for_CNN_course_by_deeplearning.ai/">
            Notes for CNN course by deeplearning.ai (week 3)
            <small>02 Feb 2021</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>

  </body>
</html>
