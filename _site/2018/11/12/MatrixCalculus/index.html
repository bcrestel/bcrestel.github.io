<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Conventions for Matrix Calculus &middot; Fourre-tout
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- Mathjax -->
   <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
   <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       tex2jax: {
         inlineMath: [ ['$','$'], ["\\(","\\)"] ],
         displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
         processEscapes: true
       }
     });
   </script>
   <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

   <!-- tags -->

  









</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="https://bcrestel.github.io">
          Fourre-tout
        </a>
      </h1>
      <p class="lead">Some notes, some thoughts, some ideas</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="https://bcrestel.github.io">Home</a>

      

      
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/archive/">Archive</a>
          
        
      
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/tags/">Tags</a>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      


    <a href="mailto:ben.crestel@zoho.com"><i class="fa fa-envelope"></i></a>
    <a href="https://linkedin.com/in/bcrestel"><i class="fa fa-linkedin"></i></a>
    <a href="https://github.com/bcrestel"><i class="fa fa-github"></i></a>
    </nav>

    <p>&copy; 2021. All rights reserved.</p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Conventions for Matrix Calculus</h1>
  <span class="post-date">12 Nov 2018</span>
  <h2 id="notes">Notes</h2>

<p>In optimization, you need to take derivatives. To do that in $\mathbb{R}^n$, you
need matrix calculus. The objective of this note is to summarize the important
definitions and conventions, explain them whenever possible, and show a few
examples.</p>

<h4 id="frechet-and-gateaux-derivatives">Frechet and Gateaux derivatives</h4>

<p>Let’s define a function $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$.  That
function is 
<a href="https://en.wikipedia.org/wiki/Fr%C3%A9chet_derivative">Frechet differentiable</a> 
at $x$ if
there exists a bounded (necessarily the case for an operator between finite dimensional
spaces) linear operator<br />
$Df(x): \mathbb{R}^n \rightarrow \mathbb{R}^m$ 
such that
\[ \frac{| f(x+h) - f(x) - Df(x)h |}{| h |} \rightarrow 0 \] 
as $|h| \rightarrow 0$.</p>

<p>A somehow weaker definition of differentiability is the <a href="https://en.wikipedia.org/wiki/G%C3%A2teaux_derivative">Gateaux
differentiability</a>.  A
function $f$ is Gateaux differentiable at $x$ if, for any $v
\in \mathbb{R}^n$ the following limit exists
\[ lim_{\varepsilon \rightarrow 0} \frac{f(x + \varepsilon v) - f(x)} \varepsilon . \]
If that limit exists, we can calculate it as
\[ lim_{\varepsilon \rightarrow 0} \frac{f(x + \varepsilon v) -
f(x)}\varepsilon = \frac{d}{d\varepsilon} f(x + \varepsilon v)
|_{\varepsilon=0} . \]</p>

<p>If $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ 
is Frechet differentiable, it is necessarily Gateaux differentiable, and
\(\frac{d}{d \varepsilon} f(x + \varepsilon v) |_{\varepsilon=0} = Df(x) v\).</p>

<h4 id="gradient-of-a-functional">Gradient of a functional</h4>

<p>Let’s call $H = \mathbb{R}^n$, which is a Hilbert space with inner product
$\langle \cdotp, \cdotp \rangle$.  For a functional $f: \mathbb{R}^n \rightarrow
\mathbb{R}$, the derivative $Df(x)$ is, by definition, an element of the dual
space $H^*$.
Applying <a href="https://en.wikipedia.org/wiki/Riesz_representation_theorem">Riesz representation
theorem</a>, we know
there is an element $g_x \in H$ such that for any $v \in H$,
\(DF(x) v = \langle g_x, v \rangle\). That element $g_x$ is the gradient of
the functional $f$. This clearly defines the gradient of a functional, 
without having to agree on notations or conventions.</p>

<h4 id="general-case">General case</h4>

<p>What can we do for a function $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$?
First, we can’t apply Riesz representation theorem. Also, it is not clear how we
optimize that function $f$. We’d need to define a <a href="https://en.wikipedia.org/wiki/Total_order#Orders_on_the_Cartesian_product_of_totally_ordered_sets">total
order</a>
on $\mathbb{R}^m$ that would coincide with the objective of optimization.  For
that reason, I see the definition of a gradient in that case as more of a
convention.  There are really two conventions, which are a transpose of each
other (see <a href="https://en.wikipedia.org/wiki/Matrix_calculus#Layout_conventions">layout
conventions</a>),
and I adopt the convention used in Nocedal &amp; Wright’s Numerical Optimization
textbook (section A.2 Derivatives).
Linear maps between two finite-dimensional spaces can
all be described by the action of a matrix.
Nocedal &amp; Wright call the Jacobian the matrix 
$J(x) \in \mathbb{R}^{m \times n}$ 
that verifies, for any $v \in \mathbb{R}^n$, $Df(x)v = J(x) \cdotp v$.
The gradient is defined to be the transpose,</p>

\[\begin{align} 
J(x) &amp; = \left[ \frac{\partial f_i}{\partial x_j} \right]_{ij} 
\in \mathbb{R}^{m \times n} \\
\nabla f(x) &amp; = \left[ \frac{\partial f_j}{\partial x_i} \right]_{ij} = J(x)^T
= \begin{bmatrix}
\frac{\partial f_1}{\partial x_1} &amp; \dots &amp; \frac{\partial f_m}{\partial x_1} \\
\vdots &amp; &amp; \vdots \\
\frac{\partial f_1}{\partial x_n} &amp; \dots &amp; \frac{\partial f_m}{\partial x_n} 
\end{bmatrix}
\in \mathbb{R}^{n \times m} 
\end{align}\]

<p>One thing to be careful about with that notation, 
the chain-rule, as we know it, applies to the Jacobian, i.e., if $h(x) =
f(g(x))$, then</p>

\[J_h(x) = J_f(g(x)) \cdotp J_g(x)\]

<p>and therefore in terms of the gradient, we get the transpose,</p>

\[\nabla h(x) = \nabla g(x) \cdotp \nabla f(g(x))\]

<p>For instance, let’s assume $f: \mathbb{R}^m \rightarrow \mathbb{R}$ and $g: \mathbb{R}^n
\rightarrow \mathbb{R}^m$. Then, with $y_k = (g(x))_k$, for any $i=1,\dots,n$,</p>

\[\frac{\partial h}{\partial x_i} = \sum_{k=1}^m \frac{\partial f}{\partial
y_k} \frac{\partial y_k}{\partial x_i} 
= \left [\frac{\partial (g(x))_i}{\partial x_i} \right]_i^T \cdotp \nabla f(g(x))\]

<p>Then putting all indices $i$ together (in rows), we get the expression above for the
gradient.</p>

<h4 id="derivative-with-respect-to-a-matrix">Derivative with respect to a matrix</h4>

<p>In that case also, this is just a convenient notation. For a function $f :
\mathbb{R}^{m \times n} \rightarrow \mathbb{R}$, we define</p>

\[\frac{\partial f(M)}{\partial M} = \left[
\frac{\partial f(M)}{\partial m_{ij}} \right]_{ij}\]

<h2 id="examples">Examples</h2>

<h4 id="if-fx--ax--b">If $f(x) = Ax + b$</h4>

<p>We then have $f : \mathbb{R}^n \rightarrow \mathbb{R}^m$.
We can apply the definition of the Gateaux derivative,</p>

\[f(x + \varepsilon v) = f(x) + \varepsilon A v\]

<p>We can conlude directly that</p>

\[\nabla f(x) = A^T\]

<h4 id="if-fx--frac12-xt-q-x-">If $f(x) = \frac12 x^T Q x $</h4>

<p>We then have $f : \mathbb{R}^n \rightarrow \mathbb{R}$.
We can apply the definition of the Gateaux derivative,</p>

\[f(x + \varepsilon v) = f(x) + \frac12 \varepsilon 
\left( x^T Q v + v^T Q x \right) + 
\frac12 \varepsilon^2 v^T Q v\]

<p>We conlude that</p>

\[\nabla f(x) = \frac12 ( Q + Q^T) x\]

<p>In the special case that $Q=Q^T$ (symmetric), we have</p>

\[\nabla f(x) =  Q x\]

<h4 id="if-fx--frac12--axb2">If $f(x) = \frac12 | Ax+b|^2$</h4>

<p>We then have $f : \mathbb{R}^n \rightarrow \mathbb{R}$.
Following the same approach, we get</p>

\[f(x + \varepsilon v) = f(x) + 
\frac12 \varepsilon \left( (Ax+b)^T A v + (Av)^T(Ax+b) \right) + 
\frac12 \varepsilon^2 \|Av\|^2\]

<p>We can conlude that</p>

\[\nabla f(x) = A^T \cdotp (Ax+b)\]

<p>Alternatively, we can use the chain-rule with $g(y) = \frac12 | y |^2$ and
$f(x) = g(Ax + b)$. 
Since $\nabla (Ax+b) = A^T$ and $\nabla g(y) = y$, we have</p>

\[\nabla f(x) = A^T \cdotp (Ax+b)\]

<h4 id="if-fa--axb">If $f(A) = Ax+b$</h4>

<p>In that case, I’m not sure it helps to talk about a gradient. However we can
still calculate the derivative (e.g., using the formula for the Gateaux
derivative), and we get</p>

\[Df(A) M = M \cdotp x\]

<h4 id="if-fa--frac12--axb2">If $f(A) = \frac12 | Ax+b|^2$</h4>

<p>Here we have $f: \mathbb{R}^{m \times n} \rightarrow \mathbb{R}$.
It’s tempting to use the chain-rule, but I couldn’t agree with myself on a
logical convention. And I think sadly this is the main conclusion of that small
post: it’s safer to always rely on the entry-wise derivative. In this case,
with $y = Ax + b \in \mathbb{R}^m$, we have</p>

\[\begin{align*}
\frac{\partial f(A)}{\partial a_{ij}} &amp; = 
\sum_k \frac{\partial g(Ax + b)}{\partial y_k} \frac{\partial y_k}{\partial
a_{ij}} \\
&amp; = \left[ \frac{\partial y_1}{\partial a_{ij}}, \dots, \frac{\partial y_m}{\partial
a_{ij}} \right] \cdotp (Ax + b)
\end{align*}\]

<p>Let’s look at the partial derivatives for $y$, using the notation $\delta_{ik} =
1$ if $i=k$ and $0$ otherwise,</p>

\[\frac{\partial y_k}{\partial a_{ij}} = x_j \delta_{ik}\]

<p>Such that</p>

\[\frac{\partial f(A)}{\partial a_{ij}}  = (Ax+b)_i x_j\]

<p>And putting all indices back together,</p>

\[\frac{\partial f(A)}{\partial A}  = (Ax+b) \cdotp x^T\]


<span>[
  
    
    <a href="/tag/optimization"><code class="highligher-rouge"><nobr>optimization</nobr></code>&nbsp;</a>
  
    
    <a href="/tag/calculus"><code class="highligher-rouge"><nobr>calculus</nobr></code>&nbsp;</a>
  
]</span>
</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2021/02/13/Sequence_Models_by_deeplearning.ai_week_1/">
            Sequence Models by deeplearning.ai
            <small>13 Feb 2021</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2021/02/08/Notes_for_CNN_course_by_deeplearning.ai/">
            Notes for CNN course by deeplearning.ai (week 4)
            <small>08 Feb 2021</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2021/02/02/Notes_for_CNN_course_by_deeplearning.ai/">
            Notes for CNN course by deeplearning.ai (week 3)
            <small>02 Feb 2021</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>

  </body>
</html>
