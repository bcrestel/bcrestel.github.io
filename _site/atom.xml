<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Fourre-tout</title>
 <link href="https://bcrestel.github.io/atom.xml" rel="self"/>
 <link href="https://bcrestel.github.io/"/>
 <updated>2021-02-17T14:52:51-05:00</updated>
 <id>https://bcrestel.github.io</id>
 <author>
   <name>Ben C</name>
   <email></email>
 </author>

 
 <entry>
   <title>Sequence Models by deeplearning.ai</title>
   <link href="https://bcrestel.github.io/2021/02/13/Sequence_Models_by_deeplearning.ai_week_1/"/>
   <updated>2021-02-13T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2021/02/13/Sequence_Models_by_deeplearning.ai_week_1</id>
   <content type="html">&lt;h1 id=&quot;week-1&quot;&gt;Week 1&lt;/h1&gt;

&lt;h2 id=&quot;video-notation&quot;&gt;Video: Notation&lt;/h2&gt;

&lt;p&gt;\(x^{&amp;lt;t&amp;gt;}\): \(t^\text{th}\) entry in sequence $x$&lt;/p&gt;

&lt;p&gt;$T_x$: length of sequence $x$&lt;/p&gt;

&lt;p&gt;\(x^{(i)&amp;lt;t&amp;gt;}\): entry $t$ in training example $i$.&lt;/p&gt;

&lt;p&gt;Vocabulary, Dictionary: ordered list of all words used (10,000-1M).
Then each word in that vocabulary can be encoded via one-hot encoding.
For unknown words, you can have an “unknown” token.&lt;/p&gt;

&lt;h2 id=&quot;video-rnn-model&quot;&gt;Video: RNN Model&lt;/h2&gt;

&lt;p&gt;Activation which is passed from one step to the next is called $a$.
Typically set \(a^{&amp;lt;0&amp;gt;}=0\), then
\(a^{&amp;lt;t&amp;gt;} = g_1(W_{aa} a^{&amp;lt;t-1&amp;gt;} + W_{ax} x^{&amp;lt;t&amp;gt;} + b_a)\). 
Activation function
typically tanh/ReLU.
Notation often condensed by stacking the matrices such that $W_a = [W_{aa},
W_{ax}]$.&lt;/p&gt;

&lt;p&gt;Output of RNN layer at step t is called $y$.
\(y^{&amp;lt;t&amp;gt;} = g_2(W_{ya} a^{&amp;lt;t&amp;gt;} + b_y)\). Activation function typically sigmoid.&lt;/p&gt;

&lt;p&gt;Important details is that the weights $W_a, W_y$ are shared across all time
steps.&lt;/p&gt;

&lt;h2 id=&quot;video-backpropagation-through-time&quot;&gt;Video: Backpropagation through time&lt;/h2&gt;

&lt;p&gt;Don’t give much details, simply that the loss is the sum of an elementary loss
at each time step. Something that can be denoted by
\(L(y, \hat{y}) = \sum_{t=1}^{T_y} L(y^{&amp;lt;t&amp;gt;}, \hat{y}^{&amp;lt;t&amp;gt;})\).
When you back-propagate, you back propagate through time, ie, “from right to
left”. Again, keep in mind that the weights are shared across all time steps.&lt;/p&gt;

&lt;h2 id=&quot;video-different-types-of-rnns&quot;&gt;Video: Different types of RNNs&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;one-to-one: not really an RNN; it’s just a fc layer&lt;/li&gt;
  &lt;li&gt;one-to-many: only have an input for the first step. After that first step, \(x^{&amp;lt;t&amp;gt;}\) is 
replaced by the output generated in the previous step \(y^{&amp;lt;t-1&amp;gt;}\) (like when
DeepAR is predicting). This is the case of music generation, for example.&lt;/li&gt;
  &lt;li&gt;many-to-one: You have multiple intputs, but only one output you care about.
In that case, you only care about the last output.
This is the case of sentiment analysis, for example.&lt;/li&gt;
  &lt;li&gt;many-to-many ($T_x=T_y$): standard case where each input gives an output, at
each step. There is a potential limitation that the output only depends on the
previous steps; but this can be remedied with bidirectionnal RNNs (or LSTMs).&lt;/li&gt;
  &lt;li&gt;many-to-many ($T_x \neq T_y$): in that case, you want to use an
 encoder-decoder architecture. That is, you first pass all of the inputs
($T_x$), without using the outputs. Then after passing the input, you start
generating the output (most likely feeding each output as an input at the next
step).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This list does not include attention, which is covered in week 3.&lt;/p&gt;

&lt;h2 id=&quot;video-language-model-and-sequence-generation&quot;&gt;Video: Language model and sequence generation&lt;/h2&gt;

&lt;p&gt;Language model returns the likelihood of a sentence; this can used to decide
what was most likely said for instance (by comparing likelihood of 2 sentences).&lt;/p&gt;

&lt;p&gt;To do that, you can formulate it as a many-to-many RNN, where the output of
the RNN is passed through a softmax and is trained to predict the probability of
the next word, given the first few words in the sentence, \(P(y^{&amp;lt;t&amp;gt;} |
x^{&amp;lt;1&amp;gt;},...)\). Because you now have a lag, you start with the first input being
0, ie, \(x^{&amp;lt;0&amp;gt;}=0\).&lt;/p&gt;

&lt;p&gt;You define the loss at each time step to be \(-\sum_i y_i log(\hat{y}_i)\)
(cross-entropy loss), with
$y_i$ being non-zero (ie, 1) only for the true word. Then for the overall loss you
sum over all time steps. That is equivalent to the log-likelihood of the
probability of that sentence, as you recursively condition on the previous works
in the sentence. That is
\(log P(x_1, x_2, x_3) = log P(x_3|x_1,x_2) + log P(x_2 | x_1) + log P(x_1)\).&lt;/p&gt;

&lt;h2 id=&quot;video-sampling-novel-sequences&quot;&gt;Video: Sampling novel sequences&lt;/h2&gt;

&lt;p&gt;Sample from a trained language model (see previous video). You sample by
recursively sampling a word from your language model, then plugging it back into
your model as an input for the following step.&lt;/p&gt;

&lt;h2 id=&quot;video-vanishing-gradients-with-rnns&quot;&gt;Video: Vanishing gradients with RNNs&lt;/h2&gt;

&lt;p&gt;Deep neural networks are hard to train as they lead to vanishing/exploding
gradients. RNNs are no different; long sequences will make it resemble a deep
neural networks. If vanishing gradients can be hard to spot, exploding gradients
will lead to extremely large parameters (potentially NaN) and can be remedied by
gradient clipping (re-scaling the gradient so that its norm does not exceed a
fixed threshold).&lt;/p&gt;

&lt;h2 id=&quot;video-gated-recurrent-unit-gru&quot;&gt;Video: Gated Recurrent Unit (GRU)&lt;/h2&gt;

&lt;p&gt;The main idea is to introduce a memory cell $c$ to try and keep information for
longer number of steps.
In GRU, $c$ directly replaces the activation $a$. It is updated in each cell by
taking a convex combination of its previous value and a tentative update
$\tilde{c}^t$,&lt;/p&gt;

&lt;p&gt;\(c^t = \Gamma_u * \tilde{c}^t + (1-\Gamma_u) * c^{t-1}\),
where $\Gamma_u$ is a gate value between 0 and 1, and the multiplications are
point-wise.  It is given by
\(\Gamma_u = \sigma(W_u [c^{t-1}, x^t] + b_u)\) where $\sigma$ is the sigmoid
function. The tentative update is given by&lt;/p&gt;

&lt;p&gt;\(\tilde{c}^t = tanh(W_c [\Gamma_r * c^{t-1}, x^t] + b_c)\), with $\Gamma_r$ a
relevance gate 
\(\Gamma_r = \sigma(W_r [c^{t-1}, x^t] + b_r)\).&lt;/p&gt;

&lt;h2 id=&quot;video-long-short-term-memory-lstm&quot;&gt;Video: Long Short Term Memory (LSTM)&lt;/h2&gt;

&lt;p&gt;It is slightly different from the GRU with 3 gates (update, forget, output) instead of 2, and the
relevance gate is replaced by direclty using the previous output.&lt;/p&gt;

&lt;p&gt;\(\Gamma_{u,f,o} = \sigma(W_{u,f,o} [a^{t-1}, x^t] + b_{u,f,o})\). 
Notice that
instead of $c^{t-1}$ in the gates, we now use $a^{t-1}$.&lt;/p&gt;

&lt;p&gt;Now the udpate becomes&lt;/p&gt;

&lt;p&gt;\(c^t = \Gamma_u * \tilde{c}^t + \Gamma_f * c^{t-1}\). 
The $\tilde{c}$ is calcultaed in terms of $a^{t-1}$ instead of $c^{t-1}$,&lt;/p&gt;

&lt;p&gt;\(\tilde{c}^t = tanh(W_c [a^{t-1}, x^t] + b_c)\).
And the output is&lt;/p&gt;

&lt;p&gt;\(a^t = \Gamma_o * tanh(c^t)\).&lt;/p&gt;

&lt;p&gt;A really good reference for GRU and LSTM is &lt;a href=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;Colah’s blog post on
LSTM&lt;/a&gt;. There, he
uses
the more typical notation for LSTM and calls $h^t$ the output (instead of $a^t$).&lt;/p&gt;

&lt;h2 id=&quot;video-bidirectional-rnn&quot;&gt;Video: Bidirectional RNN&lt;/h2&gt;

&lt;p&gt;Unroll input forward and backward (not backprop though). Then concatenate output
from both directions and pass to linear layer to generate output of each step.&lt;/p&gt;

&lt;p&gt;Can also have Bidirectionnal LSTM or GRU. Actually BLSTM is very popular for
many NLP tasks.&lt;/p&gt;

&lt;p&gt;Disadvantage: you need entire sequence before making a prediction. This can be
a problem for certain applications (eg, speech recognition).&lt;/p&gt;

&lt;h2 id=&quot;video-deep-rnns&quot;&gt;Video: Deep RNNs&lt;/h2&gt;

&lt;p&gt;You can stack layers of RNNs/LSTMs/GRUs on top of each other.
For each layer, the input stacks its activation from the previous step with the
output from the layer below at that same time steps.&lt;/p&gt;

&lt;p&gt;For RNNs, 3 layers is already quite a lot; so not very deep RNNs networks.
But sometimes, you find a bunch of RNN layers, then on top seats a fc deep
network.&lt;/p&gt;

&lt;h1 id=&quot;week-2&quot;&gt;Week 2&lt;/h1&gt;

</content>
 </entry>
 
 <entry>
   <title>Notes for CNN course by deeplearning.ai (week 4)</title>
   <link href="https://bcrestel.github.io/2021/02/08/Notes_for_CNN_course_by_deeplearning.ai/"/>
   <updated>2021-02-08T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2021/02/08/Notes_for_CNN_course_by_deeplearning.ai</id>
   <content type="html">&lt;p&gt;This week focuses on Face Recognition and Neural Style Transfer.&lt;/p&gt;

&lt;h2 id=&quot;video-what-is-face-recognition&quot;&gt;Video: What is face recognition?&lt;/h2&gt;

&lt;p&gt;Verification vs Recognition?
Verification means you have a tuple (image, name) and you must return whether
they correspond to the same person or not.&lt;/p&gt;

&lt;p&gt;Recognition means you have K persons in your database and given an input image
you try to detect whether that image belongs to one of the K persons in your
database.&lt;/p&gt;

&lt;p&gt;You actually need very high accurate verification systems to be able to do
recognition systems. As you are basically running a recognition K times. So you
have K more chances to make an error. This is why recognition is typically
considered a much harder problem.&lt;/p&gt;

&lt;p&gt;Also, face recognition is one-shot learning problem. You typically have a single
example of each face, and you want to train a face recognition algorithm from
that.&lt;/p&gt;

&lt;h2 id=&quot;video-one-shot-learning&quot;&gt;Video: One Shot Learning&lt;/h2&gt;

&lt;p&gt;The challenge with face recognition is that you have very little samples (1 per
individual). That is a one shot learning problem.&lt;/p&gt;

&lt;p&gt;You cannot solve it as a typical object/face recognition problem. Instead what
you can do is train a network to return a similarity function between a given
input image and each of the individuals you have in your database (+ category
“unknown”). And you can decide of a threshold under which you declare a match
between the input and a person in your db.&lt;/p&gt;

&lt;h2 id=&quot;video-siamese-network&quot;&gt;Video: Siamese Network&lt;/h2&gt;

&lt;p&gt;You can define 2 identical networks (same parameters -&amp;gt; siamese)) that will each take a face image as input,
pass it through a set of conv nets, then fc layers, then output a vector (say,
128-d vector). Then you can define similarity between both faces as
$| f(x_1) - f(x_2)|^2$&lt;/p&gt;

&lt;h2 id=&quot;video-triplet-loss&quot;&gt;Video: Triplet Loss&lt;/h2&gt;

&lt;p&gt;Loss is defined from a triplet of 3 images. Using the notion of margin (support
vector margin), you define a positive float $\alpha$, then you want to have
$| f(A) - f(P)|^2 - | f(A) - f(N)|^2 + \alpha \leq 0$, where A=anchor, P=positive,
N=negative. Actually, since we don’t need this to be extremely negative, we just
want this to less than zero. So to avoid having the network pushing on a corner
case, we take $max(|…. + \alpha, 0)$.&lt;/p&gt;

&lt;p&gt;To train that system, you need to have multiple faces per person (so not really
one shot….). Then when selecting the triplet, you want to select triplet such
that the 2 norm diff are actually close to each other. So that it forces the
network to learn something.
That is to say, you don’t pick A, P, N at random.&lt;/p&gt;

&lt;h2 id=&quot;video-face-verification-and-binary-classification&quot;&gt;Video: Face Verification and Binary Classification&lt;/h2&gt;

&lt;p&gt;Face recognition can also be posed as a binary classification problem, by taking
the same idea of 2 siamese networks, then passing both 128-d output vectors to a
logistic regression unit (maybe passing the absolute value of the entry-wise
difference between both 128-d vectors). There are also variants.&lt;/p&gt;

&lt;p&gt;Notice that you can pre-compute the encodings for all people in your database,
so that you only have to compute the encodings for the face you are trying to
recognize (at inference time).
This will speed up computation time.&lt;/p&gt;

&lt;h2 id=&quot;video-what-is-neural-style-transfer&quot;&gt;Video: What is neural style transfer?&lt;/h2&gt;

&lt;p&gt;To implement style transfer, you need to understand (and use) the features
extracted by the different layers of a convnet. But for that, you need to
understand what is being extracted.&lt;/p&gt;

&lt;h2 id=&quot;video-what-are-deep-convnets-learning&quot;&gt;Video: What are deep ConvNets learning?&lt;/h2&gt;

&lt;p&gt;Typically, layers detect from coarser to finer details as you go deeper into the
convnet.&lt;/p&gt;

&lt;h2 id=&quot;video-cost-function&quot;&gt;Video: Cost Function&lt;/h2&gt;

&lt;p&gt;Cost function is the sum of a content cost function and a style cost function,
both measuring the similarity between the generated image and the content image
or style image.&lt;/p&gt;

&lt;p&gt;The image is generated by applying gradient descent
directly to the generated image, starting from a random image.&lt;/p&gt;

&lt;h2 id=&quot;video-content-cost-function&quot;&gt;Video: Content Cost Function&lt;/h2&gt;

&lt;p&gt;The content is defined as the squared norm of the difference between the activation of both
content (input) image and generated image, at a given layer of a pre-trained
ConvNet. The choice of what layer activation you are using is decided by how
much granularity you want to preserve in the generated image.&lt;/p&gt;

&lt;h2 id=&quot;video-style-cost&quot;&gt;Video: Style Cost&lt;/h2&gt;

&lt;p&gt;As for the content cost function, you pick a layer from your ConvNet, then work
with the activation function of that layer.&lt;/p&gt;

&lt;p&gt;The style of an image is defined via its Style matrix, which is an $n_c \times
n_c$ matrix where each entry is the point-wise multiplication of all activations
of each channel. In the video, they call that the correlation between both
channel; that’d be true if the channels all had zero means.
\(G_{k,k&apos;} = \sum_i \sum_j a_{ijk} a_{ijk&apos;}\).&lt;/p&gt;

&lt;p&gt;You calculate the style matrix for the style image and the generated image. The
style cost function is defined as the squared Frobenius norm of the difference
between both style matrices.
Except that there is a normalization constant in front (but there is another
multiplicative factor in front of the cost function).&lt;/p&gt;

&lt;p&gt;Actually, the overall style cost function is a weighted sum of the style cost
function of each individual layer.&lt;/p&gt;

&lt;p&gt;The overall cost function is the weighted sum of the content cost function and
the style cost function. Then you minimize your cost function (gradient descent,
or other optimizer) by changing the generated image.&lt;/p&gt;

&lt;h2 id=&quot;video-1d-and-3d-generalizations&quot;&gt;Video: 1D and 3D Generalizations&lt;/h2&gt;

&lt;p&gt;Show how convolutional layers can be used on 1d and 3d data, not just 2d data.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes for CNN course by deeplearning.ai (week 3)</title>
   <link href="https://bcrestel.github.io/2021/02/02/Notes_for_CNN_course_by_deeplearning.ai/"/>
   <updated>2021-02-02T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2021/02/02/Notes_for_CNN_course_by_deeplearning.ai</id>
   <content type="html">&lt;h2 id=&quot;video-object-localization&quot;&gt;Video: Object Localization&lt;/h2&gt;

&lt;p&gt;Distinguish:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Classification: just tell what is on the picture (typically a single object
 per picture)&lt;/li&gt;
  &lt;li&gt;Classification with localization: tell what it is + say where it is (box
 around object). Typically 1 object/picture&lt;/li&gt;
  &lt;li&gt;Object Localization: Box around all objects in the picture (typically multiple
 objects per picture)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Classification with localization:
in addition to softmax output layer, add another layer for box info: bx, by, bh,
bw = coordinates of center of the box (bx, by) and height/width of the box (bh,
bw).&lt;/p&gt;

&lt;p&gt;Look at problem where object can be: pedestrian, car, motorcycle, or background
(no object). Proposes parametrization:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;pc = 1 if object (1,2,3) or 0 if no object (4)?&lt;/li&gt;
  &lt;li&gt;bx, by, bh, bw&lt;/li&gt;
  &lt;li&gt;c1, c2, c3: 1/0 if an object is there&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What is the loss in that case? Loss will differ whether you have an object or
not. When no object (ie, pc=0), then loss = $\text{dist}(y_1, \hat{y}_h)$. If
there is an object (pc=1), then loss = $\sum_i \text{dist}(y_i,\hat{y}_i)$. For
the distance, we can use different distance depending on the outputs. For
instance, a log like feature loss (? log loss) for c1, c2, c3, a squared error
for the box, and logistic regression loss for pc.&lt;/p&gt;

&lt;p&gt;I honestly don’t like that parametrization. I would rather combine pc and c1,
c2, c3 and train that with a &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html&quot;&gt;cross entropy
loss&lt;/a&gt;
(or softmax + log-likelihood) and use a squared error loss (or whatever) for the
box. In the case there is not object, just set the target box to be the entire
picture (centered at the center).&lt;/p&gt;

&lt;h2 id=&quot;video-landmark-detection&quot;&gt;Video: Landmark Detection&lt;/h2&gt;

&lt;p&gt;Instead of outputing the parameters of a box, you can simply output coordinates
of important parts of an image (called landmark).
Useful in AR, or snapchat features.&lt;/p&gt;

&lt;p&gt;Can modify the output layer to return coordinates of as many landmarks as you
want. Problem is you need a training dataset. So someone has to annotate
pictures with all these landmarks on it.
Also, important for the labels to be consistent across all images (eg, label 1
always for left eye).&lt;/p&gt;

&lt;h2 id=&quot;video-object-detection&quot;&gt;Video: Object detection&lt;/h2&gt;

&lt;p&gt;Object detection means we want to place boxes around potentially multiple
objects on an image.&lt;/p&gt;

&lt;p&gt;Introduces Sliding Windows Detection algorithm. If we try to detect cars on an
image:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Train a ConvNet to detect car from tightly cropped pictures&lt;/li&gt;
  &lt;li&gt;Then given an object where you want to detect objects, slide a window across
the entire image, and each time you slide it, run the small window through your
car detector (step 1)&lt;/li&gt;
  &lt;li&gt;Repeat procedure with larger and larger windows.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If there is a (or multiple) car(s) on the picture, at some point, one of the
windows should have isolated that car and the detector should have detected it.&lt;/p&gt;

&lt;p&gt;Disadvantage of sliding windows detection: computational cost! Uses many, many
windows. Could use coarser strides (when shifting windows), but that will reduce
efficiency of the localization algorithm.&lt;/p&gt;

&lt;p&gt;That technique worked when we had cheap detector. With ConvNet, this doesn’t
work anymore. But there is a solution (next video)&lt;/p&gt;

&lt;h2 id=&quot;video-convolutional-implementation-of-sliding-windows&quot;&gt;Video: Convolutional Implementation of Sliding Windows&lt;/h2&gt;

&lt;p&gt;Reference: &lt;a href=&quot;https://arxiv.org/abs/1312.6229&quot;&gt;OverFeat, by Sermanet et al.,
2014&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;When running the sliding window algorithm through a ConvNet, there is a lot of
repeated calculations. You can leverage this by running the entire image through
your ConvNet (instead of the cropped window). The result will be a larger output
and each additional values correspond to running additional cropped windows
through the network.
With that convolutional approach, the stride of the sliding window is given by
the size of the max pooling layer.&lt;/p&gt;

&lt;h2 id=&quot;video-bounding-box-predictions&quot;&gt;Video: Bounding Box Predictions&lt;/h2&gt;

&lt;p&gt;Sliding windows return bounding boxes that are not very accurate, as box could
not be exactly rectangular and/or sliding windows may not overlap exact location
of best box.&lt;/p&gt;

&lt;p&gt;Solution: &lt;a href=&quot;https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf&quot;&gt;YOLO (You Only Look Once)
algorithm&lt;/a&gt;.
The idea is actually very close to the one introduced for the classification
with localization problem. Except that here you split your image into a grid.
Then in each grid, you return the 8-dimensional vector described earlier: pc,
bx, by, bh, bw, c1, c2, c3. So you run your image through a ConvNet only once
and output a tensor of dimension $N \times N \times 8$, where $N$ is the
dimension of your output grid. For each grid, you try to match the target 8-d
vector. That way, the boxes can be defined much more precisely (no fixed boxes;
instead the network infers the center and dimensions of the box).
Also, because we only use the input image once, the YOLO algorithm is quite fast
and can be used for real-time image detection.&lt;/p&gt;

&lt;p&gt;The box is encoded relatively to each individual sub-grid. The position must be
between 0 and 1 (sometimes passed through a sigmoid), but height and width can
be greater than 1 (sometimes passed through an exponential to force
non-negativity).&lt;/p&gt;

&lt;h2 id=&quot;video-intersection-over-union&quot;&gt;Video: Intersection Over Union&lt;/h2&gt;

&lt;p&gt;How can you evaluate object localization algorithm? You can use Intersection
Over Union, which is a measure of overlap between 2 boxes. It takes the ratio of
the area of their intersection over the area of their union. That ratio is
between 0 (both boxes don’t intersect at all) and 1 (both boxes are equal).
Often, it is considered that a IoU greater than 0.5 means the box is correct.&lt;/p&gt;

&lt;h2 id=&quot;video-non-max-suppression&quot;&gt;Video: Non-max Suppression&lt;/h2&gt;

&lt;p&gt;Even though each target box only belongs to a single cell in the grid, when
predicting, the network is likely to return multiple boxes corresponding to the
same object. A simple solution is to use the non-max suppression algo. While
there are boxes, pick the one with the highest pc score (in case there is only a
single category detected) and discard all other boxes that have a high IoU score
with that box (eg, greater than 0.5). Then move on to the box with next highest
score, etc…&lt;/p&gt;

&lt;h2 id=&quot;video-anchor-boxes&quot;&gt;Video: Anchor Boxes&lt;/h2&gt;

&lt;p&gt;What can you do if multiple boxes have their center inside the same grid cell?
Even though it doesn’t happen too often (especially with a fine enough grid),
it’s good to have a plan for that.&lt;/p&gt;

&lt;p&gt;The solution propoosed is simply to repeat the output vector for a single object
by the number of objects you want to be able to detect. So for instance, to
allow detection of 2 objects per grid cell, the output will be: pc,
h_x,…,c_1,c_2,c_3, pc, h_x,…,c_3. How do you decide which object is object 1
and object 2?&lt;/p&gt;

&lt;p&gt;You can do that using anchor boxes. You set 2 anchor boxes, for instance 2
rectangular boxes, one vertical and one horizontal. In each grid cell, if an
object has a shape more vertical it is object 1 and if it is more horizontal it
is object 2 (can use IoU to categorize each object)). Then you classify each target object according to that rule, and
you train your object detection network. It will allow the network to specialize
its output to each type of box.&lt;/p&gt;

&lt;h2 id=&quot;video-yolo-algorithm&quot;&gt;Video: YOLO Algorithm&lt;/h2&gt;

&lt;p&gt;The YOLO algorithm puts together all the different components we saw in that
week to do fast object detection.&lt;/p&gt;

&lt;p&gt;Set your target vectors using the box parametrization we introduced before (pc,
hx,…,c1,…) and the idea of anchor boxes. Then you can train your network
using that dataset.
After running prediction, you post-process the results using a non-max
suppression algorithm. And you’re done.&lt;/p&gt;

&lt;h2 id=&quot;region-proposals&quot;&gt;Region Proposals&lt;/h2&gt;

&lt;p&gt;Alternative idea to first propose a small number of regions where an object
could be, then classify each proposed region. The proposed regions come from a
segmentation type algorithm. Different approaches exist, with fastest approach
being formulated as a convnet.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>How to rename a commit</title>
   <link href="https://bcrestel.github.io/2021/02/02/How_to_rename_a_commit/"/>
   <updated>2021-02-02T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2021/02/02/How_to_rename_a_commit</id>
   <content type="html">&lt;p&gt;In all cases, I’m assuming the targeted commit hasn’t been pushed. If that’s the
case, then it’s a different story (and probably a bad idea to do it).&lt;/p&gt;

&lt;h2 id=&quot;if-its-the-latest-commit&quot;&gt;If it’s the latest commit&lt;/h2&gt;

&lt;p&gt;You can simply amend the latest commit by doing&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git commit --amend
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;then change the commit name and save&lt;/p&gt;

&lt;h2 id=&quot;if-its-not-the-latest-commit&quot;&gt;If it’s not the latest commit&lt;/h2&gt;

&lt;p&gt;In that case, you will need to rebase.
Assume you want to change the Nth commit from the top (the latest is the first).
Then interactively rebase the last N commits,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git rebase -i HEAD~N
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will prompt a window that shows the last N commits. From there, you can do
a few things (and a lot of damage, so careful!). But in our case, you just want
to change &lt;code&gt;pick&lt;/code&gt; to &lt;code&gt;reword&lt;/code&gt; in front of the commits that you want to modify.
Then save. It will then open each commit windows for the commits you selected in
the previous step. For each commit, modify the name the way you want it, then
save.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes for CNN course by deeplearning.ai (week 2)</title>
   <link href="https://bcrestel.github.io/2021/01/23/Notes_for_CNN_course_by_deeplearning.ai/"/>
   <updated>2021-01-23T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2021/01/23/Notes_for_CNN_course_by_deeplearning.ai</id>
   <content type="html">&lt;p&gt;These are my notes for &lt;a href=&quot;https://www.coursera.org/learn/convolutional-neural-networks/home/week/2&quot;&gt;week
2&lt;/a&gt;,
focusing on case studies.&lt;/p&gt;

&lt;h2 id=&quot;video-why-look-at-case-studies&quot;&gt;Video: Why look at case studies?&lt;/h2&gt;

&lt;p&gt;Look at classic CNN is a good way to gain intuition about how CNNs work. Also,
in computer vision, successful architecture for one task is often good for
another task.&lt;/p&gt;

&lt;p&gt;Classic CNN examples looked at this week:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;LeNet-5&lt;/li&gt;
  &lt;li&gt;AlexNet&lt;/li&gt;
  &lt;li&gt;VGG&lt;/li&gt;
  &lt;li&gt;ResNet&lt;/li&gt;
  &lt;li&gt;Inception&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;video-classic-networks&quot;&gt;Video: Classic Networks&lt;/h2&gt;

&lt;h3 id=&quot;lenet-5-1998&quot;&gt;&lt;a href=&quot;http://www.iro.umontreal.ca/~lisa/bib/pub_subject/finance/pointeurs/lecun-98.pdf&quot;&gt;LeNet-5 (1998)&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Introduced to recognize black&amp;amp;white hand-written digits (32x32x1). The
&lt;a href=&quot;https://miro.medium.com/max/4308/1*1TI1aGBZ4dybR6__DI9dzA.png&quot;&gt;architecture&lt;/a&gt; is
described below. Note that 
at the time of the paper, padding was not common. So all convolutional layers
shrink the size of the image.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Convolutional layer: 5x5, stride=1, 6 channels -&amp;gt; output: 28x28xg&lt;/li&gt;
  &lt;li&gt;average pooling (today would probably use max pooling): 2x2, stride=2 -&amp;gt;
 output: 14x14x6.&lt;/li&gt;
  &lt;li&gt;Convolutional layer: 5x5, stride=1, 16 channels -&amp;gt; output: 10x10x16&lt;/li&gt;
  &lt;li&gt;average pooling : 2x2, stride=2 -&amp;gt; output: 5x5x16.&lt;/li&gt;
  &lt;li&gt;fully connected layers: 400x120, 120x84, 84x10.&lt;/li&gt;
  &lt;li&gt;output layer today would probably be a softmax. In the paper, they used
 something else that is not common today.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All non-linearities were tanh or sigmoid. Also, they applied activation
functions after the pooling layers.
Total network had about 60,000 parameters, which is small by today’s standard
(&amp;gt;1M parameters).
However variation in image size through the network remains current: height and
width go down, number of channels go up.
Another aspect that remains often true today is the alternance of the layers:
conv, pool, conv, pool, fc, output.
A last note, they had a different way of apply the kernels to the input image
(i.e, didn’t have kernels with the same number of channels as the input).&lt;/p&gt;

&lt;h3 id=&quot;alexnet-2012&quot;&gt;&lt;a href=&quot;https://courses.grainger.illinois.edu/ece544na/fa2013/krizhevsky2012.pdf&quot;&gt;AlexNet (2012)&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;The
&lt;a href=&quot;https://neurohive.io/wp-content/uploads/2018/10/AlexNet-1.png&quot;&gt;architecture&lt;/a&gt; is
described below. The input image is 227x227x3.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;conv layer, 11x11, stride=4, 96 channels -&amp;gt; 55x55x96&lt;/li&gt;
  &lt;li&gt;max pooling, 3x3, stride=2 -&amp;gt; 27x27x96&lt;/li&gt;
  &lt;li&gt;conv layer, 5x5, same padding, 256 channels -&amp;gt; 27x27x256&lt;/li&gt;
  &lt;li&gt;max pooling, 3x3, stride=2 -&amp;gt; 13x13x256&lt;/li&gt;
  &lt;li&gt;conv layer, 3x3, same padding, 384 channels -&amp;gt;13x13x384&lt;/li&gt;
  &lt;li&gt;conv layer, 3x3, same padding, 384 channels -&amp;gt;13x13x384&lt;/li&gt;
  &lt;li&gt;conv layer, 3x3, same padding, 256 channels -&amp;gt;13x13x256&lt;/li&gt;
  &lt;li&gt;max pooling, 3x3, stride=2 -&amp;gt; 6x6x256&lt;/li&gt;
  &lt;li&gt;conv layer, 3x3, same padding, 384 channels -&amp;gt;13x13x384&lt;/li&gt;
  &lt;li&gt;conv layer, 3x3, same padding, 384 channels -&amp;gt;13x13x384&lt;/li&gt;
  &lt;li&gt;conv layer, 3x3, same padding, 256 channels -&amp;gt;13x13x256&lt;/li&gt;
  &lt;li&gt;Fully-connected layers: 9216 -&amp;gt; 4096 -&amp;gt; 4096 -&amp;gt; 1,000 (Softmax)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Many similarities with LeNet-5, but AlexNet has about 60M parameters.
Also, they used ReLU activation functions.
They also used another type of layer called Local Response Normalization, which
normalizes each pixel/position across all channels. This type of layer is not
really used anymore as it was found that it had a very small impact on the
results.&lt;/p&gt;

&lt;h3 id=&quot;vgg-16-2015&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf%20http://arxiv.org/abs/1409.1556.pdf&quot;&gt;VGG-16 (2015)&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;The architecture can be visualized
&lt;a href=&quot;https://miro.medium.com/max/850/1*_Lg1i7wv1pLpzp2F4MLrvw.png&quot;&gt;here&lt;/a&gt;.
VGG-16 really simplified the neural networks architecture. It only relies on a
single type of convolutional layer (3x3, stride=1, same padding) and a single
type of max pooling (2x2, stride=2).
The input is 224x224x3.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;conv: 64 channels&lt;/li&gt;
  &lt;li&gt;conv: 64 channels -&amp;gt; 224x224x64&lt;/li&gt;
  &lt;li&gt;max pooling -&amp;gt; 112x112x64&lt;/li&gt;
  &lt;li&gt;conv: 128 channels&lt;/li&gt;
  &lt;li&gt;conv: 128 channels -&amp;gt; 112x112x128&lt;/li&gt;
  &lt;li&gt;max pooling -&amp;gt; 56x56x128&lt;/li&gt;
  &lt;li&gt;conv: 256 channels&lt;/li&gt;
  &lt;li&gt;conv: 256 channels&lt;/li&gt;
  &lt;li&gt;conv: 256 channels -&amp;gt; 56x56x256&lt;/li&gt;
  &lt;li&gt;max pooling -&amp;gt; 28x28x256&lt;/li&gt;
  &lt;li&gt;conv: 512 channels&lt;/li&gt;
  &lt;li&gt;conv: 512 channels&lt;/li&gt;
  &lt;li&gt;conv: 512 channels -&amp;gt; 28x28x512&lt;/li&gt;
  &lt;li&gt;max pooling -&amp;gt; 14x14x512&lt;/li&gt;
  &lt;li&gt;conv: 512 channels&lt;/li&gt;
  &lt;li&gt;conv: 512 channels&lt;/li&gt;
  &lt;li&gt;conv: 512 channels -&amp;gt; 14x14x512&lt;/li&gt;
  &lt;li&gt;max pooling -&amp;gt; 7x7x512&lt;/li&gt;
  &lt;li&gt;fc: 4096 -&amp;gt; 4096 -&amp;gt; 10,000 (softmax)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;VGG-16 has about 138M parameters, which is large even by today’s standards.
16 is the number of layers. There is also a VGG-19, but performance is
comparable.
VGG is attractive to the community b/c it simplifies the construction of the
network by systematizing the change in height/width and channels from one layer
to the next.&lt;/p&gt;

&lt;h2 id=&quot;video-resnets-2016&quot;&gt;Video: &lt;a href=&quot;https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf&quot;&gt;ResNets (2016)&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;ResNets was introduced to help training of very deep neural networks.
In practice, we notice that training error for plain network will start to
increase after a certain depth is reached.
ResNets allows to train networks of 100 or 1,000 layers deep and still see the
training error gradually decrease as the depth of the network increases. ResNets
helps with the problems of vanishing and exploding gradients.&lt;/p&gt;

&lt;p&gt;Elementary block of a ResNet is a Residual Block, which adds a skip connection
(or shortcut) to typical fully-connected layers. In a given layer, after the
linear part but before the application of the ReLU activation function, the
value of layer output a few levels below is added, eg, 
$a^{[l+2]} = g(z^{[l+1]} + a^{[l]})$.&lt;/p&gt;

&lt;h2 id=&quot;video-why-resnets-work&quot;&gt;Video: Why ResNets work?&lt;/h2&gt;

&lt;p&gt;Uses an intuitive example to try and show why ResNets work.
Intuition is that ResNets can very easily learn the identity map (W=0, b=0).
That means, it’s easy for a ResNets layer plugged after an existing DL to do at
least as well. Then after training, it’s easy to imagine that the addition of
the ResNets will improve performance.&lt;/p&gt;

&lt;p&gt;Something else to notice about ResNets is that you need to have $z^{[l+2]}$ and
$a^{[l]}$ that have same dimension. For that reason, you often see people using
the same convolution (or same padding).
However, this is not mandatory, as one can add a linear transformation of
$a^{[l]}$ so that its dimension match with $z^{[l+2]}$. That linear
transformation can be learned or fixed (eg, padding,…).&lt;/p&gt;

&lt;h2 id=&quot;video-networks-in-networks-and-1x1-convolutions&quot;&gt;Video: Networks in Networks and 1x1 Convolutions&lt;/h2&gt;

&lt;p&gt;In the paper &lt;a href=&quot;https://arxiv.org/pdf/1312.4400.pdf&quot;&gt;Network in Network&lt;/a&gt;, they
discuss the use of a 1x1 convolution. It’s interesting in the case of a
multi-channel image, where it takes a linear combination of each channels
(before passing it through a non-linear activation function).&lt;/p&gt;

&lt;p&gt;This idea of 1x1 convolution can be used to shrink the number of channels of an
image w/o altering the height or width.
altering&lt;/p&gt;

&lt;h2 id=&quot;video-inception-network-motivation&quot;&gt;Video: Inception Network Motivation&lt;/h2&gt;

&lt;p&gt;Main motivation is that instead of choosing amongh a 1x1 convolution, a 3x3
convolution, a 5x5 convolution, or a max-pooling layer, why don’t you just do
them all in a single layer. Then stack all the outputs along the channel
dimension. That’s what the inception layer is about.
You of course need same padding to preserve the height and width and be able to
stack them along the channel dimension.&lt;/p&gt;

&lt;p&gt;Problem of that approach is the computational cost. For instance, 5x5 conv with
same padding, going from 28x28x192 to 28x28x32 involves about 120M fp
operations.
A solution is to introduce a bottleneck layer, that is 1x1 convolution that
reduces the number of channles (eg, from 192 down to 16). Then apply the 5x5
convolution on that output with a reduced number of channels. You can reduce the
computational cost by 1/10th.&lt;/p&gt;

&lt;h2 id=&quot;video-inception-network&quot;&gt;Video: &lt;a href=&quot;https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf&quot;&gt;Inception network&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;A visualization of the architecture of the inception network can be found
&lt;a href=&quot;https://cdn.analyticsvidhya.com/wp-content/uploads/2018/10/googlenet.png&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Rough idea is as descrbied in the previous section: apply 1x1, 3x3, 5x5
convolutions, and max-pooling in parallel, the concatenate all the channels
together. To reduce computational cost, 3x3 and 5x5 convolutional layers are
preceded by a bottleneck layer, and to avoid having max-pooling dominate the
output, it is followed by a bottleneck layer. All of this represents an
inception module.
The inception network is more or less a stack of multiple inception modules.&lt;/p&gt;

&lt;p&gt;A noticeable addition is that you find intermediate probes in the network that
try to predict the outcome with a sequence of fully-connected layers followed by
a softmax. The rationale behind that is to try and force the network to
regularize itself by trying to do these intermediate predictions.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Important note&lt;/em&gt;: Inception was the architecture used to introduce &lt;a href=&quot;http://proceedings.mlr.press/v37/ioffe15.pdf&quot;&gt;batch
norm&lt;/a&gt;. A few other modifications
were made to the network though (learning rate, no dropout,…).&lt;/p&gt;

&lt;h2 id=&quot;video-using-open-source-implementation&quot;&gt;Video: Using open-source implementation&lt;/h2&gt;

&lt;p&gt;github&lt;/p&gt;

&lt;h2 id=&quot;video-transfer-learning&quot;&gt;Video: Transfer Learning&lt;/h2&gt;

&lt;p&gt;At least in computer vision, transfer learning should always be considered
unless you have an exceptionally large dataset. Transfer learning ranges from:
i. keeping all layers frozen and replacing/training the output layer (eg,
softmax layer)
ii. keeping a few layers frozen (if you have more data)
iii. re-training the entire network, starting from the trained model as
initialization.&lt;/p&gt;

&lt;h2 id=&quot;video-data-augmentation&quot;&gt;Video: Data augmentation&lt;/h2&gt;

&lt;p&gt;In most computer vision applications today, you never have enough data. Data
augmentation can help “get” more data.&lt;/p&gt;

&lt;p&gt;A few common methods:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;mirroring&lt;/li&gt;
  &lt;li&gt;random cropping&lt;/li&gt;
  &lt;li&gt;rotation (less common)&lt;/li&gt;
  &lt;li&gt;shearing (less common)&lt;/li&gt;
  &lt;li&gt;local warping (less common)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Another type of data augmentation is color shifting, where you add random
perturbations to the RGB channels.
Also, there is a “PCA color augmentation” (see AlexNet paper).&lt;/p&gt;

&lt;p&gt;In terms of computational efficiency, data augmentation can be done a separate
CPU thread, before passing the augmented data to the CPU/GPU used for training,
which is completely paralelizable.&lt;/p&gt;

&lt;p&gt;Note that data augmentation most often comes with a set of hyparameters that
also need to be selected.&lt;/p&gt;

&lt;h2 id=&quot;video-state-of-computer-vision&quot;&gt;Video: State of Computer Vision&lt;/h2&gt;

&lt;p&gt;Less data means you’ll need more hand-engineering to get good results.&lt;/p&gt;

&lt;p&gt;Other tips:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Ensemble: good for competition or benchmark, but rarely used in production due
 to the computational cost&lt;/li&gt;
  &lt;li&gt;multi-crop at test time: compute prediction for multiple cropped version of
  your input image, then average the results. Same problem as ensemble for
production solution&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>The Future of PyTorch</title>
   <link href="https://bcrestel.github.io/2021/01/20/The_Future_of_PyTorch/"/>
   <updated>2021-01-20T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2021/01/20/The_Future_of_PyTorch</id>
   <content type="html">&lt;p&gt;PyTorch
&lt;a href=&quot;https://ai.facebook.com/blog/reengineering-facebook-ais-deep-learning-platforms-for-interoperability/&quot;&gt;announced&lt;/a&gt;
that it will make some changes to its library, fully integrating
&lt;a href=&quot;https://www.pytorchlightning.ai/&quot;&gt;Lightning&lt;/a&gt; and &lt;a href=&quot;https://hydra.cc/&quot;&gt;Hydra&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;PyTorch Lightning is a framework used for all engineering matters of a Deep
Learning model, for instance it can be used to simplify the training loops. It
recently
&lt;a href=&quot;https://medium.com/pytorch/pytorch-lightning-1-0-from-0-600k-80fc65e2fab0&quot;&gt;released&lt;/a&gt;
its version 1.0 with a stable API. Lightning will also allow easy quantization,
checkpointing, and others.&lt;/p&gt;

&lt;p&gt;For Hydra, I’m less sure what it’s about, but it is
&lt;a href=&quot;https://medium.com/pytorch/hydra-a-fresh-look-at-configuration-for-machine-learning-projects-50583186b710&quot;&gt;described&lt;/a&gt;
as a way to handle all the config files.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Google AutoML for forecasting</title>
   <link href="https://bcrestel.github.io/2021/01/19/Google_AutoML_for_forecasting/"/>
   <updated>2021-01-19T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2021/01/19/Google_AutoML_for_forecasting</id>
   <content type="html">&lt;p&gt;GoogleAI wrote a &lt;a href=&quot;https://ai.googleblog.com/2020/12/using-automl-for-time-series-forecasting.html&quot;&gt;blog
post&lt;/a&gt;
about its new autoML solution for time series forecasting. They tested their
framework on a few different Kaggle competitions, including M5, and obtained
very good results (top 10%) without any manual intervention.&lt;/p&gt;

&lt;p&gt;I actually find that as a proof of quality (as in, not overfitting) that they
get very good results, but not the best results.
There is still a bit of hand-crafted features like the way they deal with
sparsity, very common in retail time series; they add a separate predictor for
whether the next prediciton will be 0 or not (in addition to predicting the
value).
Overall, it’s hard to judge as they don’t provide much information besides the
standard
&lt;a href=&quot;https://1.bp.blogspot.com/-5VIKGwKurE4/X8p0t96KlzI/AAAAAAAAG3w/R_mfc8UOYG8tSbn0RjvdNhS8z9RPYxxZwCLcBGAsYHQ/s1999/image2.png&quot;&gt;skecth&lt;/a&gt;
of an autoML pipeline: feature engineering, architecture search, hyperparameter
search, ensembling of best models. It is likely that for the architecture search
part, they re-used results from Google previous
&lt;a href=&quot;https://ai.googleblog.com/2017/05/using-machine-learning-to-explore.html&quot;&gt;work&lt;/a&gt;
on the matter (evolutionary algorithms and RL).&lt;/p&gt;

&lt;p&gt;I don’t really like their exclusive reliance on RNN layers, but this is still
considered standard by many. And it’d be interesting to see how this behaves in
“real” real-life scenarios, not in competitions.
Overall, I still feel there is a lot of manual work that is required for
ML-based time series forecasting. Whether you hide that in a pipeline and call
that autoML or not is a different topic.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Underspecification in ML</title>
   <link href="https://bcrestel.github.io/2021/01/13/Underspecification_in_ML/"/>
   <updated>2021-01-13T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2021/01/13/Underspecification_in_ML</id>
   <content type="html">&lt;p&gt;Researchers at Google published a &lt;a href=&quot;https://arxiv.org/pdf/2011.03395.pdf&quot;&gt;paper&lt;/a&gt; on the randomness of Deep Learning.
Based on the initial weights, different trained models will be obtained. Theys
show empirically that despite all these models being equivalent during
training, their real-life performance can be dramatically different.&lt;/p&gt;

&lt;p&gt;MIT Tech Review published a blog &lt;a href=&quot;https://www.technologyreview.com/2020/11/18/1012234/training-machine-learning-broken-real-world-heath-nlp-computer-vision/?utm_medium=tr_social&amp;amp;utm_campaign=site_visitor.unpaid.engagement&amp;amp;utm_source=Twitter#Echobox=1609536735&quot;&gt;post&lt;/a&gt; about it&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Michelangelo Uber ML platform</title>
   <link href="https://bcrestel.github.io/2021/01/13/Michelangelo_Uber_ML_platform/"/>
   <updated>2021-01-13T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2021/01/13/Michelangelo_Uber_ML_platform</id>
   <content type="html">&lt;p&gt;Uber has a blog
&lt;a href=&quot;https://eng.uber.com/michelangelo-machine-learning-platform/&quot;&gt;post&lt;/a&gt; detailing
their ML platform which I found interesting.&lt;br /&gt;
I’m using this post to save highlights and comments.&lt;/p&gt;

&lt;p&gt;Their article is from 2017. They
started building Michelangelo around mid-2015.
&lt;em&gt;It is designed to cover the end-to-end ML workflow: manage data, train,
evaluate, and deploy models, make predictions, and monitor predictions. The
system also supports traditional ML models, time series forecasting, and deep
learning.&lt;/em&gt;: That looks like a pretty comprehensive system, at least on the
paper.&lt;/p&gt;

&lt;p&gt;They mention that prior to Michelangelo, all ML was ad-hoc, done on a desktop
(!), and with different production solutions for each team. So basically, it was
a patchwork, a very heterogeneous picture. They were starting to see technical
debt in their ML projects. So the plan: &lt;em&gt;Michelangelo is designed to address
these gaps by standardizing the workflows and tools across teams though an
end-to-end system that enables users across the company to easily build and
operate machine learning systems at scale.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;They mention different open-source softwars that they used to build
Michelangelo:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;HDFS: Hadoop Distributed File System&lt;/li&gt;
  &lt;li&gt;Spark: Compute engine for Apache Hadoop&lt;/li&gt;
  &lt;li&gt;Samza: Samza is Linkedin’s framework for continusou data processing&lt;/li&gt;
  &lt;li&gt;Cassandra: it is a free and open-source, distributed, wide column store, NoSQL
 database management system designed to handle large amounts of data across
 many commodity servers, providing high availability with no single point of
 failure.&lt;/li&gt;
  &lt;li&gt;MLLIB: Apache’s scalable ML library; fits into Spark and interoperates with
 Numpy and R.&lt;/li&gt;
  &lt;li&gt;XGBoost: they mention that they use gradient boosted decision trees to solve
 the meal time delivery problem at UberEats.&lt;/li&gt;
  &lt;li&gt;TensorFlow&lt;/li&gt;
  &lt;li&gt;(Apache) Kafka: open-source stream processing platform; to handle real-time
 data feeds.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I understand that they do not describe their datalake in this post, but mentions
its existence (duh): &lt;em&gt;Michelangelo is built on top of Uber’s data and compute
infrastructure, providing a data lake that stores all of Uber’s transactional
and logged data, Kafka brokers that aggregate logged messages from all Uber’s
services, a Samza streaming compute engine, managed Cassandra clusters, and
Uber’s in-house service provisioning and deployment tools.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Standard ML workflow handled by Michelangelo:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Manage data: handle create of data pipeline (generate features, labels,…) +
allow data management in the form of a Feature Store (save features to share
with another project). Detail some technical challenges with online acces (no
HDFS; need to pre-compute). Features in Feature Store are updated daily. Also
created a Domain Specific Language to transform existing features (extract
day-of-week, eg); this DSL is part of the model configuration and therefore
guarantees that the same transforms are applied for training and inference.&lt;/li&gt;
  &lt;li&gt;Train models: &lt;em&gt;A model configuration specifies the model type,
hyper-parameters, data source reference, and feature DSL expressions, as well as
compute resource requirements (the number of machines, how much memory, whether
or not to use GPUs, etc.). It is used to configure the training job, which is
run on a YARN or Mesos cluster.&lt;/em&gt; At the time of the article, framework only did
offline training. After training completed, publish a report with 
performance metrics and results plots, and save &lt;em&gt;the original configuration, the learned
parameters, and the evaluation report&lt;/em&gt;. Michelangelo can be used for
hyperparameters search, and works with single models or &lt;em&gt;partitioned models&lt;/em&gt;,
i.e. models trained on multiple folds.&lt;/li&gt;
  &lt;li&gt;Evaluate models: Michelangelo typically used to identify best model then push
it to production. They have a system to keep track of all experiments run (ie,
all models trained) by storing: basic info (user name, time, duration,…),
datasets used, model configuration (ie, also including features, and even
features importance visualization), results (metrics, charts, full learned
parameters of the trained model, info/data required for model visualization).&lt;/li&gt;
  &lt;li&gt;Deploy models: 3 types of deployment: offline (model packaged in a container,
ready to be loaded on demand), online (model deployed and queried via Remote
Procedure Calls), or library deployment (invoked via Java API). &lt;em&gt;In all cases,
the required model artifacts (metadata files, model parameter files, and
compiled DSL expressions) are packaged in a ZIP archive and copied to the
relevant hosts across Uber’s data centers using our standard code deployment
infrastructure.&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Make predictions: Framework allows to deploy multiple models to the same
servicing constainer which can be used for A/B testing or seamless model update.
They have a nice summary
&lt;a href=&quot;http://1fykyq3mdn5r21tpna3wkdyi-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/image3.png&quot;&gt;plot&lt;/a&gt;
of the different scenarios.&lt;/li&gt;
  &lt;li&gt;Monitor predictions: Michelangelo allows to log a percentage of the
prediction data then combine with realized values to calculate real performance
of the model. They can then visualize evoluation of model performance and/or set
threshold for automatic alerts. See
&lt;a href=&quot;http://1fykyq3mdn5r21tpna3wkdyi-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/image8.png&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Future developments (at the time): autoML, model viz, online learning,
distributed DL.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Notes for CNN course by deeplearning.ai (week 1)</title>
   <link href="https://bcrestel.github.io/2021/01/11/Notes_for_CNN_course_by_deeplearning.ai/"/>
   <updated>2021-01-11T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2021/01/11/Notes_for_CNN_course_by_deeplearning.ai</id>
   <content type="html">&lt;p&gt;I decided to take the course &lt;a href=&quot;https://www.coursera.org/learn/convolutional-neural-networks&quot;&gt;Convolutional Neural
Networks&lt;/a&gt; by
&lt;a href=&quot;https://www.deeplearning.ai/&quot;&gt;deeplearning.ai&lt;/a&gt; and hosted by Coursera. And I’m
going to take some notes in this document&lt;/p&gt;

&lt;h1 id=&quot;week-1-foundations-of-convolutional-neural-networks&quot;&gt;Week 1: Foundations of Convolutional Neural Networks&lt;/h1&gt;

&lt;p&gt;Objectives:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Explain the convolution operation&lt;/li&gt;
  &lt;li&gt;Apply two different types of pooling operations&lt;/li&gt;
  &lt;li&gt;Identify the components used in a convolutional neural network (padding, stride, filter, …) and their purpose&lt;/li&gt;
  &lt;li&gt;Build and train a ConvNet in TensorFlow for a classification problem&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;video-computer-vision&quot;&gt;Video: Computer Vision&lt;/h2&gt;

&lt;p&gt;Example of vision problems:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Image classification: decide what picture represents (given set number of categories)&lt;/li&gt;
  &lt;li&gt;object detection: draw boxes around specific objects in a picture&lt;/li&gt;
  &lt;li&gt;neural style transfer: content image + style image -&amp;gt; content image with the
 style of the style image&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Challenges:
Input can be really big: 64x64x3 images -&amp;gt; 12288 pixels; very small. Large
images can get really big very quickly. 1000x1000x3 = 3M pixels. With a first
layer of 1,000 nodes, you end up with 30M weights just for the first layer. With
so many parameters, hard not to overfit; plus need lots of memory.&lt;/p&gt;

&lt;p&gt;To still be able to use large images, you need to use CNN.&lt;/p&gt;

&lt;h2 id=&quot;video-edge-detection-example&quot;&gt;Video: Edge Detection Example&lt;/h2&gt;

&lt;p&gt;CNN based off the &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolution&quot;&gt;convolution
operation&lt;/a&gt;. Illustrated by the edge detection
example.
Another interest ref for convolution is &lt;a href=&quot;https://colah.github.io/posts/2014-07-Understanding-Convolutions/&quot;&gt;Colah’s
blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;How do you detect edges in an image?
Show an example of a 6x6 matrix convolved with a 3x3 kernel (or filter), giving
a 4x4 image.
You just slide the kernel over the image and at each step multiply the entries
pointwise then sum them all.
Similar to what you would do with the actual operator, \(f \star g = \int_{-\infty}^{\infty} f(\tau)g(t-\tau) d\tau\), where for each
$t$ the function $g$ (the kernal, or filter) is shifted by an amount $t$.&lt;/p&gt;

&lt;p&gt;Actually, this mathematical definition is a little bit different as the kernel
is mirrored around its axes (we have $g(t-\tau)$ as a function of $\tau$). This
is discussed in the section on &lt;a href=&quot;## Video: Strided Convolutions&quot;&gt;Video: Strided Convolutions&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Don’t need to re-implement a convolution operator: python -&amp;gt; conv_forward,
tensorflow -&amp;gt; tf.nn.conv2d, keras -&amp;gt; cond2D&lt;/p&gt;

&lt;p&gt;Why is this doing edge detection?
Look at picture with 2 colors separated by a vertical edge. 
Apply 3x3 kernel: 1,0,-1 (in x direction; same in y).
Obtain a thick edge, in your 4x4 image, in the middle.&lt;/p&gt;

&lt;h2 id=&quot;video-more-edge-detection&quot;&gt;Video: More Edge Detection&lt;/h2&gt;

&lt;p&gt;Introduces more examples for filters, for instance for horizontal edge
detection. But also different types of vertical edge detectors:
eg, 1,0,-1//2,0,-2//1,0,-1 (Sobel filter)
or 3,0,-3//10,0,-10//3,0,-3/ (Scharr filter).
But you can parametrize the values of your kernel, and learn the ideal kernel
for your problem. And that is a key idea of modern computer vision.&lt;/p&gt;

&lt;h2 id=&quot;video-padding&quot;&gt;Video: Padding&lt;/h2&gt;

&lt;p&gt;Discrete convolution presented shrink the image, so you could only apply it a
few times. Initial image n x n, kernel f x f, then convolved image $n-f+1 \times
n-f+1$.&lt;/p&gt;

&lt;p&gt;Also, pixels close to the edge of an image are used much less in the convolution
compared to central pixels.&lt;/p&gt;

&lt;p&gt;Solution for both problems: pad the image before applying convolution operator,
i.e., add pixels on the outside of the image: eg, 6x6 -&amp;gt;(padding) 8x8
-&amp;gt;(convolution w/ 3x3 kernel) 6x6. 
Padded convoluted image has dim $n+2p-f+1 \times n+2p-f+1$.
By convention, you pad with 0’s.&lt;/p&gt;

&lt;p&gt;How much to pad? Valid convolution vs Same convolution&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Valid convolution = no padding&lt;/li&gt;
  &lt;li&gt;Same convolution: pad so as to offset the shrinking effect of the convolution
(final image has same dim as input image) -&amp;gt; $2p = f-1$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that by convention, in computer vision, f is typically an odd number: 1
(less common), 3,
5, 7. That
has the advantage that you can have a symmetric padding.&lt;/p&gt;

&lt;h2 id=&quot;video-strided-convolutions&quot;&gt;Video: Strided Convolutions&lt;/h2&gt;

&lt;p&gt;Stride = by how many pixels you shift the kernel (vertically and horizontally).
So the convolved image will be smaller with stride 2 than with stride 1.
Resulting image side length: $(n+2p-f)/s+1$ (potentially rounded if needed)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-correlation&quot;&gt;Cross-correlation&lt;/a&gt; vs
convolution:&lt;br /&gt;
As discussed earlier, the actual convolution operator would require to flip the
kernel around its axes, since we do \(\int_{-\infty}^{\infty} f(\tau) g(t-\tau)
d\tau\). But in deep learning, people don’t flip it. So in that sense, this is
closer to the cross-correlation operator whic is defined, for real functions, as
\(\int_{-\infty}^{\infty} f(\tau) g(t+\tau) d\tau\). 
But as explained in the video, the convention is such that people in deep
learning still call that a convolution operator.&lt;/p&gt;

&lt;h2 id=&quot;video-convolutions-over-volumes&quot;&gt;Video: Convolutions over volumes&lt;/h2&gt;

&lt;p&gt;Application: RGB images (3 color channels).&lt;br /&gt;
Convolve RGB image with a 3d kernel (same number of channels).&lt;/p&gt;

&lt;p&gt;Notation: Image 6 x 6 x 3 = (height, width, channels).
Number of channels in image and kernel must be equal.&lt;/p&gt;

&lt;p&gt;Really this is 2d convolutions repeated over a stack of 2d images. Not really a 3d
convolution. You could imagine having an image of dim 6 x 6 x 6, and convolve
along the third dim also.&lt;/p&gt;

&lt;p&gt;Multiple filter:
You can apply different kernels to the same image(s) and stack the results to
generate also a 3d output. For intance, you could combine the output of a
vertical edge detector and horizontal edge detector.&lt;/p&gt;

&lt;p&gt;Dim summary when using &lt;code&gt;nb_filters_used&lt;/code&gt; filters on an image of &lt;code&gt;c&lt;/code&gt; channels: image = n x n x c, kernel = f x f x c, then output = n-f+1 x n-f+1 x
nb_filters_used&lt;/p&gt;

&lt;h2 id=&quot;video-one-layer-of-a-convolutional-layer&quot;&gt;Video: One layer of a convolutional layer&lt;/h2&gt;

&lt;p&gt;Convolutional layer = activation_function (multiple filters–with the right
number of channels– + a bias), where bias is a single real number.
So output has same dimension as the example in the previous video. Don’t forget
that there is a single bias per filter.&lt;/p&gt;

&lt;p&gt;And number of parameters is equal to (dim of kernel + 1) x nb_kernels. And this
number of parameters is independent of the dimension of the input!&lt;/p&gt;

&lt;p&gt;Notations for layer $l$:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$f^{[l]}$ = filter size&lt;/li&gt;
  &lt;li&gt;$p^{[l]}$ = padding&lt;/li&gt;
  &lt;li&gt;$s^{[l]}$ = stride&lt;/li&gt;
  &lt;li&gt;input: $n_H^{[l-1]} \times n_w^{[l-1]} \times n_C^{[l-1]}$&lt;/li&gt;
  &lt;li&gt;output: $n_H^{[l]} \times n_w^{[l]} \times n_C^{[l]}$, where
\(n_{H/W}^{[l]} = (n_{H/W}^{[l-1]} + 2p^{[l]} - f^{[l]}) / s^{[l]} + 1\).&lt;/li&gt;
  &lt;li&gt;And $n_c^{[l]}$ is equal to the number of filters used in the convolutional
layer.&lt;/li&gt;
  &lt;li&gt;Each filter has dim $f^{[l]} \times f^{[l]} \times n_c^{[l-1]}$ as the
number of channels in the kernel has to be equal to the number of channels in
the input image.&lt;/li&gt;
  &lt;li&gt;Weights: $f^{[l]} \times f^{[l]} \times n_c^{[l-1]} \times n_c^{[l]}$&lt;/li&gt;
  &lt;li&gt;Bias: $n_c^{[l]}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note: ordering Batch, Height, Width, Channel is not universal as some people
will put channel after batch.&lt;/p&gt;

&lt;h2 id=&quot;video-a-simple-convolutional-network-example&quot;&gt;Video: A simple convolutional network example&lt;/h2&gt;

&lt;p&gt;Basic idea is to stack convolutional layers one after the other. At the end, you
can flatten the last image and pass it to an appropriate loss function.&lt;/p&gt;

&lt;p&gt;One general trend is that the image dimension is going down as we progress in
the net, while the number of channels go up.&lt;/p&gt;

&lt;p&gt;Basic convolution network includes: convolutional layers, pooling layers, and
fully connected layers.&lt;/p&gt;

&lt;h2 id=&quot;video-pooling-layers&quot;&gt;Video: Pooling Layers&lt;/h2&gt;

&lt;p&gt;Pooling layers are often found in convolutional networks. The intuition behind
these layers is not well understood, but they are commonly used as they seem to
improve performance.&lt;/p&gt;

&lt;p&gt;The idea is to apply a filter (or kernel), the same way we did it for
convolution layers. That is a pooling layers has a size $f$ and a stride $s$.
But instead of applying a convolution kernel for every step of the filter, we
instead apply a single function. In practice, we find max pooling (most common)
and average pooling (not as common). 
For max pooling for instance, at every step of the filter, we take the max
within that filter.
We sometimes find average pooling at the end of a network, to compress an image
into a single pixel.
Interestingly, pooling layers have no parameter to learn (only hyperparameters).&lt;/p&gt;

&lt;p&gt;When the input image has multiple channels, we apply the pooling filter to each
layer.
Most commonly, we don’t apply padding with pooling layers.&lt;/p&gt;

&lt;p&gt;Also, it is important to realize that padding, in the context of pooling
layers, does not have the same meaning as for convolutional layers. In the
context of pooling layers, same padding means that the picture will be enlarged
such that the kernel can be applied an exact number of times. Whereas with valid
padding, the kernel is applied as many times as it can fit. 
In other words, even with same padding, the output image does not necessarily
have the same dimension as the input. For instance,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;A = tf.ones((1, 14, 32, 1))
P1 = tf.nn.max_pool(A, ksize=[1,8,8,1], strides=[1,8,8,1], padding=&apos;SAME&apos;)
P2 = tf.nn.max_pool(A, ksize=[1,8,8,1], strides=[1,8,8,1], padding=&apos;VALID&apos;)
with tf.Session() as sess:
    print(f&quot;P1.shape = {sess.run(P1).shape}&quot;)
    print(f&quot;P2.shape = {sess.run(P2).shape}&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;will give&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;P1.shape = (1, 2, 4, 1)
P2.shape = (1, 1, 4, 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This
&lt;a href=&quot;https://datascience.stackexchange.com/questions/67334/whats-the-purpose-of-padding-with-maxpooling&quot;&gt;post&lt;/a&gt;
also explains it well.&lt;/p&gt;

&lt;h2 id=&quot;video-cnn-example&quot;&gt;Video: CNN example&lt;/h2&gt;

&lt;p&gt;Introduce an example of CNN to do digit recognition on a 32 x 32 x 3 RGB image.
The presented CNN is inspired by LeNet-5.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;convolutional layer 1: 6 filters with hyperparameters $f=5, s=1, p=0$&lt;/li&gt;
  &lt;li&gt;max pooling layer 1: $f=2, s=2$.&lt;/li&gt;
  &lt;li&gt;convolutional layer 2: 16 filters with hyperparameters $f=5, s=1, p=0$&lt;/li&gt;
  &lt;li&gt;max pooling layer 2: $f=2, s=2$.&lt;/li&gt;
  &lt;li&gt;fully-connected layer 3: flatten output of max pooling layer 2 -&amp;gt; fc3 = 400 x
 120&lt;/li&gt;
  &lt;li&gt;fc4: 120 x 84&lt;/li&gt;
  &lt;li&gt;Softmax layer (I guess fc5 + softmax): 84 x 10 (10 digits to recognize)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We notice that throughout the layers, the image decreases while the number of
channels increase.
Also, we notice the alternance conv layer / pool layer a few times, followed by
a few fully-connected layers.&lt;/p&gt;

&lt;h2 id=&quot;video-why-convolutions&quot;&gt;Video: Why convolutions?&lt;/h2&gt;

&lt;p&gt;What makes convolution layers efficient for image recognition?&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;weight sharing: you work on the input image with limited number of parameters&lt;/li&gt;
  &lt;li&gt;sparsity of connections: each output pixel only depends on a few of the input
 pixels&lt;/li&gt;
  &lt;li&gt;Conv layers are believed to be good at preserving translation invariance.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;assignments&quot;&gt;Assignments&lt;/h2&gt;

&lt;p&gt;I put my assignments on
&lt;a href=&quot;https://github.com/bcrestel/cnn_deeplearningai/tree/main/assignments/week1&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;additional-references&quot;&gt;Additional references&lt;/h2&gt;

&lt;p&gt;This blog
&lt;a href=&quot;https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md&quot;&gt;post&lt;/a&gt; has
very nice visualizations of what convolutional layers do.&lt;/p&gt;

&lt;h2 id=&quot;supplementary-note&quot;&gt;Supplementary note&lt;/h2&gt;

&lt;p&gt;When you have a dilated convolution, the dimension of the output becomes
\(int \left( \frac{N+2p-d*(f-1) - 1}{s} \right) + 1\).&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>First steps with rpy2</title>
   <link href="https://bcrestel.github.io/2021/01/03/First_steps_with_rpy2/"/>
   <updated>2021-01-03T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2021/01/03/First_steps_with_rpy2</id>
   <content type="html">&lt;h1 id=&quot;how-to-install-in-docker&quot;&gt;How to install in Docker&lt;/h1&gt;
&lt;p&gt;I haven’t found a way to install it via pipenv, so I added the following lines
directly to my Dockerfile&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;RUN apt-get update &amp;amp;&amp;amp; apt-get install -y r-base
RUN pip install rpy2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Yes, don’t forget to install R before installing &lt;code&gt;rpy2&lt;/code&gt;.&lt;/p&gt;

&lt;h1 id=&quot;how-to-install-and-load-a-r-package&quot;&gt;How to install and load a R package&lt;/h1&gt;
&lt;p&gt;Default package should come with your R installation so that you can direclty
load them by doing&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;my_r_package_loaded = rpackages.importr(&quot;my_r_package&quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can then use your loaded R package like a module. For instance,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;my_r_package_loaded.this_amazing_script(var1=1.0, var2=&apos;hello&apos;, var3=3.4)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But for specific packages, like here the &lt;code&gt;TOSTER&lt;/code&gt; packace, you can do
this&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from rpy2 import robjects
import rpy2.robjects.packages as rpackages

utils = rpackages.importr(&apos;utils&apos;)
utils.chooseCRANmirror(ind=1)
utils.install_packages(&quot;TOSTER&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;how-to-run-a-r-script-in-a-notebook&quot;&gt;How to run a R script in a notebook&lt;/h1&gt;
&lt;p&gt;Given the script is in the same directory as your notebook, you can do&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import rpy2.robjects as robjects
r = robjects.r
r.source(&apos;my_first_r_script.r&apos;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;how-to-plot-from-r-using-rpy2&quot;&gt;How to plot from R using rpy2&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;import rpy2.robjects as robjects
from rpy2.robjects.lib import grdevices
from IPython.display import Image, display

with grdevices.render_to_bytesio(grdevices.jpeg, width=1024, height=896, res=150) as img:
    r.source(&apos;my_first_r_plot.r&apos;)
    
display(Image(data=img.getvalue(), format=&apos;jpeg&apos;, embed=True))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;More on the plotting can be found in this demo
&lt;a href=&quot;https://github.com/marsja/jupyter/blob/master/Rpy2%20and%20R%20plots%20in%20a%20Jupyter%20Notebook!.ipynb&quot;&gt;notebook&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Fixing jekyll</title>
   <link href="https://bcrestel.github.io/2020/10/29/Fixing_jekyll/"/>
   <updated>2020-10-29T00:00:00-04:00</updated>
   <id>https://bcrestel.github.io/2020/10/29/Fixing_jekyll</id>
   <content type="html">&lt;p&gt;After upgrading to Catalina, I could not run &lt;code&gt;jekyll&lt;/code&gt; locally to render that
website on my laptop. I was getting a weird error message about the ruby
interpreter being bad.&lt;/p&gt;

&lt;p&gt;The solution is to use a different ruby interpreter, installed via Homebrew:
&lt;code&gt;brew install ruby&lt;/code&gt;. Then update all your paths so that the command line will
default to this newly installed interpreter:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export PATH=&quot;/Users/bencrestel/local/homebrew/opt/ruby/bin:$PATH&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once this is done, we can install the jekyll gem (and bundler, but I’m not sure
what that is): &lt;code&gt;gem install --user-install bundler jekyll&lt;/code&gt;. Next, you add that
to your PATH&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export PATH=&quot;/Users/bencrestel/.gem/ruby/2.7.0/bin&quot;:$PATH
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;so that the right version of &lt;code&gt;jekyll&lt;/code&gt; will be used. And you’re good to go.&lt;/p&gt;

&lt;p&gt;Ref: All the info was in that
&lt;a href=&quot;https://github.com/MichaelCurrin/learn-to-code/blob/master/en/topics/scripting_languages/Ruby/README.md#install-and-upgrade&quot;&gt;post&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Signing commits in github</title>
   <link href="https://bcrestel.github.io/2020/10/28/Signing_commits_in_github/"/>
   <updated>2020-10-28T00:00:00-04:00</updated>
   <id>https://bcrestel.github.io/2020/10/28/Signing_commits_in_github</id>
   <content type="html">&lt;p&gt;My company asked us to start signing our commits. Let’s see how we can do that.&lt;/p&gt;

&lt;p&gt;Github put together a
&lt;a href=&quot;https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/managing-commit-signature-verification&quot;&gt;page&lt;/a&gt;
to explain the different steps.&lt;/p&gt;

&lt;h1 id=&quot;on-my-local-machine&quot;&gt;On my local machine&lt;/h1&gt;

&lt;h2 id=&quot;create-a-gpg-key&quot;&gt;Create a GPG key&lt;/h2&gt;

&lt;p&gt;The first step is to add a GPG key to you github account. You can check if you
already have one by going to Settings &amp;gt; SSH and GPG keys. If the field for GPG
is empty, then you need to create one. To do so, you need
&lt;a href=&quot;https://blog.ghostinthemachines.com/2015/03/01/how-to-use-gpg-command-line/&quot;&gt;gnupg&lt;/a&gt;.
On Mac OSX, you can simply install the command line by typing &lt;code&gt;brew install
gpg&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;When this is installed, you can create a GPG key by typing &lt;code&gt;gpg
--full-generate-key&lt;/code&gt;. You’ll be prompted with a series of questions, and after
that you get a key.&lt;/p&gt;

&lt;h2 id=&quot;exporting-your-key-to-github&quot;&gt;Exporting your key to github&lt;/h2&gt;

&lt;p&gt;Now you need to export your public key. To do, find the info
of your key, by typing&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;gpg --list-secret-keys --keyid-format LONG
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On the row
&lt;code&gt;sec&lt;/code&gt;, you’ll see &lt;code&gt;rsa4096&lt;/code&gt; if you chose 4,096 bits RSA key, and after that is
your key ID. You can next export that key by typing&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;gpg --armor --export &amp;lt;key_ID&amp;gt; &amp;gt; .gpg/public.key
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will save your public key to a file
&lt;code&gt;.gpg/public.key&lt;/code&gt;. You can share that file with whoever needs it.&lt;/p&gt;

&lt;p&gt;In the case of github, you’ll copy the content of that file into the box
provided when you want to add a new GPG key.&lt;/p&gt;

&lt;h2 id=&quot;sign-your-commits&quot;&gt;Sign your commits&lt;/h2&gt;

&lt;p&gt;Now that this is done, you add your GPG key to your local git config by doing
&lt;code&gt;git config --global user.signingkey &amp;lt;key_ID&amp;gt;&lt;/code&gt;. You can then sign your commit by
adding the &lt;code&gt;-S&lt;/code&gt; argument when doing &lt;code&gt;git commit&lt;/code&gt;….. Well, that is if we didn’t
break anything. Turns out that homebrew upgraded in the process of installing
gpg, and it broke all my symlinks for python3. It actually installed 3.9, which
as unversioned symlinks placed in a different folder. So I had to fix all that,
and re-install our library.&lt;/p&gt;

&lt;p&gt;Now that this works, I still had a few things to do before being able to sign my
commits. The first, mandatory step is to
&lt;a href=&quot;https://github.com/keybase/keybase-issues/issues/2798&quot;&gt;create&lt;/a&gt; the following
environment variable &lt;code&gt;export GPG_TTY=$(tty)&lt;/code&gt;. Otherwise, you will never get a
prompt for the gpg passphrase and your signed commit will fail.
Once this is done, you can sign your commit with the argument &lt;code&gt;-S&lt;/code&gt;, then enter
your GPG passphrase. 
You can check that a commit was signed by doing&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git verify-commit &amp;lt;commit_hash&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you want to automate all that, just enter the following
2 lines:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git config gpg.program gpg
git config commit.gpgsign true
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and you can commit the same way you were doing before (without the &lt;code&gt;-S&lt;/code&gt;
argument).&lt;/p&gt;

&lt;h2 id=&quot;save-passphrase-of-the-gpg-key&quot;&gt;Save passphrase of the GPG key&lt;/h2&gt;

&lt;p&gt;Now I still had to enter my passphrase when signing commits. Which can quickly
become annoying. So I followed the steps highlighted
&lt;a href=&quot;https://gist.github.com/bcomnes/647477a3a143774069755d672cb395ca&quot;&gt;here&lt;/a&gt;. At a
high level, you need to &lt;code&gt;brew install pinentry-mac&lt;/code&gt;, then create 2 files,
&lt;code&gt;~/.gnupg/gpg-agent.conf&lt;/code&gt; in which you add&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Connects gpg-agent to the OSX keychain via the brew-installed$
# pinentry program from GPGtools. This is the OSX &apos;magic sauce&apos;,$
# allowing the gpg key&apos;s passphrase to be stored in the login$
# keychain, enabling automatic key signing.$
pinentry-program /usr/local/bin/pinentry-mac
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;then &lt;code&gt;~/.gnupg/gpg.conf&lt;/code&gt; where you add&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;use-agent
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;on-a-remote-server&quot;&gt;On a remote server&lt;/h1&gt;
&lt;p&gt;Obvioulsy, you don’t need to re-create the key. You can simply modify the global
&lt;code&gt;.gitconfig&lt;/code&gt; file to add &lt;code&gt;user.name, user.email, user.signingkey&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Then, you also need to add your &lt;strong&gt;private&lt;/strong&gt; key to &lt;code&gt;gpg&lt;/code&gt; on the remote server.
To do that, you first need to export your private key from your local machine,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;gpg --armor --export-secret-keys &amp;lt;key_ID&amp;gt; &amp;gt; ~/.gpg/private.key
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then on the remote server, you add that private key by doing&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;gpg --import ~/.../private.key
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will be prompted for your GPG passphrase, and that’s it. You can check that
you added the GPG key successfully doing&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;gpg --list-secret-keys --keyid-format LONG
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After that, you can sign your commits with the &lt;code&gt;-S&lt;/code&gt; option&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git commit -S
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that I didn’t have to set up the environment variable &lt;code&gt;GPG_TTY&lt;/code&gt; (even
though that environment variable was not defined).&lt;/p&gt;

&lt;p&gt;You can automate the signing of commits the way you would do locally.&lt;/p&gt;

&lt;p&gt;It looks like we could use &lt;code&gt;pinentry&lt;/code&gt; on a unix server. But I haven’t tried yet.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;p&gt;This
&lt;a href=&quot;https://juliansimioni.com/blog/troubleshooting-gpg-git-commit-signing/&quot;&gt;post&lt;/a&gt;
is pretty good.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Ideal Calibration</title>
   <link href="https://bcrestel.github.io/2020/10/26/Ideal_Calibration/"/>
   <updated>2020-10-26T00:00:00-04:00</updated>
   <id>https://bcrestel.github.io/2020/10/26/Ideal_Calibration</id>
   <content type="html">&lt;p&gt;In the paper &lt;a href=&quot;https://arxiv.org/pdf/1807.00263.pdf&quot;&gt;Accurate Uncertainties for Deep Learning Using Calibrated
Regression&lt;/a&gt;, the authors detail the ideal
calibration. For background, a calibration $R$ takes a distributional forecast
$H(x_t)$ ($=F_x$) and returns a better approximation to the true cdf $F(y)$. First of
all, an ideal forecast will possess the property that
\[ F_x(y) = F_Y(y) =  \mathbb{P}[Y \leq y ] , \]
or equivalently if $y = F_x^{-1}(p)$ with $p\in[0,1]$,
\[ p = \mathbb{P}[Y \leq F_x^{-1}(p)]. \]&lt;/p&gt;

&lt;p&gt;Now a calibration is a function $R: [0,1] \rightarrow [0,1]$ that applies to the
output of the distributional model. Let’s see what $R$ needs to be to improve
the approximation of the initial forecast. That is,
\[ 
\mathbb{P}[Y \leq (R \circ F_x)^{-1}(p) ]  = 
\mathbb{P}[Y \leq F_x^{-1}(R^{-1}(p))] 
\]
If we define $R$ as 
\[ R(p) = \mathbb{P}[Y \leq F_x^{-1}(p)] \]
then we have
\[ 
\mathbb{P}[Y \leq (R \circ F_x)^{-1}(p) ]  = 
\mathbb{P}[Y \leq F_x^{-1}(R^{-1}(p))] = R(R^{-1}(p))=p
\]
In some sense, $R$ corresponds to the corrected quantile for $Y$ when
approximated by the cdf $F_x$.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Transformer</title>
   <link href="https://bcrestel.github.io/2020/10/21/transformer/"/>
   <updated>2020-10-21T00:00:00-04:00</updated>
   <id>https://bcrestel.github.io/2020/10/21/transformer</id>
   <content type="html">&lt;p&gt;I started looking into the Transformer model. It was first introduced in the
paper &lt;a href=&quot;https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&quot;&gt;Attention is all you
need&lt;/a&gt;. I don’t
have time to dive into the details, but one key part of the paper is the
generalization of the concept of attention. In the
&lt;a href=&quot;https://arxiv.org/pdf/1409.0473.pdf&quot;&gt;paper&lt;/a&gt; that made attention works,
attention is applied to an encoder-decoder model, and the attention score is
calculated as a linear combination of all the hidden states of the encoder. That
linear combination is calculated by another neural network (feedforward, I
believe) that takes as input the current hidden state of the decoder and all the
hidden states of the encoder). The output is then passed to a softmax, then that
score multiply each hidden states of the encoder, which we call the values. So
the attention score is
\[ Attention = \sum_i Scores_i  Values_i \]
where $Scores \in \mathbb{R}^n, Values \in \mathbb{R}^{n \times d_v}$, and $d_v$
is the number of hidden cells.&lt;/p&gt;

&lt;p&gt;The problem of that approach is that you actually calculate each attention score
one point at a time, so if you have $m$ words in your input sequence and $n$
words in your output sequence, you’ll pass through that network $m \times n$,
which can get slow pretty quickly.
So in the Tranformers paper, the authors replace the scores generated by a
neural network with a dot product. To do so, they introduce the new concepts of
queries and keys. One analogy that I heard to describe is that would like a
fuzzy lookup in a dict: if your query matches a key, no problem, but if not,
this fuzzy dict will return a mix of the values whose keys resemble the most the
query. That is the idea of the attention mechanism.
So with symbols, the attention mechanism becomes
\[ Attention = \text{softmax}(\frac{Q K^T}{\sqrt{d_k}}) V \]
where $Q \in \mathbb{R}^{m \times d_k}, K \in \mathbb{R}^{n \times d_k}, V \in
\mathbb{R}^{n \times d_v}$.
In the initial attention paper, the keys and values are the same, the query is
the hidden cell from the decoder, and the dot
product $Q K^T$ is replaced with a neural network that takes $Q, K$ as inputs.&lt;/p&gt;

&lt;p&gt;Additional References:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://jalammar.github.io/illustrated-transformer/&quot;&gt;Illustrated Transformer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms&quot;&gt;stackexchange&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/&quot;&gt;Paper Dissected: Attention is all you need”&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Under determined least squares</title>
   <link href="https://bcrestel.github.io/2020/10/06/Under_determined_least_squares/"/>
   <updated>2020-10-06T00:00:00-04:00</updated>
   <id>https://bcrestel.github.io/2020/10/06/Under_determined_least_squares</id>
   <content type="html">&lt;p&gt;I somehow always forget these things. So I’ll mark it down here once and for
all.&lt;/p&gt;

&lt;p&gt;When looking for parameter $\beta$ in a linear system
\[ y = X . \beta + e \]
where $y \in \mathbb{R}^{n \times 1}, X \in \mathbb{R}^{n \times  m}, \beta \in
\mathbb{R}^{m \times 1}$, and noise $e \sim \mathcal{N}(0, \sigma^2)$, it is
formulated as the following linear least squares problem
\[ \hat{\beta} = \arg \min_\beta || X . \beta - y ||_2^2 \]&lt;/p&gt;

&lt;h3 id=&quot;when-x-is-full-rank&quot;&gt;When $X$ is full rank&lt;/h3&gt;
&lt;p&gt;When $X$ is full rank, then we can just solve the normal equation, by solving
the first-order optimality condition of the least-squares problem, 
\[ X^T . (X . \hat{\beta} - y) = 0 \]
which gives the solution
\[ \hat{\beta} = (X^T.X)^{-1} . X^T.y \]
This requires $X$ to be full rank so that $X^T.X$ is invertible.&lt;/p&gt;

&lt;h3 id=&quot;when-x-is-rank-deficient&quot;&gt;When $X$ is rank deficient&lt;/h3&gt;
&lt;p&gt;If $X$ doesn’t have full rank, then we don’t have a unique answer $\hat{\beta}$.
Instead we have an entire linear space of solution. That is given any solution
$\hat{\beta}$, then any non-trivial vector $x$ in the null space of $X$, $\hat{\beta} + x$
will still be a solution; the fact that such a non-trivial $x$ exists is due to
the fact that $X$ is rank deficient.
All of this matters if you care about $\hat{\beta}$, as it doesn’t have a unique
value anymore.&lt;/p&gt;

&lt;p&gt;But if all you care about is transforming a given $X$ into a $y$, then that’s no
problem. All you need is to fine a (any) solution $\hat{\beta}$. To do this, we
typically rely on the
&lt;a href=&quot;https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse#Linear_least-squares&quot;&gt;pseudo-inverse&lt;/a&gt;.
Let’s see why. When $X$ is rank-deficient, we can still write its singular value
decomposition, $X = U.\Sigma.V^\star$, where $U \in \mathbb{R}^{n \times n},
\Sigma \in \mathbb{R}^{n \times m}, V\in\mathbb{R}^{m \times m}$. 
$U, V$ are unitary matrices.
$\Sigma$
is a diagonal matrix that contains the singular values (typically written in
decreasing order). Since $X$ is rank-deficient, some of these singular values
are zero. Let’s assume $rank(X) = r$. Then the reduced SVD is obtained by
keeping the first $r$ columns of $U$ and $V$, and keeping only the first $r$
singular values of $\Sigma$. We write this as $X = U_r . \Sigma_r . V_r^\star$,
where $U_r \in \mathbb{R}^{n\times r}, V_r \in \mathbb{R}^{m \times r}$ and
$\Sigma \in \mathbb{R}^{r \times r}$. Replacing this reduced SVD into the
least-squares formulation, we can left multiply by a unitary matrix $U^\star$.
Due to the orthogonality of the columns of $U$, we get
\[ \arg \min_\beta || \Sigma_r . V_r^\star . \beta - U_r^\star . y ||^2 + 
|| - U_{n-r}^\star . y ||^2 \]
Since the second term does not depend on $\beta$, it won’t impact the value of
$\beta$ such that we can remove it. Also, we can left-multiply by
$\Sigma_r^{-1}$ which is invertible since all singular values in $\Sigma_r$ are
non-zero. We get
\[ \arg \min_\beta || V_r^\star . \beta - \Sigma_r^{-1} . U_r^\star . y ||^2 \]
And we’re done. Well, almost. Now we see where the non-unicity lies. The term
$V_r^\star . \beta$ is uniquely defined by this equation. And the least-squares
equation will be minimum (actually zero) when
\[ V_r^\star . \beta = \Sigma_r^{-1} . U_r^\star . y \]
However, $\beta$ is not uniquely defined. Indeed, let’s assume we know a
solution $\hat{\beta}$ to the equation above. Then adding any vector of the form
$v = V_{m-r} . z$, we still get a solution. 
We can do though is to look for a special solution, and typically we look for
the solution with the minimum norm. First, we can easily verify that
\[ \hat{\beta} = V_r . \Sigma_r^{-1} . U_r^\star . y \]
is a solution to the minimization problem above, as it will lead to a zero norm.
Now this is the unique solution in the subspace described by the $V_r$. But any
other solution of the form $\hat{\beta} + V_{m-r} . z$ will have a larger norm,
since $V_r$ and $V_{m-r}$ are orthogonal to each other.
So we found the minimum norm solution. It is often written in terms of the
&lt;a href=&quot;https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse#Singular_value_decomposition_(SVD)&quot;&gt;pseudo-inverse&lt;/a&gt; of $X$, i.e., $\hat{\beta} = X^+ . y$, where $X^+ = V . \Sigma^+
. U^\star$, where $\Sigma^+$ is a diagonal matrix with all singluar values
inverted, except when they were zero in which case they are left unchanged. We
can easily verify that the reduced form of the pseudo-inverse is indeed $V_r .
\Sigma_r^{-1} . U_r^\star$.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Prediction interval vs Confidence interval</title>
   <link href="https://bcrestel.github.io/2020/10/05/Prediction_interval_vs_Confidence_interval/"/>
   <updated>2020-10-05T00:00:00-04:00</updated>
   <id>https://bcrestel.github.io/2020/10/05/Prediction_interval_vs_Confidence_interval</id>
   <content type="html">&lt;p&gt;Rob Hyndman has a blog &lt;a href=&quot;https://robjhyndman.com/hyndsight/intervals/&quot;&gt;post&lt;/a&gt;
where he details the difference between confidence interval, prediction
interval, and credible interval.&lt;/p&gt;

&lt;p&gt;I think he makes an good point that confidence interval and prediction interval
are often used inter-changeably, even they are quite different. Confidence
interval comes from the realm of frequentist inference, and applies to a
parameter that has been evaluated using a statistical method. A 95% confidence
interval is an interval that will contain the true value of that parameter 95%
of the time, if we could repeat the experiment indefinitly.&lt;/p&gt;

&lt;p&gt;A prediction interval is an interval for a predicted value. Not for a model
parameter. That predicted value does not exist yet (that’s why we need to
predict it), and its uncertainty is represented by a random variable. A 70%
prediction interval will contain 70% of the mass of that random variable; or to
be more general, this would be the 70% HDI.&lt;/p&gt;

&lt;p&gt;A credibility interval is kind of a mix between the two. It’s the Bayesian
equivalent of a confidence interval, and therefore applies to the posterior
distribution of a model parameter. A 75% credibility interval is the 75% HDI of
the posterior distribution.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Pass environment variables in your ssh session</title>
   <link href="https://bcrestel.github.io/2020/08/11/ssh-env/"/>
   <updated>2020-08-11T00:00:00-04:00</updated>
   <id>https://bcrestel.github.io/2020/08/11/ssh-env</id>
   <content type="html">&lt;p&gt;You can pass local environment variables to your ssh session, and even use those
to define environment variables inside your ssh session. For that, you need to
execute a command which requires the flag &lt;code&gt;-t&lt;/code&gt; prior to your server address. An
example,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ssh -p 2222 -A -t myname@myserver &quot;SERVER_ENV_VAR=$LOCAL_ENV_VAR; export SERVER_ENV_VAR; bash -l&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After that, you should see an environemnt variable called &lt;code&gt;SERVER_ENV_VAR&lt;/code&gt;
inside your ssh session.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>rst in pycharm</title>
   <link href="https://bcrestel.github.io/2020/08/04/rst_pycharm/"/>
   <updated>2020-08-04T00:00:00-04:00</updated>
   <id>https://bcrestel.github.io/2020/08/04/rst_pycharm</id>
   <content type="html">&lt;p&gt;In order to render rst files in pycharm, you need to set up some environment
variables, otherwise you’ll get the error &lt;code&gt;NameError: Cannot find docutils in selected interpreter.&lt;/code&gt;
In short, you need to add&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export LC_ALL=en_US.UTF-8  
export LANG=en_US.UTF-8
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;to your &lt;code&gt;.bash_profile&lt;/code&gt;. You can then restart pycharm and it should work.&lt;/p&gt;

&lt;p&gt;Ref: &lt;a href=&quot;https://stackoverflow.com/questions/55522176/pycharm-and-rst-nameerror-cannot-find-docutils-in-selected-interpreter&quot;&gt;Stackoverflow&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Pycharm with ssh interpreter</title>
   <link href="https://bcrestel.github.io/2020/07/23/pycharmssh/"/>
   <updated>2020-07-23T00:00:00-04:00</updated>
   <id>https://bcrestel.github.io/2020/07/23/pycharmssh</id>
   <content type="html">&lt;p&gt;If you have a Docker image running on a remote server, you can set up PyCharm to
use the python interpreter in that image locally.
To do so, you go to &lt;code&gt;Preferences &amp;gt; Project &amp;gt; Project Interpreter&lt;/code&gt;. You then
select the &lt;code&gt;SSH Interpreter&lt;/code&gt; option. Then you need to set up your connection,
indicate the right port.
There is more on this in this
&lt;a href=&quot;https://www.jetbrains.com/help/pycharm/configuring-remote-interpreters-via-ssh.html&quot;&gt;article&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Anyway, once this all done properly, I was still having some issue with running
&lt;code&gt;pytest&lt;/code&gt; with this ssh interpreter. &lt;code&gt;pytest&lt;/code&gt; would point to my local path. I had
to define a path mapping between my local path and my remote path. To do so, you
go to &lt;code&gt;Preferences &amp;gt; Project &amp;gt; Project Interpreter&lt;/code&gt;. Below the project
interpreter, you see &lt;code&gt;Path mappings&lt;/code&gt;, and you can define one from your local to
your remote. After that, &lt;code&gt;pytest&lt;/code&gt; should be able to find your test.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Installing cmake 3.17.3 inside a Docker image</title>
   <link href="https://bcrestel.github.io/2020/07/21/cmake/"/>
   <updated>2020-07-21T00:00:00-04:00</updated>
   <id>https://bcrestel.github.io/2020/07/21/cmake</id>
   <content type="html">&lt;p&gt;Installation of xgboost requires cmake of version at least 3.17.3. And apt-get
was only installing cmake 3.5.x, or something like that. A nice solution is
described in this &lt;a href=&quot;https://askubuntu.com/a/865294&quot;&gt;post&lt;/a&gt;. I picked the second
approach, from the binary. This is the code I included in my Dockerfile:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;RUN mkdir /opt/cmake &amp;amp;&amp;amp; \
    cd /opt/cmake &amp;amp;&amp;amp; \
    wget https://github.com/Kitware/CMake/releases/download/v3.17.3/cmake-3.17.3-Linux-x86_64.sh &amp;amp;&amp;amp; \
    bash cmake-3.17.3-Linux-x86_64.sh --skip-license &amp;amp;&amp;amp; \
    ln -s /opt/cmake/bin/cmake /usr/local/bin/cmake &amp;amp;&amp;amp; \
    echo $(cmake --version)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The last line is just to check that it’s working.
Also, note that you can install different versions of &lt;code&gt;cmake&lt;/code&gt;. You can find your
favorite one on their &lt;a href=&quot;https://cmake.org/download/&quot;&gt;website&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Passing environment variables to you Docker build</title>
   <link href="https://bcrestel.github.io/2020/07/21/buildarg/"/>
   <updated>2020-07-21T00:00:00-04:00</updated>
   <id>https://bcrestel.github.io/2020/07/21/buildarg</id>
   <content type="html">&lt;p&gt;It can be useful to pass environment variable from your local environment to
your Docker build. This situation happened when I had to pass pypi keys to
install specific packages.
But since Docker is encapsulated, you need to take a couple
of steps to make it happen. Note that I’m assuming you’re building your Docker
image through a &lt;code&gt;Dockerfile&lt;/code&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;You need to pass the environment variables to your build command using the
flag &lt;code&gt;--build-arg&lt;/code&gt;. For instance,
    &lt;pre&gt;&lt;code&gt;docker build --build-arg DOCKER_ENV_VAR=$MY_LOCAL_ENV_VAR -f Dockerfile -t my_image:my_tag .
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;You need to define these variables in your Dockerfile. Continuing on the
previous example, this would mean adding the following line in your
&lt;code&gt;Dockerfile&lt;/code&gt;:
    &lt;pre&gt;&lt;code&gt;ARG DOCKER_ENV_VAR
...
RUN apt-get install &amp;lt;something&amp;gt; --flag=$DOCKER_ENV_VAR
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</content>
 </entry>
 
 <entry>
   <title>pip-tools</title>
   <link href="https://bcrestel.github.io/2020/07/02/pip-tools/"/>
   <updated>2020-07-02T00:00:00-04:00</updated>
   <id>https://bcrestel.github.io/2020/07/02/pip-tools</id>
   <content type="html">&lt;p&gt;There are a few ways to manage dependencies: conda, poetry, pipenv. I recently
discovered a different way, &lt;a href=&quot;https://github.com/jazzband/pip-tools&quot;&gt;pip-tools&lt;/a&gt;.
It’s actually very easy to use and in particular easy to integrate with a docker
image. You simply create a requirements.in file which pip-compile converts to a
requirements.txt file that you can then pip install inside your image by doing
&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;There are multiple comparisons of poetry, pipenv, and pip-tools out there,
including &lt;a href=&quot;https://pythonspeed.com/articles/pipenv-docker/&quot;&gt;this one&lt;/a&gt; that
compares specifically in the context of combining with docker, and &lt;a href=&quot;https://hynek.me/articles/python-app-deps-2018/#pip-tools--everything-old-is-new-again&quot;&gt;that
one&lt;/a&gt;
that did a dec 2019 update and still declares pip-tools the winner. I also found
that &lt;a href=&quot;https://alexwlchan.net/2017/10/pip-tools/&quot;&gt;blog post&lt;/a&gt; useful as it shows
a quick example of how to write a requirements.in.&lt;/p&gt;

&lt;p&gt;You can install &lt;code&gt;pip-tools&lt;/code&gt; through pip, &lt;code&gt;pip install pip-tools&lt;/code&gt;. The only
things you need to be careful with are the python version and OS you use to
convert you requirements.in file to requirements.txt file. These needs to be the
same as what you’ll use for your virtual environment. With Docker, this can be
controlled by applying &lt;code&gt;pip-tools&lt;/code&gt; inside a running container, then re-building
that image.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Bayesian Networks</title>
   <link href="https://bcrestel.github.io/2020/04/14/bn/"/>
   <updated>2020-04-14T00:00:00-04:00</updated>
   <id>https://bcrestel.github.io/2020/04/14/bn</id>
   <content type="html">&lt;p&gt;Bayesian Networks are probabilistic graphical models that offer a convenient,
compact way of representing joint probability distribution. A Bayesian Network
consists of a Directed Acyclic Graph (DAG) that connects different parameters
(node), each edge indicating a dependence (an edge from node A to node B if
the variable A helps explain B). Each node (random variable) is associated a
distribution in the form of a Conditional Probability Distribution (CPD), the
condition being on all the parent of that node. By definition, Bayesian Networks
do not contain cycles. Which is not the case of Markov Random Fields. For that
reason, Bayesian Networks are most often used when one tries to understand a
causal relationship between the variables.&lt;/p&gt;

&lt;p&gt;The construction of a Bayesian Networks involve at least 2 steps:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Generating the structure of the DAG (i.e., what nodes are connected and in
what direction). That is what the &lt;a href=&quot;https://papers.nips.cc/paper/8157-dags-with-no-tears-continuous-optimization-for-structure-learning.pdf&quot;&gt;DAGs with NO
TEARS&lt;/a&gt;
algorithm does, in an efficient wayi (along with
&lt;a href=&quot;https://github.com/xunzheng/notears&quot;&gt;code&lt;/a&gt;).&lt;/li&gt;
  &lt;li&gt;Estimating the CPDs. This can be done by MLE or Bayesian estimation.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The website for the Quantum Black library causalnex contains a brief
&lt;a href=&quot;https://causalnex.readthedocs.io/en/latest/04_user_guide/04_user_guide.html#&quot;&gt;introduction to Bayesian
Networks&lt;/a&gt;.
A longer, mode in-depth explanation can be found in this Stanford class on
&lt;a href=&quot;https://ermongroup.github.io/cs228-notes/&quot;&gt;Probabilistic Graphical Models&lt;/a&gt;.
An in-between solution might be to look at the slides for these two
presentations &lt;a href=&quot;http://www.ee.columbia.edu/~vittorio/Lecture12.pdf&quot;&gt;1&lt;/a&gt; and
&lt;a href=&quot;http://www.cs.tau.ac.il/~haimk/pgm-seminar/Graphicals-tomer.pdf&quot;&gt;2&lt;/a&gt;.
For sequential or temporal models, Dynamic Bayesian Networks were developped.
Kevin Muprhy has a &lt;a href=&quot;https://www.cs.ubc.ca/~murphyk/Papers/dbntalk.pdf&quot;&gt;tutorial&lt;/a&gt;
on his webpage.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>CausalNex, a toolkit for causal reasoning</title>
   <link href="https://bcrestel.github.io/2020/04/07/causalnex/"/>
   <updated>2020-04-07T00:00:00-04:00</updated>
   <id>https://bcrestel.github.io/2020/04/07/causalnex</id>
   <content type="html">&lt;p&gt;&lt;a href=&quot;https://causalnex.readthedocs.io/en/latest/index.html&quot;&gt;CausalNex&lt;/a&gt; is a library
developped by QuantumBlack to facilitate the causal analysis of a dataset.  At
its root, CausalNex relies on Bayesian Networks.&lt;br /&gt;
For more on Bayesian Networks, have a look at
&lt;a href=&quot;https://en.wikipedia.org/wiki/Bayesian_network&quot;&gt;Wikipedia&lt;/a&gt; and a
&lt;a href=&quot;https://www.cs.ubc.ca/~murphyk/Bayes/bnintro.html&quot;&gt;tutorial&lt;/a&gt; by Kevin Murphy.
The training of these Bayesian
Networks (causal inference) uses the algorithm introduced in the paper &lt;a href=&quot;https://arxiv.org/pdf/1803.01422.pdf&quot;&gt;DAGs
with NO TEARS&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;installation&quot;&gt;Installation&lt;/h1&gt;

&lt;p&gt;The &lt;a href=&quot;https://causalnex.readthedocs.io/en/latest/02_getting_started/02_install.html&quot;&gt;documentation&lt;/a&gt; is pretty clear.
The library can be easily installed by doing &lt;code&gt;pip install causalnex&lt;/code&gt;.
Note that for some reason (not clear to me), we can’t install via &lt;code&gt;poetry&lt;/code&gt;.
Also, &lt;code&gt;causalnex&lt;/code&gt; requires &lt;code&gt;pandas=0.24.0&lt;/code&gt;, which seems to be a problem with the
current project.&lt;/p&gt;

&lt;p&gt;Last, but not least, &lt;code&gt;causalnex&lt;/code&gt; requires the library &lt;code&gt;pygraphviz&lt;/code&gt; which has to
be installed separately. And of course, &lt;code&gt;pip install pygraphviz&lt;/code&gt; returns an
error. I ended up having to install everything but &lt;code&gt;causalnex&lt;/code&gt; via &lt;code&gt;conda&lt;/code&gt; the
&lt;code&gt;pip install causalnex&lt;/code&gt;. But this may not be convenient for everyone, and it’s
weird that the library is so finicky.&lt;/p&gt;

&lt;h1 id=&quot;tutorial&quot;&gt;Tutorial&lt;/h1&gt;

&lt;p&gt;The documentation contains (for now) a single
&lt;a href=&quot;https://causalnex.readthedocs.io/en/latest/03_tutorial/03_tutorial.html#&quot;&gt;tutorial&lt;/a&gt;
that I will go through.
The first thing we need to do is download the
&lt;a href=&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/00320/student.zip&quot;&gt;dataset&lt;/a&gt;, and unzip it.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Jupyter Extensions</title>
   <link href="https://bcrestel.github.io/2020/03/31/nbextension/"/>
   <updated>2020-03-31T00:00:00-04:00</updated>
   <id>https://bcrestel.github.io/2020/03/31/nbextension</id>
   <content type="html">&lt;p&gt;Jupyter Notebook offer really neat extensions that can honestly transform your
experience working with notebooks.&lt;/p&gt;

&lt;h1 id=&quot;how-to-install&quot;&gt;How to install&lt;/h1&gt;
&lt;p&gt;First step is to install. There are different ways (&lt;code&gt;conda&lt;/code&gt;, &lt;code&gt;pip&lt;/code&gt;,
&lt;code&gt;poetry&lt;/code&gt;,…). You can check out the documentation
&lt;a href=&quot;https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/install.html&quot;&gt;here&lt;/a&gt;.
When you install &lt;code&gt;jupyter_contrib_nbextensions&lt;/code&gt;, it will automatically install
&lt;code&gt;jupyter_nbextensions_configurator&lt;/code&gt; (see
&lt;a href=&quot;https://github.com/Jupyter-contrib/jupyter_nbextensions_configurator&quot;&gt;here&lt;/a&gt;),
which provides a nice GUI to enable/disable the extensions.&lt;/p&gt;

&lt;p&gt;The whole process is pretty easy, but there are 2 actions that you need to take
before having the luxury of enjoying all the goodies:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Activate the configurator
    &lt;pre&gt;&lt;code&gt;jupyter nbextensions_configurator enable --user
&lt;/code&gt;&lt;/pre&gt;
    &lt;p&gt;then&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Activate the extensions
    &lt;pre&gt;&lt;code&gt;jupyter contrib nbextension install --user
&lt;/code&gt;&lt;/pre&gt;
    &lt;p&gt;I’m honestly not sure of the order. I did in that order, but maybe it doesn’t
matter.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;what-extensions&quot;&gt;What extensions?&lt;/h1&gt;
&lt;p&gt;A few useful extensions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;table of content&lt;/li&gt;
  &lt;li&gt;collapsible headings&lt;/li&gt;
  &lt;li&gt;move selected cells&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Article about AI companies</title>
   <link href="https://bcrestel.github.io/2020/02/24/aharticle/"/>
   <updated>2020-02-24T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2020/02/24/aharticle</id>
   <content type="html">&lt;p&gt;Andreessen Horowitz published a great
&lt;a href=&quot;https://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software/&quot;&gt;article&lt;/a&gt;
that is getting everyone in the AI space to talk about. I’ll post some comments
later, but I just want to bookmark this one for now.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Running a jupyter notebook on a remote server</title>
   <link href="https://bcrestel.github.io/2019/10/21/remotenotebook/"/>
   <updated>2019-10-21T00:00:00-04:00</updated>
   <id>https://bcrestel.github.io/2019/10/21/remotenotebook</id>
   <content type="html">&lt;p&gt;Super simple, but because I have the memory of a squirrel I need to mark it
down. So when you have session running on a remote server, you can start a
jupyter notebook on that server. The catch is that you need to specify the ip of
that remote server, otherwise you won’t connect to the server but locally. If
you are in bash session on that server, you can do&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;jupyter notebook --ip=$(hostname -I)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you want to start the notebook directly, without connecting to bash first,
you can do something like&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;server exec sessionid&amp;gt; -- bash -c &apos;jupyter notebook --ip=$(hostname -I)&apos;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When running inside a Docker image, you need to take a few more steps. First,
you need to publish the &lt;code&gt;8888&lt;/code&gt; port of your machine, i.e.,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -it -p 8888:8888 -v &amp;lt;...&amp;gt; image:version /bin/bash
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then inside your container&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;jupyter notebook --ip=$(hostname -I) --allow-root
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then on my laptop, the second url was the one that worked
(&lt;code&gt;http://127.0.0.1:888/?token=...&lt;/code&gt;).&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>How-to Poetry</title>
   <link href="https://bcrestel.github.io/2019/07/31/poetry/"/>
   <updated>2019-07-31T00:00:00-04:00</updated>
   <id>https://bcrestel.github.io/2019/07/31/poetry</id>
   <content type="html">&lt;p&gt;Poetry is a way to manage virtual environment in Python, a bit similar to
Anaconda.
The way it is being using in t-s is that there is a &lt;code&gt;pyproject.toml&lt;/code&gt; file and a
lock file in the root of the repo, which means you can just &lt;code&gt;poetry install&lt;/code&gt; to
create the virtualenv. By default, the “extras” dependencies will not be
installed. To install those, you instaed need to do &lt;code&gt;poetry install --extras &quot;&amp;lt;name of
package&amp;gt;&quot;&lt;/code&gt;; this is equivalent to doing &lt;code&gt;poetry install&lt;/code&gt; and on top of that
installing the extra dependencies requested.
When you update the version of certain dependencies, you can update you poetry
environment by doing &lt;code&gt;poetry install&lt;/code&gt;.
To add new dependencies without modifying the &lt;code&gt;pyproject.toml&lt;/code&gt;, you can do
&lt;code&gt;poetry add &amp;lt;name_of_dependency&amp;gt;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Once you’re all set up, you can run some commands inside your virtual
environment. For instance, to run &lt;code&gt;ipython&lt;/code&gt;, you would do&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;poetry run ipython
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To start a jupyter notebook session, you would do&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;poetry run jupyter-notebook
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Their &lt;a href=&quot;https://poetry.eustace.io/docs/cli/&quot;&gt;documentation&lt;/a&gt; is pretty good.&lt;/p&gt;

&lt;h1 id=&quot;how-to-add-a-dependency&quot;&gt;How to add a dependency&lt;/h1&gt;
&lt;p&gt;The first time you create a &lt;code&gt;pyproject.toml&lt;/code&gt; file and you run &lt;code&gt;poetry install&lt;/code&gt;,
poetry will resolve all the conflicts and save the version of each dependency in
a lock file, &lt;code&gt;poetry.lock&lt;/code&gt;. You should version control both files.&lt;/p&gt;

&lt;p&gt;If you want to add a new dependency, add it in the &lt;code&gt;poetry.toml&lt;/code&gt; file then run
&lt;code&gt;poetry install&lt;/code&gt;, which will update the lock file and commit both.&lt;/p&gt;

&lt;p&gt;Because &lt;code&gt;poetry&lt;/code&gt; resolves conflicts for you, you will not necessarily have, in
your lock file, the latest verison of all dependencies as requested in your
&lt;code&gt;pyproject.toml&lt;/code&gt; file. If you want to update your dependencies, you need to run
&lt;code&gt;poetry update&lt;/code&gt;, which will effectively delete your lock file and installing
again.&lt;/p&gt;

&lt;p&gt;Note that sometimes, adding directly into the &lt;code&gt;pyproject.toml&lt;/code&gt; file doesn’t work
(SolverProblemError…version solving failed) even though &lt;code&gt;poetry&lt;/code&gt; should be
able to find it. The workaround (which is a
&lt;a href=&quot;https://github.com/python-poetry/poetry/issues/1281&quot;&gt;bug&lt;/a&gt;) is to install that
dependency via &lt;code&gt;poetry add &amp;lt;dependency&amp;gt;&lt;/code&gt;. This let &lt;code&gt;poetry&lt;/code&gt; add that dependency
to the &lt;code&gt;.toml&lt;/code&gt; file then resolves conflicts in the &lt;code&gt;.lock&lt;/code&gt; file.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Bias adjustment in Box-Cox transformation</title>
   <link href="https://bcrestel.github.io/2019/07/29/boxcox/"/>
   <updated>2019-07-29T00:00:00-04:00</updated>
   <id>https://bcrestel.github.io/2019/07/29/boxcox</id>
   <content type="html">&lt;p&gt;The &lt;a href=&quot;https://otexts.com/fpp2/transformations.html&quot;&gt;Box-Cox transformation&lt;/a&gt; is a parametric transformation that includes the
logarithmic transfomration as a special case, and is defined as&lt;/p&gt;

\[w_t = \left\{
\begin{aligned}
&amp;amp; \log(y_t) , \text{ if } \lambda = 0 \\
&amp;amp; \frac{(y_t)^\lambda - 1}{\lambda} , \text{ otherwise}
\end{aligned} \right.\]

&lt;p&gt;Quick node: the inclusion of the log-transformation is justified by the fact
that $(y_t)^\lambda \approx 1 + \lambda \log(y_t)$ when $\lambda \rightarrow 0$.&lt;/p&gt;

&lt;p&gt;Assuming you get better distribution with $w_t$, you can run your inference on
that variable. But once this is done, you still need to revert it back to the
quantity of interest, that is $y_t$. The inverse Box-Cox transformation is given
by&lt;/p&gt;

\[y_t = \left\{
\begin{aligned}
&amp;amp; \exp(w_t) , \text{ if } \lambda = 0 \\
&amp;amp; \left(\lambda w_t + 1 \right)^{1/\lambda} , \text{ otherwise}
\end{aligned} \right.\]

&lt;p&gt;However, one needs to be careful about the distribution of the inverted
prediction~$y_t$. 
A general (non-parametric) way of handling this, and the way chosen by
&lt;a href=&quot;https://robjhyndman.com/hyndsight/backtransforming/&quot;&gt;Hyndman&lt;/a&gt;, is to do a
Taylor expansion around the mean. That is, calling $f$ the inverted Box-Cox
transformation, and calling $\mu$ and $\sigma^2$ the mean and variance of the
transformed variable $w_t$, we would have&lt;/p&gt;

&lt;p&gt;\(y_t \approx f(w_t) = f(\mu) + (w_t - \mu) f&apos;(\mu) + \frac12 (w_t - \mu)^2
f&apos;&apos;(\mu).\) Then taking the mean of that expression, we get
\(\mathbb{E}[y_t] = f(\mu) + \frac12 \sigma^2 f&apos;&apos;(\mu).\)&lt;/p&gt;

&lt;p&gt;However in the special case of the log-transformation, and if the
transformed variable $w_t$ was assumed to be normal (large class of models will
make that assumption when calculating the uncertainty around the mean), we can
simply use the results of a
&lt;a href=&quot;https://en.wikipedia.org/wiki/Log-normal_distribution&quot;&gt;log-Normal&lt;/a&gt;, which tells
us that (1) $\exp(w_t)$ is the median of $y_t$, and (2) the mean of $y_t$ is
given by $\exp(\mu + \sigma^2/2)$. In the case $\sigma^2 \ll 1$, we recover the
expression derived from the Taylor expression.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Bayesian inference in Deep Learning</title>
   <link href="https://bcrestel.github.io/2019/05/10/swa/"/>
   <updated>2019-05-10T00:00:00-04:00</updated>
   <id>https://bcrestel.github.io/2019/05/10/swa</id>
   <content type="html">&lt;p&gt;It is possible to perform full-on Bayesian inference in Deep Learning. One could
train a network, assume some sort of Gaussian distribution around the weights
that were found, and the likelihood comes directly from the loss function
since, in general, that loss function is derived from MLE. However, this is
costly, and in practice it’s not clear whether people do that or not.&lt;/p&gt;

&lt;p&gt;There has been some attempts at using variational inference, i.e., approximating
the posterior with a simpler distribution (e.g., Gaussian), by looking for the
candidate distribution that minimizes the Kullback-Leibler divergence between
the posterior and the candidate distribution. It also seems to be
computationally expensive.&lt;/p&gt;

&lt;p&gt;The most popular option is to use Monte-Carlo Dropout (MC Dropout) at inference
time (see &lt;a href=&quot;https://arxiv.org/pdf/1506.02142.pdf&quot;&gt;here&lt;/a&gt; and
&lt;a href=&quot;https://www.cs.ox.ac.uk/people/yarin.gal/website/PDFs/DLW_ICML_2015_dropout_bayesian_poster.pdf&quot;&gt;here&lt;/a&gt;).
The idea is to generate samples of the solution of the NN by randomly shutting
down a certain numbers of cells for each forward propagation. The author proves
some interesting properties of their methods.&lt;/p&gt;

&lt;p&gt;More recently, Stochastic Weight Averaging (SWA) was introduced (see
&lt;a href=&quot;https://arxiv.org/pdf/1803.05407.pdf&quot;&gt;paper&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/&quot;&gt;blog
post&lt;/a&gt;). 
SWA is not, so to speak, a new optimization algorightm, it simply modifies the
typical learning rate strategy (high learning rate, following by continuously
decreasing rate, until a very slow learning rate) to always maintain a
relatively large learning rate and continue to explore the loss function. And
after the training procedure, the latest weights, i.e., corresponding to the
exploration phase, are averaged out. That average is designed to position the
SWA weight in the middle of a large flat region (the authors assume that NN
minima are in large flat portions of the loss function), instead of close to the
edge (typical optim, e.g., SGD), leading to better generalization properties.
You have the option, in the exploration phase, to only includes a regular
sub-set of the weights.
There is potential issue when SWA is issued in conjunction with batch
normalization; because batch normalization learns the statistics of the
activation during training, but the SWA weights were never used during that
phase. The solution proposed by the authors is to re-pass the training set
through the network after training (i.e., with SWA weights), and re-calculate
the BN statistics at that time.&lt;/p&gt;

&lt;p&gt;The authors also have an extension called &lt;a href=&quot;https://arxiv.org/abs/1902.02476&quot;&gt;SWA-Gaussian&lt;/a&gt; (SWAG) that allows to
carry uncertainty quantification by estimating the first 2 moments of the weight
distribution, again based on snapshots of the exploration phase.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Installing pyomo</title>
   <link href="https://bcrestel.github.io/2019/03/01/pyomo/"/>
   <updated>2019-03-01T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2019/03/01/pyomo</id>
   <content type="html">&lt;h2 id=&quot;to-install-it&quot;&gt;To install it&lt;/h2&gt;
&lt;p&gt;As often, the easiest way to install it was to use anaconda. It is even
documented in the
&lt;a href=&quot;https://pyomo.readthedocs.io/en/latest/installation.html#using-conda&quot;&gt;pyomo&lt;/a&gt;
documentation.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Tracking notebooks with git</title>
   <link href="https://bcrestel.github.io/2019/03/01/git-notebooks/"/>
   <updated>2019-03-01T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2019/03/01/git-notebooks</id>
   <content type="html">&lt;p&gt;The main challent of tracking notebooks with git is that every time you
re-generate an output, git will keep track of that. It makes each commit large,
and hard to keep track of.
Fortunately, there is a script that removes the output prior to your commit. I’m
using &lt;a href=&quot;https://pypi.org/project/nbstripout/&quot;&gt;nbstripout&lt;/a&gt;. You can turn it on by
doing, inside your git repo,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nbstripout --install
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and turn it off by doing&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nbstripout --install
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It works great. Unfortunately, it doesn’t behave super well when switching to
other branches where nbstripout was not applied; basically, it removes the
output from every notebook it fines, and then requires you to &lt;code&gt;git commit&lt;/code&gt;
before you can switch out of that branch. Not great. So instead, I apply it
manually to the notebook that I want to commit. You can do so by typing&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nbstripout FILE.ipynb
&lt;/code&gt;&lt;/pre&gt;

</content>
 </entry>
 
 <entry>
   <title>Note on Brownian motion</title>
   <link href="https://bcrestel.github.io/2019/02/04/BM/"/>
   <updated>2019-02-04T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2019/02/04/BM</id>
   <content type="html">&lt;p&gt;I just want to summarize a few results that I find useful when dealing with
arithmetic and geometric Brownian motions.
For simplicity, I looked  at unitary time steps. If it’s not the case (e.g., daily
rate of change, looked hourly), then some minor adjustment needs to be done.&lt;/p&gt;

&lt;h2 id=&quot;arithmetic-brownian-motion&quot;&gt;Arithmetic Brownian motion&lt;/h2&gt;

&lt;p&gt;The SDE for the ABM is given by
\(dI_t = \mu I_0 d_t + \sigma I_0 dW_t\)&lt;/p&gt;

&lt;p&gt;The solution is then
\(I_d = I_0 (1 + \mu d + \sigma W_d)\)
where $W_d$ is a Brownian motion. This means the following moments for the
quantity $I_d$,&lt;/p&gt;

\[\begin{aligned}
\mathbb{E}[I_d] &amp;amp; = I_0 (1 + \mu d) \\
\text{Var}[I_d] &amp;amp; = \sigma^2 I_0^2 d \\
\text{Cov}[I_d, I_{d&apos;}] &amp;amp; = \sigma^2 I_0^2 \min(d,d&apos;) \\
\end{aligned}\]

&lt;h2 id=&quot;geometric-brownian-motion&quot;&gt;Geometric Brownian motion&lt;/h2&gt;

&lt;p&gt;You can find some info from Wikipedia, 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Geometric_Brownian_motion&quot;&gt;here&lt;/a&gt;
and 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Log-normal_distribution&quot;&gt;here&lt;/a&gt;.
The SDE for GBM is
\(dI_t = \mu I_t d_t + \sigma I_t dW_t\).
Then the solution is
\(I_d = I_0 \exp \left( (\mu - \sigma^2/2)d + \sigma W_d \right)\).
This means the following moments for the
quantity $I_d$,&lt;/p&gt;

\[\begin{aligned}
\mathbb{E}[I_d] &amp;amp; = I_0 e^{\mu d} \\
\text{Var}[I_d] &amp;amp; = I_0^2 e^{2\mu d}(e^{\sigma^2 d} - 1) \\
\text{Cov}[I_d, I_{d&apos;}] &amp;amp; = I_0^2 e^{\mu(d+d&apos;)} \left( e^{\sigma^2 \min(d,d&apos;)}
-1 \right)
\end{aligned}\]

&lt;p&gt;For the covariance, you can derive it as&lt;/p&gt;

\[\begin{aligned}
\text{Cov}[I_d, I_{d&apos;}] &amp;amp; = \mathbb{E}[I_d I_{d&apos;}] - \mathbb{E}[I_d]
\mathbb{E}[I_{d&apos;}] \\
&amp;amp; = I_0^2 e^{(\mu-\sigma^2/2)(d+d&apos;)} \mathbb{E}[e^{\sigma(W_d + W_{d&apos;})}] - 
I_0^2 e^{\mu(d+d&apos;)}
\end{aligned}\]

&lt;p&gt;And the last part is equal to $\mathbb{E}[e^Y]$ where $Y$ is 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Log-normal_distribution&quot;&gt;log-normal&lt;/a&gt; 
with
parameters $\log Y \sim \mathcal{N}(0, \sigma^2(d+d’+2\min(d,d’)))$, such that
$\mathbb{E}[e^Y] = \exp[\sigma^2/2(d+d’+2\min(d,d’)]$ and the result follows.&lt;/p&gt;

&lt;p&gt;It is interesting to convert the variables $\mu, \sigma$ into the empirical
moments of the quantity $I_d$. This is&lt;/p&gt;

\[\begin{aligned}
\mu &amp;amp; = \frac1d \log \frac{\mathbb{E}[I_d]}{I_0} \\
\sigma^2 &amp;amp; = \frac1d \log \left( 1 + \frac{\text{Var}[I_d]}{I_0^2 e^{2 \mu d}} \right)
\end{aligned}\]
</content>
 </entry>
 
 <entry>
   <title>Fast AI's Pytorch tutorial</title>
   <link href="https://bcrestel.github.io/2019/01/28/nn_tutorial/"/>
   <updated>2019-01-28T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2019/01/28/nn_tutorial</id>
   <content type="html">&lt;p&gt;Fast AI wrote a really nice pytorch
&lt;a href=&quot;https://pytorch.org/tutorials/beginner/nn_tutorial.html&quot;&gt;tutorial&lt;/a&gt;
which starts from a very manual
implementation of a one hidden layer neural network trained on MNIST data, then
gradually adds on pytorch built-in capabilities.
It nicely shows off what pytorch can do, and how it simplifies the coding.
The different steps involve using:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;torch.nn.functional&lt;/code&gt;: provides already built-in function for most common activation 
functions and loss functions.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;nn.Module&lt;/code&gt;: provides a class to define a neural network that can be inherited
 when defining your own NN.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;nn.Linear&lt;/code&gt;: already defines a linear layer (also have convolution
layers,….)&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;optim&lt;/code&gt;: provides a broad selection of optimizer that can be used to train your
 neural network. You still need to compute the gradient yourself, and zero out
the gradient after the update step.
The parameters of the NN are passed to the optimizer when being instantiated,
which, in addition to telling the optimizer what parameters to optimize, also
provide the optimizer with gradient information.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Dataset&lt;/code&gt;: provides a convenient way to manipulate datasets; allows to slice
train and test sets together.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;DataLoader&lt;/code&gt;: manages mini-batches automatically; you just give it a Dataset and
a batch size.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;conv2d&lt;/code&gt;: pytorch has a lot of layers already built-in that can be directly 
used when building an object of type &lt;code&gt;nn.Module&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;nn.Sequential&lt;/code&gt;:  it is another way of defining a model, instead of &lt;code&gt;nn.Module&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Literature on GANs</title>
   <link href="https://bcrestel.github.io/2019/01/14/gan/"/>
   <updated>2019-01-14T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2019/01/14/gan</id>
   <content type="html">&lt;p&gt;GANs are a popular types of generative models that attemps to approximate a
probabilty distribution. It does so by setting up a game between 2 agents, one
that is tasked with generating samples (the generator), and one that is tasked
with deciding whether samples generated are real or fake (the discriminator).
The tyical setup uses deep nets as both generator and discriminator, and each
have different cost functions. They are trained by alternating optimization
steps on each.&lt;/p&gt;

&lt;p&gt;One criticism is that the distribution learned by GANs have a very-low support,
and therefore only learn a low-dimensional subset of the target distribution. This is
discussed in &lt;a href=&quot;https://arxiv.org/abs/1706.08224&quot;&gt;Do GANs actually learn the distribution? An empirical
study&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There exists different architecture for GANs. In &lt;a href=&quot;http://papers.nips.cc/paper/7350-are-gans-created-equal-a-large-scale-study&quot;&gt;Are GANs Created Equal? A
Large-Scale
Study&lt;/a&gt;,
the authors compare different GANs and find no consistent, meaningful difference
among those. They also propose some ways to compare GANs; another paper in that
direction is &lt;a href=&quot;http://papers.nips.cc/paper/7769-assessing-generative-models-via-precision-and-recall&quot;&gt;Assessing Generative Models via Precision and
Recall&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This
&lt;a href=&quot;https://towardsdatascience.com/gan-ways-to-improve-gan-performance-acf37f9f59b&quot;&gt;article&lt;/a&gt;
discusses a few workaround to some of the biggest pitfalls of GANs, namely:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;mode collapse&lt;/li&gt;
  &lt;li&gt;non-convergence (oscillation)&lt;/li&gt;
  &lt;li&gt;vanishing gradient (when discriminator is over-confident)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Unfortunately, I find most of the solutions to be ad-hoc solutions, like duct
tape, not really trying to address the underlying problems.
This article still has some merit, at least as a nice concise summary of the
current state of research (e.g., summarize all cost functions used in GANs).&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Literature on Deep Learning</title>
   <link href="https://bcrestel.github.io/2019/01/14/dl/"/>
   <updated>2019-01-14T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2019/01/14/dl</id>
   <content type="html">&lt;h1 id=&quot;regularization&quot;&gt;Regularization&lt;/h1&gt;

&lt;p&gt;In &lt;a href=&quot;https://arxiv.org/pdf/1710.10686.pdf&quot;&gt;Regularization for Deep Learning: A
Taxonomy&lt;/a&gt;, the authors list and classify a
large number of regularization techniques for DNN (as of October 2017).&lt;/p&gt;

&lt;p&gt;One way Deep Learning avoids over-fitting is by building sparsity.
The former can be achieved with $L_1$ regularization, or with a post hoc
pruning, i.e., removing some neurons from the network after training is
complete. This pruning can be completely random, but most likely will be
targeted provided some sort of metric (magnitude of weights, gradient,…). In
&lt;a href=&quot;https://openreview.net/pdf?id=HkghWScuoQ&quot;&gt;Targeted Dropout&lt;/a&gt;, the authors
observe that dropout also promote sparity (in the sense of small numbers of high
activations; see &lt;a href=&quot;https://wiki.tum.de/display/lfdv/Dropout#Dropout-EffectonSparsity&quot;&gt;here&lt;/a&gt;), 
and they propose to dropout, with a
higher probability, neurons that would be pruned in the post-training stage.&lt;/p&gt;

&lt;p&gt;Dropout typically doesn’t work as well for CNN. In &lt;a href=&quot;http://papers.nips.cc/paper/8271-dropblock-a-regularization-method-for-convolutional-networks&quot;&gt;DropBlock: A regularization method for convolutional networks
&lt;/a&gt;,
the authors postulate that this is due to the spatial correlation between
neurons in a CNN, and propose to drop units in a spatially correlated manner
(DropBlock). They report better results.&lt;/p&gt;

&lt;p&gt;A similar issue applied to LSTM-based networks, for which (traditional) dropout doesn’t work. In
&lt;a href=&quot;https://arxiv.org/pdf/1409.2329.pdf&quot;&gt;Recurrent neural network regularization&lt;/a&gt;,
the authors introduce a modified way of applying dropout to network with LSTM
cells; the key is to apply dropout only to non-recurrent connections, i.e.,
connections between different layers of LSTM cells.&lt;/p&gt;

&lt;h1 id=&quot;optimization&quot;&gt;Optimization&lt;/h1&gt;

&lt;p&gt;In &lt;a href=&quot;http://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization.pdf&quot;&gt;Adaptive Methods for Nonconvex
Optimization&lt;/a&gt;,
the authors study convergence properties of scaled gradient-based methods, and
highlight the benefit of gradually increasing the mini-batch size during
training.&lt;/p&gt;

&lt;h1 id=&quot;network-understanding&quot;&gt;Network understanding&lt;/h1&gt;

&lt;p&gt;In &lt;a href=&quot;https://arxiv.org/pdf/1810.01075.pdf&quot;&gt;Implicit Self-Regularization in Deep Neural
Networks&lt;/a&gt;, the authors try to understand
why DNN work so well and do not overfit by applying ranomd matrix theory to the
eigenstructure of the last 2 layers of wide range of (fully connected) popular
networks. Their findings include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;DNN are self-regularizing&lt;/li&gt;
  &lt;li&gt;self-regularize in different ways depending on whether it’s an old network
 (Tikhonov-like) or a more modern architecture (heaviy-tailed
self-regularization).&lt;/li&gt;
  &lt;li&gt;connect batch size with self-regularzation properties (small batch-size are
 better self-regularizing).&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Fitting power models</title>
   <link href="https://bcrestel.github.io/2019/01/09/nlrpower/"/>
   <updated>2019-01-09T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2019/01/09/nlrpower</id>
   <content type="html">&lt;p&gt;Market impact is often defined as relating the price difference to the volume
(or POV) traded, i.e.,
\(S_T - S_0 = k v_T + \varepsilon\)
That model assumes a normal price dynamics, which may or may not make sense
depending on your time scale, but this could easily modify to assume a
log-normal price dynamics by using the difference of the log of the prices
instead.&lt;/p&gt;

&lt;p&gt;However, another very common approach is to assume a power law for the market
impact, i.e., something like
\(S_T - S_0 = k v_T^\alpha + \varepsilon\)
Now comes the questions of fitting that model. And this is what this post is
about.&lt;/p&gt;

&lt;p&gt;There are 2 approaches to go about that:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;you could fit directly using nonlinear regression techniqes.&lt;/li&gt;
  &lt;li&gt;you could fit instead the log of that expression
\(\log(S_T - S_0) = \log(k) + \alpha \log(v_T) + \varepsilon\)
However, those two expressions are not equivalent, primarily because of their
assumptions on the noise distributions. This is something that is explained in
the introduction of that
&lt;a href=&quot;https://doi.org/10.1890/11-0538.1&quot;&gt;paper&lt;/a&gt;
(note that the conclusions of that paper are heavily criticized by that other
&lt;a href=&quot;https://doi.org/10.1111/bij.12396&quot;&gt;paper&lt;/a&gt;).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You can estimate the change in the noise variance when applying method 2 by
using the &lt;a href=&quot;/2018/05/04/deltameth&quot;&gt;Delta method&lt;/a&gt;. This is also discussed into that
stackexchange
&lt;a href=&quot;https://math.stackexchange.com/questions/3625/easy-to-implement-method-to-fit-a-power-function-regression&quot;&gt;question&lt;/a&gt;.
In the most relevant answer, the suggestion is to first fit using the
log-transform, then use the coefficients obtained with that method to start a
nonlinear regression solved using Newton’s method.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Intro to pytorch</title>
   <link href="https://bcrestel.github.io/2019/01/07/torch/"/>
   <updated>2019-01-07T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2019/01/07/torch</id>
   <content type="html">&lt;p&gt;I want to use this post to summarize a few important facts about using pytorch
to build and train deep neural nets.&lt;/p&gt;

&lt;h2 id=&quot;building-a-neural-net&quot;&gt;Building a neural net&lt;/h2&gt;

&lt;p&gt;First a general note: pytorch uses
&lt;a href=&quot;https://pytorch.org/docs/stable/tensors.html&quot;&gt;tensors&lt;/a&gt; as its main dataframe.
For most operations, tensors can be manipulated very much like a numpy array. But
tensors can be set to a specific device (e.g., GPU), and tensors have certain
data types that need to be understood. By default, tensors are single precision
(i.e., &lt;code&gt;torch.Tensor&lt;/code&gt; corresponds to &lt;code&gt;torch.FloatTensor&lt;/code&gt;–see &lt;a href=&quot;https://pytorch.org/docs/master/tensors.html&quot;&gt;here&lt;/a&gt;);
this is considered fine to train neural networks, but this will create some
situation when trying to test the gradient (see below).
You can find a summary of pytorch tensors basic functionalities
&lt;a href=&quot;https://www.kdnuggets.com/2018/05/pytorch-tensor-basics.html&quot;&gt;here&lt;/a&gt;.
There is also an interesting &lt;a href=&quot;/2019/01/28/nn_tutorial/&quot;&gt;tutorial&lt;/a&gt; put together by FastAI which builds a
simple NN in Python, then gradually adds on pytorch capabilities to simplify and
clarify the code; this allows you to see what each component of pytorch do.&lt;/p&gt;

&lt;p&gt;A few quick notes about pytorch:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;a trailing &lt;code&gt;_&lt;/code&gt; indicates the operator is performed in-place.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You build a neural net by inheriting the torch class &lt;code&gt;Module&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;forward&quot;&gt;Forward&lt;/h3&gt;
&lt;p&gt;At it most basic, you only need to define the method &lt;code&gt;forward(self, x)&lt;/code&gt; which
defines the forward propagation of your neural net. Typically, all function(al)s
you need to build your deep net are defined in the constructor, i.e. &lt;code&gt;def
__init__(self)&lt;/code&gt;.
To stack the layers of your neural net, you propagate the input variable
(typically &lt;code&gt;x&lt;/code&gt;) from one layer to the next one, and return it in the end.
Torch provides default function(al)s to define each layer (convolutional, rnn,
lstm, pooling, activation functions,…). 
The main reference for all those commands is provided in the pytorch
&lt;a href=&quot;https://pytorch.org/docs/stable/nn.html&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Below is an example that defines the AlexNet,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
	# 2d convolutional layer that takes 3 input images (3-colors)
	# returns 6 images,
	# and apply a convolution kernel of size 5x5
        self.conv1 = nn.Conv2d(3, 6, 5)
	# max pooling layer which only keeps max value in 2x2 squares (subsampling)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
	# bunch of linear layers (y = Wx + b)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
	# apply convolution kernel,
	# apply pointwise ReLU function
	# and max pooling
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
	# change shape to go from multiple small images,
	# to one long vector
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


net = Net()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;parameters-of-the-network&quot;&gt;Parameters of the network&lt;/h3&gt;
&lt;p&gt;The parameters are stored in the iterator &lt;code&gt;net.parameters()&lt;/code&gt;. You can convert
that to a list and see that the length of the list is equal to the number of
layers x 2, since you have parameters for the weights (&lt;code&gt;weight&lt;/code&gt;, $W$) and the biases
(&lt;code&gt;bias&lt;/code&gt;, $b$), and those parameters are stored separately. Typically, the parameters are
ordered, for each layer, as weights first, bias second.
To inialize weights, you can either do it by hand (with the &lt;code&gt;data&lt;/code&gt; method of
the &lt;code&gt;weight&lt;/code&gt; or &lt;code&gt;bias&lt;/code&gt; components of each layer), or use one of default
functions provided in pytorch (see
&lt;a href=&quot;https://discuss.pytorch.org/t/how-are-layer-weights-and-biases-initialized-by-default/13073&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id=&quot;loss-function&quot;&gt;Loss function&lt;/h3&gt;
&lt;p&gt;First, note the typical distinction in ML between cost function and loss
function. The cost function is the average of the loss function over all
training examples (or all training examples in that batch). The loss function is
how you measure the performance of your network against the labeled data.&lt;/p&gt;

&lt;p&gt;The loss function is defined independently from the neural net. 
I think the separation is motivated by the fact that the loss function does not
have any parameters to be trained.
Pytorch comes with a large variety of loss functions, e.g., the cross-entropy,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;criterion = nn.CrossEntropyLoss()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;an-optimizer&quot;&gt;An optimizer&lt;/h3&gt;
&lt;p&gt;Same as for the loss function, you can use one of the many optimizers provided
by pytorch, e.g., stochastic gradient descent with momentum,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;net&lt;/code&gt;, here, is the convolutional neural network we defined above. So you only
need to pass the parameters of the net to the optimizer. But, these parameters
contain also the gradient information (once calculated).&lt;/p&gt;

&lt;p&gt;Note that for some optimizers (e.g., BFGS), you need to do something more
complicated (see &lt;a href=&quot;https://pytorch.org/docs/stable/optim.html&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&quot;train-the-network&quot;&gt;Train the network&lt;/h2&gt;

&lt;h3 id=&quot;basic-procedure&quot;&gt;Basic procedure&lt;/h3&gt;
&lt;p&gt;Once we have defined a neural net, a loss function, and an optimizer, we can
start training the network. To do so, we need (1) training data, (2)
derivatives. You set up the iteration over the training data set. But once you
have a mini-batch, you need to&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;compute the loss
    &lt;pre&gt;&lt;code&gt;outputs = net(data)
loss = criterion(outputs, labels)
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;calculate the gradient
    &lt;pre&gt;&lt;code&gt;loss.backward()
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;apply one step of the optimizer:
    &lt;pre&gt;&lt;code&gt;optimizer.step()
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;looking-at-the-gradient&quot;&gt;Looking at the gradient&lt;/h3&gt;
&lt;p&gt;You can look at the gradient for each layer, either directly&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;net.conv1.bias.grad
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or through the parameters (here for the bias of the first layer),&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ii = 0
for x in net.parameters():
    if ii == 1:
        print(x.grad.data)
    ii += 1
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;gradient-check&quot;&gt;Gradient check&lt;/h3&gt;
&lt;p&gt;As an exercise, I decided to check the gradient of the neural net defined in the
pytorch’s &lt;a href=&quot;https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py&quot;&gt;CIFAR10
tutorial&lt;/a&gt;,
which is some variation of &lt;a href=&quot;https://en.wikipedia.org/wiki/AlexNet&quot;&gt;AlexNet&lt;/a&gt;.
To set the values of a layer, you can do&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mynet.layer_nb_x.&amp;lt;bias or weight&amp;gt;.data = pytorch.tensor(....,device=same_device_as_net)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So far, I actually only checked the gradient wrt the bias of the first
convolutional layer. I compare against a finite-difference approximation
(one-directional). The results are in the corresponding jupyter notebook stored
on Borgy
(/mnt/home/bencrestel/pytorch/tutorials/beginner_source/blitz/cifar10_tutorial.ipynb).
To my great suprise, the gradient checks rather well when using double precision, but
checks very poorly when using single precision.&lt;/p&gt;

&lt;p&gt;I was wondering whether I was
doing the right thing, but I found a &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/5351&quot;&gt;bug
report&lt;/a&gt; in the pytorch repo
where some user reports trouble with gradient check. That issue is answered by
one of the developpers by switching to double precision.
Another, more explicit and more convincing report that single precision should
be avoided for gradient check was found in Stanford’s online course on CNN for
visual recognition. In the &lt;a href=&quot;http://cs231n.github.io/neural-networks-3/#gradcheck&quot;&gt;gradient
checks&lt;/a&gt;, they warn the
readers to use double precision.
So it seems well accepted that in single precision, the gradient will not check.
Now on the bright side, looking at the results in single precision, it seems the
problem comes from the finite-difference check, not the analytical gradient,
which sorts of make sense.&lt;/p&gt;

&lt;h3 id=&quot;no_grad&quot;&gt;no_grad&lt;/h3&gt;
&lt;p&gt;You sometimes see code where they wrap some code, often layers or value updates,
inside a with statement&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;w1 = torch.tensor(...)
with torch.no_grad():
    w1 -= learning_rate * grad_w1
    ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is necessary to tell torch to not track the layer update in the next
backpropagation step. However, you do not need to do that if you access the
weights of a layer through the &lt;code&gt;data&lt;/code&gt; method,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;net.layer1.weights.data = ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indeed, as explained in the comments of that
&lt;a href=&quot;https://github.com/pytorch/tutorials/blob/master/beginner_source/examples_autograd/two_layer_net_autograd.py&quot;&gt;code&lt;/a&gt;,
&lt;code&gt;tensor.data&lt;/code&gt; gives a tensor that shares the storage with tensor, but doesn’t
track history.&lt;/p&gt;

&lt;h3 id=&quot;train-vs-eval&quot;&gt;train vs eval&lt;/h3&gt;
&lt;p&gt;You need to specify in what mode the network is, either &lt;code&gt;train&lt;/code&gt; (when training) 
or &lt;code&gt;eval&lt;/code&gt; (when you’re done with training and want to use the network). This is
used, for instance, by some types of layers like &lt;code&gt;BatchNorm&lt;/code&gt; or &lt;code&gt;Dropout&lt;/code&gt;, that
behave differently in training and evaluation phases.&lt;/p&gt;

&lt;h2 id=&quot;using-cuda&quot;&gt;Using CUDA&lt;/h2&gt;
&lt;p&gt;You need to explicitly transfer your data structures (neural net,…) onto the
GPU, using the &lt;code&gt;.to()&lt;/code&gt;command. To transfer the neural net, you do&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;gpu0 = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
net.to(gpu0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You also need to transfer the input data onto the GPU if you want to do that,
e.g.,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;inputs, labels = inputs.to(gpu0), labels.to(gpu0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that when applied to tensors &lt;code&gt;.to(gpu0)&lt;/code&gt; creates a copy (unlike when applied to
the model).
This is described at the end of the &lt;a href=&quot;https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py&quot;&gt;CIFAR10
tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Another good reference for all CUDA matters is the
&lt;a href=&quot;https://pytorch.org/docs/stable/notes/cuda.html&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A first interesting observation with that CIFAR10 dataset is that for a given
number of epochs, smaller mini-batches (e.g., default of 4) lead to higher
accuracy, but will take longer to train than, for instance with mini-batches of
64 pics. However, since each epoch is faster with larger mini-batches, a more
fair comparision should be for a fixed run time, in which case it seems larger
mini-batches win (for that specific application).&lt;/p&gt;

&lt;h3 id=&quot;using-more-than-1-gpu&quot;&gt;Using more than 1 GPU&lt;/h3&gt;
&lt;p&gt;Now by default, pytorch will only use 1 GPU. If you want to use more than 1 GPU,
you need to use
&lt;a href=&quot;https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html#sphx-glr-beginner-blitz-data-parallel-tutorial-py&quot;&gt;DataParallel&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Recurrent Neural Networks</title>
   <link href="https://bcrestel.github.io/2019/01/03/RNN/"/>
   <updated>2019-01-03T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2019/01/03/RNN</id>
   <content type="html">&lt;h2 id=&quot;lstm&quot;&gt;LSTM&lt;/h2&gt;

&lt;p&gt;RNNs suffer from memory problems. The solution is to use gated neuron units,
i.e, neurons that have a complex systems of masks/filters/… that process the
information flowing in. A classical first read on the subject is &lt;a href=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;Colah’s blog
post&lt;/a&gt;.
The main difference with a typical cell is the presence, in addition to the
input and the hidden state, of a cell state, intended to capture the long-term
memory of the network. That cell state is then modified through 3 gates:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;forget gate layer: applies a pointwise multiplication to the cell state
with the output of a sigmoid, therefore deciding what values of the cell state to let flow.&lt;/li&gt;
  &lt;li&gt;input gate layer: same step as above except that this will be applied to the
 output of a tanh cell, then added (pointwise) to the output of the forget gate
layer.&lt;/li&gt;
  &lt;li&gt;output gate layer: decides what is output as a hidden state. The cell state is
 untouched after the forget and input gate layers, but the hidden state will be
passed through a tanh (between -1 and 1), then filtered by a sigmoid (pointwise
multliplication).
So in short, a LSTM unit decides what to keep from the cell state, how to update
some of the entries of the cell state, then from that state state what to output
as the hidden state.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There exists of course variants:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;peepholes: each or some of the gates can have direct access to the cell state&lt;/li&gt;
  &lt;li&gt;couple forget &amp;amp; input: only update (input) the entries you forgot (weights for
 forget and input are one minus the other)&lt;/li&gt;
  &lt;li&gt;GRU: It combines the above idea (combine forget and input) with the&lt;/li&gt;
  &lt;li&gt;combination of cell state and hidden state.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Notes on convex optimization</title>
   <link href="https://bcrestel.github.io/2018/12/21/convex/"/>
   <updated>2018-12-21T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2018/12/21/convex</id>
   <content type="html">&lt;h2 id=&quot;identify-convex-constraints&quot;&gt;Identify convex constraints&lt;/h2&gt;

&lt;h3 id=&quot;inequality-constraints&quot;&gt;Inequality constraints&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;If $f$ is convex, the constraint $f(x) \leq a \in \mathbb{R}$ is convex.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Take $x_1,x_2$ that satisfy that constraint and $\alpha \in [0,1]$, 
àthen if $f$ is convex, we have
\(f(\alpha x_1 + (1-\alpha) x_2) \leq \alpha f(x_1) + (1-\alpha) f(x_2) \leq
a\).&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Setting up my MacBook Pro</title>
   <link href="https://bcrestel.github.io/2018/12/04/setupMac/"/>
   <updated>2018-12-04T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2018/12/04/setupMac</id>
   <content type="html">&lt;p&gt;I received a Mac for work. It’s beautiful, it’s fancy, and I have no idea how to use it. I’m going to summarize here the steps I followed to set up my laptop, in particular getting git, python, and a compile environment.&lt;/p&gt;

&lt;h1 id=&quot;git&quot;&gt;Git&lt;/h1&gt;
&lt;p&gt;First up, &lt;strong&gt;git&lt;/strong&gt; of course. Mac does not ship with an equivalent of apt-get, but you can install one of several package managers. I went for &lt;a href=&quot;https://docs.brew.sh/Installation&quot;&gt;Homebrew&lt;/a&gt;. 
I didn’t want to have to install it in sudo mode (not sure this was such a big deal, in the end), so I decided to install it in my personal folder (/Users/local). 
But to make it possible to execute the softwares installed via Homebrew from any directory, I had to add the path to the directory where I installed Homebrew to my PATH. 
This is done by modifying the file paths&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo vim /etc/paths
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Also, during the install, I had to install the command-line tools from XCode. This part was handled automatically by Mac App Store.
Once Homebrew was installed, I updated and upgraded&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;brew update
brew upgrade
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With Homebrew installed, I can next install git with a simple command&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;brew install git
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This installation of git ships with a bunch of other app, like gitk. The latter requires to have jdk installed, which I did from the website (can’t find the link now).&lt;/p&gt;

&lt;p&gt;Next, I need autocomplete to make it work. For that you can simply install bash-completion&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;brew install bash-completion
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then you need to point you bashrc file to that command.&lt;/p&gt;

&lt;h3 id=&quot;update-2021-02-17&quot;&gt;Update 2021-02-17&lt;/h3&gt;

&lt;p&gt;I didn’t have to change the file &lt;code&gt;/etc/paths&lt;/code&gt;. I directly changed &lt;code&gt;$PATH&lt;/code&gt; in my &lt;code&gt;.zshrc&lt;/code&gt; file. I suppose the modification of the &lt;code&gt;/etc/paths&lt;/code&gt; is more solid, but so far I haven’t run into any complications.
For the auto-completion, I added&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;autoload -Uz compinit &amp;amp;&amp;amp; compinit
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;to my &lt;code&gt;.zshrc&lt;/code&gt; file.
To add the git branch in my terminal prompt (btw, &lt;code&gt;brew install iterm&lt;/code&gt;), I added&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;source /Users/bencrestel/Work/other/zsh_setup/zsh-git-prompt/zshrc.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;after cloning the repo &lt;a href=&quot;https://github.com/olivierverdier/zsh-git-prompt&quot;&gt;zsh-git-prompt&lt;/a&gt;. I fine-tuned the aspect of the prompt and ended up with&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;PROMPT=&apos;%B%F{blue}%n%f@%F{green}%m%f:%F{red}%~%b%f$(git_super_status) %# &apos;
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&quot;docker&quot;&gt;Docker&lt;/h1&gt;
&lt;p&gt;Still using Homebrew, I could install &lt;strong&gt;docker&lt;/strong&gt;. However to have the nice Docker GUI with it, 
I had to use cask,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;brew cask install docker
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;update-2021-02-17-1&quot;&gt;Update 2021-02-17&lt;/h3&gt;

&lt;p&gt;To specify a cask, now you need to do&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;brew install --cask docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I also installed the desktop app.&lt;/p&gt;

&lt;h1 id=&quot;jekyll&quot;&gt;Jekyll&lt;/h1&gt;
&lt;p&gt;Jekyll is nice to power simple, efficient blogs. You install via Ruby, and the instructions provided &lt;a href=&quot;https://jekyllrb.com/docs/installation/macos/&quot;&gt;here&lt;/a&gt; were sufficient, except for a few jekyll modules (jekyll-gist, jekyll-seo-tag,…) which I had to install using gem again. But it worked in the end.&lt;/p&gt;

&lt;h3 id=&quot;update-2021-02-17-2&quot;&gt;Update 2021-02-17&lt;/h3&gt;

&lt;p&gt;This time I installed ruby 3.0, which seems to work a bit differently. The first steps in the above link are still required, but next to install missing dependencies (&lt;code&gt;webrick&lt;/code&gt;, &lt;code&gt;kramdown-parser-gfm&lt;/code&gt;, &lt;code&gt;jekyll-watch&lt;/code&gt;,…), I had to use &lt;code&gt;bundle add &amp;lt;...&amp;gt;&lt;/code&gt;. This install the missing dependencies locally, only for your project. The only piece that I was missing was a &lt;code&gt;Gemfile&lt;/code&gt;; you can simply create a text file with that name and add the single line &lt;code&gt;source &quot;https://rubygems.org&quot;&lt;/code&gt;.
Then everytime you do &lt;code&gt;bundle add &amp;lt;...&amp;gt;&lt;/code&gt;, it adds a new line to that &lt;code&gt;Gemfile&lt;/code&gt; with that new dependency.&lt;/p&gt;

&lt;h1 id=&quot;pipenv&quot;&gt;pipenv&lt;/h1&gt;
&lt;p&gt;Next, and still using Homebrew, I installed &lt;strong&gt;pipenv&lt;/strong&gt;, which seems to be a nice lightweight environment manager that can be useful for software development. A nice little intro &lt;a href=&quot;https://pipenv.readthedocs.io/en/latest/&quot;&gt;video&lt;/a&gt; is posted on that website.&lt;/p&gt;

&lt;h1 id=&quot;pyenv&quot;&gt;pyenv&lt;/h1&gt;
&lt;p&gt;Pyenv is meant to be a simpler way to define environments. I installed via Homebrew. Then to create a Python 3.6 environment, you do&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pyenv init
pyenv install 3.6.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note: It never worked for me&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>How to get started with Docker</title>
   <link href="https://bcrestel.github.io/2018/12/01/docker/"/>
   <updated>2018-12-01T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2018/12/01/docker</id>
   <content type="html">&lt;h2 id=&quot;running-a-docker-image&quot;&gt;Running a Docker image&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.docker.com/&quot;&gt;Docker&lt;/a&gt; is way to run a specific application without
having to install it, or compile it and deal with all the dependencies.  In
Ubuntu, it was very easy to install the docker software from the command line.
On Mac, you can look at that &lt;a href=&quot;/2018/12/04/setupMac&quot;&gt;post&lt;/a&gt;.  Once this is done,
you need to identify the image you want to run. A Docker image is a container a
certain set of applications. To run that image, you can type in the command line
&lt;code&gt;docker run &amp;lt;image&amp;gt;&lt;/code&gt; If you have never run that image before, the first
time you execute that command, docker will download the image and all other
stuff it needs. If you want to download the image without running it, you can
instead do &lt;code&gt;docker pull &amp;lt;image&amp;gt;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;A few options make the whole Docker experience a lot more useful. In
particular, you typically want that image to be opened in an interactive shell.
For that, you need the options &lt;code&gt;-it&lt;/code&gt;. Another useful feature is to be able
to access some folders of your local hard drive from within the image; this can
be done with the option &lt;code&gt;-v LOCAL_FOLDER:DOCKER_FOLDER&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;For example, when running the Fenics Docker image, I would do&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -t -i -v /home/ben/Work/fenicstools:/home/fenics/fenicstools -v /home/ben/Work/hippylib:/home/fenics/hippylib fenics20171
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;to have access to my local &lt;code&gt;fenicstools&lt;/code&gt; and &lt;code&gt;hippylib&lt;/code&gt; folders.
In another example, to run a tensorflow Docker image, I did&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -t -i -v
/home/ben/Work/Programmation/Python/mlds/tensorflow/:/home/tf/
tensorflow/tensorflow bash
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The bash was required here as by default the
&lt;a href=&quot;https://www.tensorflow.org/install/docker&quot;&gt;tensorflow&lt;/a&gt; image starts a notebook.
Actually you start in the &lt;code&gt;/notebook&lt;/code&gt; folder, and need to navigate to the
folder you defined &lt;code&gt;cd ../home/tf&lt;/code&gt;. But once you figure this out, everything
works great.&lt;/p&gt;

&lt;h3 id=&quot;running-a-jupyter-notebook-using-a-docker-image&quot;&gt;Running a jupyter notebook using a Docker image&lt;/h3&gt;

&lt;p&gt;I found the solution in this &lt;a href=&quot;https://stackoverflow.com/questions/38830610/access-jupyter-notebook-running-on-docker-container&quot;&gt;StackOverflow&lt;/a&gt; post. First you need to publish a port of the container to the host (your laptop),&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run &amp;lt;...&amp;gt; -p 8888:8888 &amp;lt;your_image&amp;gt; bash
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inside your Docker image, you can start the jupyter notebook,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;jupyter-notebook --ip 0.0.0.0 --no-browser --allow-root
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now on your local machine, from your browser, navigate to &lt;code&gt;localhost:8888/tree&lt;/code&gt;. 
You will be prompted with a menu asking for a token. After starting the jupyter notebook, 
you’ll get a http adress which contains the sequence &lt;code&gt;:8888/?token=&amp;lt;...&amp;gt;&lt;/code&gt;. 
Your token is made of all the alphanumeric characters following the equal sign.&lt;/p&gt;

&lt;h2 id=&quot;creating-a-docker-image&quot;&gt;Creating a Docker image&lt;/h2&gt;
&lt;p&gt;You can save all the characteristics of the image you want to create in a &lt;a href=&quot;https://docs.docker.com/engine/reference/builder/&quot;&gt;Dockerfile&lt;/a&gt;, then build the corresponding image by doing, if you’re in the same directory as the Dockerfile,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker build -t &amp;lt;name&amp;gt;:&amp;lt;tag&amp;gt; .
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For instance &lt;code&gt;docker build -t local/ben:latest .&lt;/code&gt;.
You can also specify the modules you want installed in a Pipfile that you load as part of your Docker file, then install with &lt;code&gt;pipenv&lt;/code&gt;. In that case, you need to first generate the lock file, then install all the modules. For instance,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Python dependencies
RUN pip install pipenv==2018.11.26
ADD ./Pipfile ./
RUN pipenv lock
RUN pipenv install --system
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Another way to build the image which may be more flexible is to use a &lt;code&gt;docker-compose.yml&lt;/code&gt; file 
(see &lt;a href=&quot;https://docs.docker.com/compose/overview/&quot;&gt;here&lt;/a&gt;) and execute it through&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker-compose up &amp;lt;name_specified_in_yml&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;up&lt;/code&gt;, by default will start the container after building and creating it. To prevent this from happening, you can pass the option &lt;code&gt;--no-start&lt;/code&gt;.
Note: I had a lot of trouble with &lt;code&gt;docker-compose&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;dockerfile-and-absolute-path&quot;&gt;Dockerfile and absolute path&lt;/h2&gt;
&lt;p&gt;Sometimes you want to create a docker image with files that are in a different
directory. However, you can’t do that with docker. When you define an absolute
path inside your Dockerfile, this refers to the absolute path in the build
context. A work-around is to create your docker image from a place where you can
access (in relative path) all the files you need. And if you want your
Dockerfile to be somewhere else, you can use the &lt;code&gt;-f&lt;/code&gt; option,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker build -f /home/Dockerfile -t mytag .
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;pruning-docker&quot;&gt;Pruning Docker&lt;/h2&gt;
&lt;p&gt;Docker have a very conservative approach to garbage
collection, it seems, and keep everything unless you asked it to delete it. The
problem is that if you’re not careful, you can end up filling up all your
available memory, and you can’t build/pull any images. The solution is to either
(1) increase the amount of memory Docker can use, or (2) prune all the
images/containers/… that you don’t need anymore. To
&lt;a href=&quot;https://docs.docker.com/config/pruning/#prune-volumes&quot;&gt;prune&lt;/a&gt; everything at
one, just do&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker system prune --volumes
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>Power iteration for the $k$ dominant eigenvectors</title>
   <link href="https://bcrestel.github.io/2018/11/28/poweriteration/"/>
   <updated>2018-11-28T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2018/11/28/poweriteration</id>
   <content type="html">&lt;p&gt;First of a disclaimer: This post is not an extensive review of state of the art
techniques to compute eigenvalues or eigenvectors of a matrix. I’m just
summarizing a simple result on the power iteration. This being said, I think
it’s fair that I try to motivate the use of power iteration, given how much bad
press this algorithm gets.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Power_iteration&quot;&gt;power iteration&lt;/a&gt; 
is a simple way to compute the dominant eigenvector of a
diagonalizable matrix $A$. It is generally slow to converge. What are the
alternatives. Typically, you could use an eigenvalue-revealing factorizations,
then get the eigenvectors with the
&lt;a href=&quot;https://en.wikipedia.org/wiki/Rayleigh_quotient_iteration&quot;&gt;Rayleigh quotient 
iteration&lt;/a&gt;.
You could even stop the factorization early and refine the eigenvalue and
compute the eigenvector at the same time with the Rayleigh quotient iteration,
which converges at a cubic rate(!).
However, the eigenvalue-revealing factorizations (that I am aware of) all
require access to the entries of the matrix. And one of the steps in the
Rayleigh quotient iteration is an 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Inverse_iteration&quot;&gt;inverse iteration&lt;/a&gt;, 
which involves solving a linear system with the matrix $A$.
All of this to say that in some situations, e.g., if the matrix $A$ is not
assembled and you can only compute a matvec, and/or if the matrix $A$ is very
large and sparse such that the matvec is cheap but the inversion costly, you may
want to rely on the power iteration.&lt;/p&gt;

&lt;p&gt;The algorithm is pretty simple. You sample a random vector $v$, then repeat the
following steps&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;multiply by $A$, i.e., $v = A.v$&lt;/li&gt;
  &lt;li&gt;normalize $v$, i.e., $v = v / | v|$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then $v$ will converge to the dominant eigenvector. Why? Since $A$ is
diagonalizable, its eigenvectors form a basis. Let’s call these eigenvectors
$q_i$ and the corresponding eigenvalues $\lambda_i$. Then we can write any
random vector as \(v = \sum_i (v^T.q_i) q_i\). And then&lt;/p&gt;

\[A^n. v = \sum_i \lambda_i^n (v^T.q_i) q_i\]

&lt;p&gt;After sufficiently many iterations, $A^n . v$ will point toward the dominant
eigenvector. To avoid blowing everything, we normalize $v$ after each step.&lt;/p&gt;

&lt;p&gt;Now the next question is: how to apply power iteration to compute the first $k$
eigenvectors? No problem, we can do that. Let’s think about the second dominant
eigenvector. It will be the dominant eigenvector if we look in the hyperplane
defined by the dominant eigenvector, that is $q_1^\perp$. One idea would be to
first compute $q_1$ using the power iteration, then repeat the same procedure
but projecting $v$ onto $q_1^\perp$ at each step. That would be:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;multiply by $A$, i.e., $v = A.v$&lt;/li&gt;
  &lt;li&gt;project onto $q_1^\perp$, i.e., $v = v - (v^T.q_1)q_1$&lt;/li&gt;
  &lt;li&gt;normalize $v$, i.e., $v = v / | v|$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This algorithm would converge to $q_2$. After doing so, we could repeat the same
procedure but project onto $(q_1,q_2)^\perp$. And so on, so forth. Now, we can
actually do all the steps at the same time. Instead of sampling a single vector,
sample a matrix $V$ with as many columns as you want eigenvectors. Then after
each left-multiplication by $A$, instead of projecting then normalizing, simply
do a &lt;a href=&quot;https://en.wikipedia.org/wiki/QR_decomposition&quot;&gt;QR decomposition&lt;/a&gt;
of $V$ and keep the $Q$ matrix.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;left-multiply by $A$, i.e., $V = A.V$&lt;/li&gt;
  &lt;li&gt;project and normalize with a QR decomposition, i.e., $V=Q$ where $Q,R = QR(V)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That matrix will converge
toward the first $k$ dominant eigenvectors. Here is the code in Python&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import numpy as np
def power_iteration_k(A, k, eps=1e-10):
    &quot;&quot;&quot;
    Inputs:
        A = matrix (symmetric)
        k = nb of eigenvectors to compute
        eps = precision
    Outputs:
        v = matrix of the k dominant eigenvectors
    &quot;&quot;&quot;
    m, n = A.shape
    v = np.random.randn(n*k).reshape((-1,k))
    v,_ = np.linalg.qr(v)
    for kk in range(1000):
        v_old = v.copy()
        v = A.dot(v)
        v,_ = np.linalg.qr(v)
        diff = np.max(np.sqrt(np.sum((v-v_old)**2, axis=0)))
        if diff &amp;lt; eps:
            return v
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>A continuation scheme for RELU activation functions</title>
   <link href="https://bcrestel.github.io/2018/11/26/softplus/"/>
   <updated>2018-11-26T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2018/11/26/softplus</id>
   <content type="html">&lt;p&gt;One of the most popular activation function nowadays is the REctified Linear
Unit (&lt;a href=&quot;https://en.wikipedia.org/wiki/Rectifier_(neural_networks)&quot;&gt;RELU&lt;/a&gt;), 
defined by $f(x) = \max(0,x)$. One of the first obvious criticism is its
non-differentiability at the origin. A smooth approximation is the 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#Softplus&quot;&gt;softplus&lt;/a&gt;
function, $f(x) = log(1 + e^x)$. However, the use of softplus is discouraged by
deep learning experts. I’m wondering if a continuation scheme on the softplus
could help during training. Instead of softplus, we could use a “smooth RELU”
defined as&lt;/p&gt;

\[f_\alpha(x) = \frac1{\alpha} log(1 + e^{\alpha x})\]

&lt;p&gt;And starting with $\alpha = 1$, i.e., the softplus function, we increase
$\alpha$ after each epoch with the effect of smoothly converging toward RELU. In
the plot below, I show RELU, softplus, and 3 examples of smooth RELUs for
$\alpha=2,4,8$.
&lt;img src=&quot;/code/2018-11-26/softplus.png&quot; alt=&quot;softplus&quot; height=&quot;350&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Convolution, cross-correlation</title>
   <link href="https://bcrestel.github.io/2018/11/26/conv/"/>
   <updated>2018-11-26T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2018/11/26/conv</id>
   <content type="html">&lt;p&gt;In this post, I want to summarize a few results about 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Convolution&quot;&gt;convolution&lt;/a&gt;
and/or
&lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-correlation&quot;&gt;cross-correlation&lt;/a&gt;. 
Note that all results are derived for real functions. To work
in the complex space, one needs to take the conjugate where appropriate (e.g, in
the inner product).&lt;/p&gt;

&lt;h2 id=&quot;definitions&quot;&gt;Definitions&lt;/h2&gt;

&lt;p&gt;Let’s not worry about regularity and assume with only work with
$\mathcal{C}^\infty(\mathbb{R})$ 
functions. This out of the way, we define the
&lt;strong&gt;convolution&lt;/strong&gt; of two functions $f,g$ as&lt;/p&gt;

\[\begin{align}
(f \star g)(t) &amp;amp; = \int_{- \infty}^{\infty} f(s) g(t-s) ds \\
&amp;amp; = \int_{- \infty}^{\infty} f(t-s) g(s) ds 
\end{align}\]

&lt;p&gt;The convolution operation commutes, i.e., $f \star g = g \star f$.
Next, we define the &lt;strong&gt;cross-correlation&lt;/strong&gt; of two (real) functions $f,g$ as&lt;/p&gt;

\[\begin{align}
(f \star g)(t) &amp;amp; = \int_{- \infty}^{\infty} f(s) g(t+s) ds \\
&amp;amp; = \int_{- \infty}^{\infty} f(s-t) g(s) ds \\
\end{align}\]

&lt;h2 id=&quot;cross-correlation-is-the-adjoint-operation-of-convolution&quot;&gt;Cross-correlation is the adjoint operation of convolution&lt;/h2&gt;

&lt;p&gt;Before we talk about the adjoint, we need to define (1) what operator we’re
talking about, and (2) what inner product we’re using. Given a function $f$,
let’s define the convolution operator $C_f: \mathcal{C}^\infty(\mathbb{R})
\rightarrow \mathcal{C}^\infty(\mathbb{R})$ as $C_f(g) = f \star g$.
And similarly we define the cross-correlation operator for $f$ as $D_f$.
For the inner-product on $\mathcal{C}^\infty(\mathbb{R})$, we use $\langle f, g
\rangle = \int_{-\infty}^{\infty} f(t) g(t) df$. Then for any $f,g,h$,&lt;/p&gt;

\[\begin{align*}
\langle C_f(g), h \rangle 
&amp;amp;  = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(s) g(t-s) ds \, h(t) dt \\
&amp;amp; = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(t-s) g(s) h(t) \, ds dt \\
&amp;amp; = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(t-s) h(t) dt \, g(s) ds \\
&amp;amp; = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(t) h(t+s) dt \, g(s) ds \\
&amp;amp; = \int_{-\infty}^{\infty} D_f(h)(s) g(s) ds \\
&amp;amp; = \langle g, D_f(h) \rangle
\end{align*}\]

&lt;p&gt;By definition of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Adjoint_operator&quot;&gt;adjoint&lt;/a&gt; 
of an operator, this shows that $C_f^* = D_f$.&lt;/p&gt;

&lt;h2 id=&quot;convolution-in-image-processing&quot;&gt;Convolution in image processing&lt;/h2&gt;

&lt;p&gt;A classical technique for edge detection is to take finite-difference derivatives 
of the discrete image. 
This can also be used for edge sharpening.
In the example below, I plot the logistic function
$\sigma(x)$ (“original”) 
along with the logistic function minus its second derivative, $\sigma(x) -
d^2 \sigma(x)/dx^2$ (“sharpened”).
&lt;img src=&quot;/code/2018-11-26/laplacian.png&quot; alt=&quot;logistic&quot; height=&quot;250&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This can be formulated as taking convolution with a
specific kernel.
First of all, let’s define a discrete convolution. Typically, the scaling
factors coming from discretization are ignored, and since we’re integrating over
the whole real line there is no boundary case, therefore the discrete
convolution between functions defined over the $\mathbb{Z}$ integers, $f,g$, is&lt;/p&gt;

\[(f \star g)[n] = \sum_{m=-\infty}^{\infty} f[m] g[n-m]\]

&lt;p&gt;In the case of the Laplacian operator (in 1D), we can use the kernel $f$ where
all entries are zero except the ones at ${-1,0,1}$ which are equal to $[1, -2,
1]$. In the plot below, I compare the second derivative of the logistic function
with the convolution of the logistic function with the Laplacian kernel
described here. Note that I rescaled the convolution by $1/h^2$ where $h$ is the
grid size (the distance between consecutive evaluation of the function. We see
that both curvs are on top of each other. The convolution with the discrete
Laplacian could therefore
be used for edge sharpening (or edge detection) as we showed in the previous
example.
&lt;img src=&quot;/code/2018-11-26/fd.png&quot; alt=&quot;logistic&quot; height=&quot;250&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>The softmax activation function</title>
   <link href="https://bcrestel.github.io/2018/11/23/softmax/"/>
   <updated>2018-11-23T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2018/11/23/softmax</id>
   <content type="html">&lt;p&gt;Notations are defined in that &lt;a href=&quot;/2018/11/13/deeplearnDIY&quot;&gt;post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A very popular activation function for classification with a deep net is the
softmax, which is defined as&lt;/p&gt;

\[{\bf a}^l = [ \frac{\exp(a^{l-1}_1)}{\sum_{i=1}^{n_{l-1}} \exp(a^{l-1}_i)},
\dots,
\frac{\exp(a^{l-1}_{n_{l-1}})}{\sum_{i=1}^{n_{l-1}} \exp(a^{l-1}_i)} ]^T\]

&lt;p&gt;That is ${\bf a}^l = F^l({\bf a}^{l-1})$ with
$F^l(x) = [f_1(x), \dots, f_{n_l}(x)]^T$ where&lt;/p&gt;

\[f_i(x) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}\]

&lt;p&gt;That type of activation function is different to what was covered in that
&lt;a href=&quot;/2018/11/13/deeplearnDIY&quot;&gt;post&lt;/a&gt; for 2 reasons: (1) it does not have any
parameters to optimize, and (2) each coordinate depends on all other
coordinates. Let’s see how we back-propagate the gradient in that case.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;For the output layer&lt;/strong&gt;, which is the most common use-case for a softmax layer,
we need to modify the chain-rule to skip the output layer and start instead at
layer $L-1$,&lt;/p&gt;

\[\frac{\partial c}{\partial {\bf b}^{L-1}} = 
\frac{\partial {\bf a}^{L-1}}{\partial {\bf b}^{L-1}}
\frac{\partial {\bf a}^{L}}{\partial {\bf a}^{L-1}}
\frac{\partial c}{\partial {\bf a}^{L}}\]

&lt;p&gt;The only term that was not covered previously is \(\frac{\partial {\bf
a}^{L}}{\partial {\bf a}^{L-1}}\) which is simply the gradient of $F^L$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;For a hidden layer&lt;/strong&gt;, which is definitely not the most common case for a
softmax activation function but could the case of another type of function, we
have&lt;/p&gt;

\[\begin{align*}
\frac{\partial c}{\partial {\bf b}^{l-1}} &amp;amp; = 
\frac{\partial {\bf a}^{l-1}}{\partial {\bf b}^{l-1}}
\frac{\partial {\bf a}^{l}}{\partial {\bf a}^{l-1}}
\frac{\partial {\bf z}^{l+1}}{\partial {\bf a}^{l}}
\frac{\partial c}{\partial {\bf z}^{l+1}} \\
&amp;amp; = \frac{\partial {\bf a}^{l-1}}{\partial {\bf b}^{l-1}}
\frac{\partial {\bf a}^{l}}{\partial {\bf a}^{l-1}}
\frac{\partial {\bf z}^{l+1}}{\partial {\bf a}^{l}}
\frac{\partial c}{\partial {\bf b}^{l+1}} \\
\end{align*}\]

&lt;p&gt;Again, the only term we didn’t before is \(\frac{\partial {\bf a}^{l}}{\partial
{\bf a}^{l-1}}\), which is the gradient of $F^l$.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Loss functions</title>
   <link href="https://bcrestel.github.io/2018/11/23/loss-fct/"/>
   <updated>2018-11-23T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2018/11/23/loss-fct</id>
   <content type="html">&lt;p&gt;This post is a simple summary of some common loss functions, and their relations
to maximum likelihood estimation (MLE). 
Given the data \(\{ ({\bf x}_i, {\bf y}_i) \}_i\),
and model parameters $\theta$,
we define a general loss function as&lt;/p&gt;

\[\mathcal{L}(\{ \theta \}) = 
\frac1N \sum_{i=1}^N 
c(\theta; {\bf x}_i, {\bf y}_i)\]

&lt;p&gt;We also assume that that model make the predictions 
\({\bf f}(\theta; {\bf x}_i)\).&lt;/p&gt;

&lt;h2 id=&quot;regression&quot;&gt;Regression&lt;/h2&gt;

&lt;p&gt;In a regression setting, a very common choice is a least-square loss function,
or $L_2$ loss function,
i.e.,&lt;/p&gt;

\[c(\theta; {\bf x}_i, {\bf y}_i) 
= \frac12 \| {\bf f}(\theta; {\bf x}_i) - {\bf y}_i \|^2\]

&lt;p&gt;To calculate the gradient of the loss function, we need the partial derivative
of $c$ with respect to the output of the model $f$, which in this case is&lt;/p&gt;

\[\frac{\partial c(\theta; {\bf x}_i, {\bf y}_i)}{\partial {\bf f}(\theta; {\bf
x}_i)} =  {\bf f}(\theta; {\bf x}_i) - {\bf y}_i\]

&lt;p&gt;The main advantage of that loss function is its simplicity. Linear regression
with that loss function can be solved analytically. The least-square loss
function is twice differentiable everywhere. In the case of a model with
Gaussian noise, it also connects with MLE. Indeed, if we assume that the true
model is&lt;/p&gt;

\[{\bf y}_i = {\bf f}(\theta; {\bf x}_i) + \varepsilon\]

&lt;p&gt;with $\varepsilon$ having a normal distribution with mean zero, then minus the
log-likelihood is given by&lt;/p&gt;

\[\frac{n}2 \log(2 \pi \sigma^2) + \frac1{2\sigma^2} \sum_i 
\| {\bf f}(\theta; {\bf x}_i) - {\bf y}_i \|^2\]

&lt;p&gt;A disadvantage of the least-square loss function is its sensitivity to outliers.
Indeed, if the \(k^\text{th}\) data point is an outlier, the square deviation
will dominate the loss function and during the training phase, the optimization
will give too much importance to that data point in an attempt to minimize the
loss function. This is related to the fact that the least-square targets the
mean of the distribution, which is sensitive to outliers.&lt;/p&gt;

&lt;p&gt;This is in contrast to the $L_1$ loss function, which targets the median the
distribution, and is&lt;/p&gt;

\[c(\theta; {\bf x}_i, {\bf y}_i) 
= | {\bf f}(\theta; {\bf x}_i) - {\bf y}_i |\]

&lt;p&gt;The main disadvantage of the $L_1$ loss function is its numerical difficulties.
It is non-differentiable at the origin, and requires specific techniques to be
handled.&lt;/p&gt;

&lt;h2 id=&quot;binary-classifier&quot;&gt;Binary classifier&lt;/h2&gt;

&lt;p&gt;Typically, loss functions for binary classification derive from MLE. Assuming the
binary classes are \(y_i \in \{0,1\}\), and assuming that the classifier
returns a function \(f({\bf x}_i) \in [0,1]\) which can be interpreted as 
\(\mathcal{P}[f({\bf x}_i) = 1 | \theta]\), then the likelihood function is
given by&lt;/p&gt;

\[\begin{align*}
&amp;amp; \prod_i (f({\bf x}_i))^{\mathcal{1}_{\{y_i=1\}}} 
(1-f({\bf x}_i))^{\mathcal{1}_{\{y_i=0\}}} \\
= &amp;amp; \prod_i (f({\bf x}_i))^{y_i} (1-f({\bf x}_i))^{1-y_i} 
\end{align*}\]

&lt;p&gt;So either $y_i=1$ and we have only the first term, or $y_i=0$ and we have the
second term. 
This is the general way of writing the likelihood function for a Bernoulli
distribution which returns value $1$ with probability $f({\bf x}_i)$ and value
$0$ with probability $1-f({\bf x}_i)$.
Then the negative log-likelihood is&lt;/p&gt;

\[- \sum_i y_i \log(f({\bf x}_i)) + (1-y_i) \log(1-f({\bf x}_i))\]

&lt;p&gt;Let’s look at an example. In the case of a logistic regression, the model is
given by&lt;/p&gt;

\[f({\bf x}_i) = \frac1{1 + e^{-{\bf x}_i^T \theta}}\]

&lt;p&gt;In that case, the negative log-likelihood is&lt;/p&gt;

\[\begin{align*}
&amp;amp; = y_i \log (1 + e^{-{\bf x}_i^T \theta})
-(1-y_i) (-{\bf x}_i^T \theta - \log(1 + e^{-{\bf x}_i^T \theta}) \\
&amp;amp; = - y_i {\bf x}_i^T \theta +  \log(1 + e^{+{\bf x}_i^T \theta}) 
\end{align*}\]

&lt;p&gt;In the case we have multiple classes \(y_i \in \{0,1,\dots,K-1\}\) and multiple
outputs \({\bf f}({\bf x}_i) \in [0,1]^K\), 
and all outputs are mutually exclusive (i.e., they always sum to $1$, as is the
case when using softmax in a neural network), 
we have the interpretation that
\({\bf f}({\bf x}_i)_j = \mathcal{P}({\bf f}({\bf x}_i) = y_j | \theta)\).
The loss function is typically defined as&lt;/p&gt;

\[c(\theta; {\bf x}_i, y_i) = - \sum_{j=0}^K \mathcal{1}_{\{y_i = j\}} \log (
\mathcal{P}({\bf f}({\bf x}_i) = j | \theta) )\]

\[c(\theta; {\bf x}_i, y_i) = - \log( {\bf f}({\bf x}_i)_{y_i} )\]

&lt;p&gt;In that case, the partial derivative of the $c$ with respect to the output of
the model is&lt;/p&gt;

\[\frac{\partial c(\theta; {\bf x}_i, y_i)}{\partial {\bf f}(\theta; {\bf x}_i)} =  
[\dots,0,- 1/{\bf f}({\bf x}_i)_{y_i},0,\dots]^T\]

&lt;p&gt;It is interesting to note that these loss functions are often called
cross-entropy, as they can be derived from information theory.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>DIY Deep Learning</title>
   <link href="https://bcrestel.github.io/2018/11/13/deeplearnDIY/"/>
   <updated>2018-11-13T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2018/11/13/deeplearnDIY</id>
   <content type="html">&lt;p&gt;As a learning tool, and inspired by that interesting
&lt;a href=&quot;https://arxiv.org/abs/1801.05894&quot;&gt;article&lt;/a&gt;, I want to build my own artificial
neural network in Python.&lt;/p&gt;

&lt;h2 id=&quot;notations&quot;&gt;Notations&lt;/h2&gt;

&lt;p&gt;The first step is define all the notations I’m going to use.
&lt;strong&gt;Layers&lt;/strong&gt; are indexed by $l=0,1,\dots,L$. 
The input layer is indexed by $0$, and the output layer is $L$. In each layer $l$,
we have $n_l$ &lt;strong&gt;neurons&lt;/strong&gt;, each producing an output $a^l_i$ for $i=1,\dots,n_l$.
All outputs from layer $l$ are stored in the vector ${\bf a}^l =
[a^l_1,\dots,a^l_{n_l}]^T$.
In layer $l$, each neuron takes an affine combination of the $n_{l-1}$ outputs
from the previous layer, then pass them through an &lt;strong&gt;activation function&lt;/strong&gt; 
$f: \mathbb{R} \rightarrow \mathbb{R}$ 
to produce the $n_l$ outputs. The &lt;strong&gt;weights&lt;/strong&gt; of the linear
combination are denoted ${\bf w}^l_i= [w^l_{i1}, \dots, w^l_{in_{l-1}}]^T$ 
and the &lt;strong&gt;constant&lt;/strong&gt; is denoted $b^l_i$, such
that&lt;/p&gt;

\[a^l_i = f(({\bf w}^l_i)^T {\bf a}^{l-1} + b^l_i )\]

&lt;p&gt;Let’s shorten the notations a bit using vectorial notations. Let’s define the total
activation function for layer $l$ as $F^l: \mathbb{R}^{n_l} \rightarrow
\mathbb{R}^{n_l}$ with $F^l(x) = [f(x_1), \dots, f(x_{n_l})]^T$. Let’s
introduce the weight matrix at layer $l$, ${\bf W}^l = [{\bf w}^l_1, \dots, {\bf
w}^l_{n_l}]^T \in \mathbb{R}^{n_l \times n_{l-1}}$, 
and the constant vector ${\bf b}^l = [b^l_1, \dots, b^l_{n_l}]^T$.
We can then write, for $l=1,\dots,L$,&lt;/p&gt;

\[{\bf a}^l = F^l \left({\bf W}^l \cdotp {\bf a}^{l-1} + {\bf b}^l \right)\]

&lt;p&gt;I’ll introduce one last variable to simplify the notation. Let’s call ${\bf
z}^l$ the input for layer $l$, i.e.,&lt;/p&gt;

\[{\bf z}^l = {\bf W}^l \cdotp {\bf a}^{l-1} + {\bf b}^l\]

&lt;p&gt;Then we can write the activation at layer $l$ as,
\({\bf a}^l = F^l \left( {\bf z}^l \right)\).&lt;/p&gt;

&lt;p&gt;The only exception is for the input layer $l=0$ where $a^0$ is not calculated
but is an input variable.
At every layer, we therefore have $n_l (n_{l-1} + 1)$ parameters for a
grand total of $\sum_{l=1}^N n_l ( n_{l-1}+1)$ parameters in the entire network.&lt;/p&gt;

&lt;h2 id=&quot;loss-function&quot;&gt;Loss function&lt;/h2&gt;

&lt;p&gt;To train the network, we need to define a loss function. Let’s assume we have
data \(\{ ({\bf x}_i,{\bf y}_i) \}_{i=1}^N\), where ${\bf x}_i$ is fed in the input
layer and ${\bf y}_i$ is compared with the output of the network. The output of
the network is therefore a function of all parameters and the input variable,
i.e., \({\bf a}^L = {\bf a}^L( \{ {\bf W}^l, {\bf b}^l \}_l; {\bf x})\).
For the loss function, I will use a least-square misfit, which leads to&lt;/p&gt;

\[\mathcal{L}(\{ {\bf W}^l, {\bf b}^l \}_l) = 
\frac1{2N} \sum_{i=1}^N 
\| {\bf a}^L( \{ {\bf W}^l, {\bf b}^l \}_l; {\bf x}_i) - {\bf y}_i \|^2\]

&lt;p&gt;To estimate the loss function, we simply plug, for each data pair $({\bf x}_i,
{\bf y}_i)$, the input ${\bf x}_i$ in the input layer, i.e., we set ${\bf a}^0 =
{\bf x}_i$, then propagate that forward sequentially. To emphasize the
dependence on a specific data point, we write ${\bf a}^k = {\bf a}^k({\bf
x}_i)$,&lt;/p&gt;

\[\begin{align*}
{\bf a}^1({\bf x}_i) &amp;amp; = F^1({\bf W}^1 {\bf x}_i + {\bf b}^1) \\
{\bf a}^2({\bf x}_i) &amp;amp; = F^2({\bf W}^2 {\bf a}^1({\bf x}_i + {\bf b}^2) \\
\vdots &amp;amp; \\
{\bf a}^L({\bf x}_i) &amp;amp; = F^L({\bf W}^L {\bf a}^{L-1}({\bf x}_i) + {\bf b}^L) 
\end{align*}\]

&lt;p&gt;Similarly we denote \({\bf z}^k = {\bf z}^k({\bf x}_i) = 
{\bf W}^k {\bf a}^{k-1}({\bf x}_i) + {\bf b}^k\).
And we decompose the loss function into 
\(\mathcal{L}(\{ {\bf W}^l, {\bf b}^l \}_l) = 
1/N \sum_{i=1}^N c(\{ {\bf W}^l, {\bf b}^l \}_l, {\bf x}_i)\) where&lt;/p&gt;

\[c(\{ {\bf W}^l, {\bf b}^l \}_l, {\bf x}_i) = 
\frac12 \| {\bf a}^L( \{ {\bf W}^l, {\bf b}^l \}_l; {\bf x_i}) - {\bf y}_i \|^2\]

&lt;h2 id=&quot;calculating-derivatives&quot;&gt;Calculating derivatives&lt;/h2&gt;

&lt;p&gt;To minimize the loss function, we need to calculate derivatives of that loss
function $L$ with respect to the parameters ${\bf W}^l$ and ${\bf b}^l$ at each
layers. This is done via the backpropagation algorithm; for more details on the
backpropagation, see this &lt;a href=&quot;/2018/11/13/backprop&quot;&gt;post&lt;/a&gt;.
The main results are that, after a forward propagation (see previous section),
the gradient of the loss function can be calculated as&lt;/p&gt;

\[\begin{align*}
\frac{\partial \mathcal{L}}{\partial {\bf b}^l} &amp;amp; = \frac1N \sum_{i=1}^N 
\frac{\partial c({\bf x}_i)}{\partial {\bf b}^l} \\
\frac{\partial \mathcal{L}}{\partial {\bf W}^l} &amp;amp; = \frac1N \sum_{i=1}^N 
\frac{\partial c({\bf x}_i)}{\partial {\bf W}^l} 
\end{align*}\]

&lt;p&gt;For each contribution $c({\bf x}_i)$ to the loss function, its derivatives 
with respect to all parameters can be
calculated sequentially starting from the output layer $L$, and moving backward
to the first layer, by using the formulas for each $i=1,\dots,N$,&lt;/p&gt;

\[\begin{align*}
\frac{\partial c({\bf x}_i)}{\partial {\bf b}^L} &amp;amp; = 
\begin{bmatrix} f&apos;( z_1^L({\bf x}_i) )  &amp;amp; &amp;amp; 0 \\   &amp;amp;   \ddots &amp;amp; \\  0    &amp;amp;  &amp;amp;   f&apos;(z_{n_L}^L({\bf x}_i))
\end{bmatrix} \cdotp 
\frac{\partial c({\bf x}_i)}{\partial {\bf a}^L} 
\\
\frac{\partial c({\bf x}_i)}{\partial {\bf b}^l} &amp;amp; = \begin{bmatrix}
f&apos;( z_1^l({\bf x}_i) )  &amp;amp; &amp;amp; 0 \\   &amp;amp;   \ddots &amp;amp; \\  0    &amp;amp;  &amp;amp;   f&apos;(z_{n_l}^l({\bf x}_i))
\end{bmatrix} \cdotp ({\bf W}^{l+1})^T \cdotp \frac{\partial c({\bf x}_i)}{\partial {\bf
b}^{l+1}} , \quad \forall l=1,\dots,L-1\\
 \frac{\partial c({\bf x}_i)}{\partial {\bf W}^l} &amp;amp; =
\frac{\partial c({\bf x}_i)}{\partial {\bf b}^l} \cdotp ({\bf a}^{l-1}({\bf x}_i))^T , 
\quad \forall l=1,\dots,L
\end{align*}\]

&lt;h2 id=&quot;initialization&quot;&gt;Initialization&lt;/h2&gt;

&lt;p&gt;Initialization is quite critical to the success of neural nets.&lt;/p&gt;

&lt;p&gt;Typically, all biases ${\bf b}^l$ are initialized to zero.&lt;/p&gt;

&lt;p&gt;You however don’t want to do that with the weights, or you’d effectively kill
all layers (see &lt;a href=&quot;/2018/11/13/backprop&quot;&gt;post&lt;/a&gt;). Also, you want to make sure that
you do not saturate all neurons, for all data points. For that reason, you tend
to choose random values centered around zero. And this must go along with 
normalized input data, to avoid immediate saturation of neurons. There are a few
heuristic to choose an weight initialization distribution.&lt;/p&gt;

&lt;h4 id=&quot;symmetry&quot;&gt;Symmetry&lt;/h4&gt;

&lt;p&gt;If in a layer one uses the exact same weights for each neuron (either at
initialization, or during the optimization), then all neurons of that layer
will remain the same. That is apparent when looking at the derivatives. Whatever
the vector misfit (for the output layer), or the derivative from the layer just
after, if all weights are the same, the derivative will be the same. It is
therefore important to “break the symmetry” in the initialization.&lt;/p&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;I implemented all those ideas from scratch in Python;
The code is available on &lt;a href=&quot;https://github.com/bcrestel/DIYDeepLearning&quot;&gt;GitHub&lt;/a&gt;.
You can use it to assemble a neural network and calculate its derivatives using
the backpropagation algorithm.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Bro, do you even backprop?</title>
   <link href="https://bcrestel.github.io/2018/11/13/backprop/"/>
   <updated>2018-11-13T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2018/11/13/backprop</id>
   <content type="html">&lt;p&gt;The goal of this post is to dive deeper into the derivation of the backpropagation
algorithm. This entry can be seen as a sub-post of that main
&lt;a href=&quot;/2018/11/13/deeplearnDIY&quot;&gt;post&lt;/a&gt;
discussing a Python implementation of an artificial neural network, and all
notations will be exactly the same. For conventions and definitions about
vector/matrix calculus, see that &lt;a href=&quot;/2018/11/12/MatrixCalculus&quot;&gt;post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The loss function is defined as the sum of individual contributions for each
data point $i$,&lt;/p&gt;

\[\mathcal{L}(\{ {\bf W}^l, {\bf b}^l \}_l) = 
\frac1N \sum_{i=1}^N c(\{ {\bf W}^l, {\bf b}^l \}_l, {\bf x}_i, {\bf y}_i)\]

&lt;p&gt;For examples of loss functions, have a look at that
&lt;a href=&quot;/2018/11/23/loss-fct&quot;&gt;post&lt;/a&gt;.
In this post, I’m going to focus only on how to calculate the derivatives 
\(\frac{\partial c({\bf x}_i)}{\partial {\bf b}^l}\)
and
\(\frac{\partial c({\bf x}_i)}{\partial {\bf W}^l}\).&lt;/p&gt;

&lt;h2 id=&quot;derivatives-with-respect-to-bf-bl&quot;&gt;Derivatives with respect to ${\bf b}^l$&lt;/h2&gt;

&lt;p&gt;Applying the chain-rule, we get that&lt;/p&gt;

\[\frac{\partial c({\bf x}_i)}{\partial {\bf b}^l} = 
\frac{\partial {\bf a}^L}{\partial {\bf b}^l} \cdotp 
\frac{\partial c({\bf x}_i)}{\partial {\bf a}^L}\]

&lt;p&gt;&lt;strong&gt;For the output layer&lt;/strong&gt;, i.e., $l=L$, since ${\bf a}^L = {\bf W}^L \cdotp {\bf
a}^{L-1} + {\bf b}^L$, applying the chain-rule again
with $z^L_k$, we have&lt;/p&gt;

\[\frac{\partial {\bf a}^L}{\partial b^L_j} = 
\sum_{k=1}^{n_L} \frac{\partial {\bf a}^L}{\partial z_k^L}
\frac{\partial z_k^L}{\partial b^L_j}\]

&lt;p&gt;Let ${\bf e}_k = [\dots,0,1,0,\dots]^T$ be the vector filled with all zeros
but for one 1 at the $k^\text{th}$ entry. Then, we have&lt;/p&gt;

\[\frac{\partial {\bf a}^L}{\partial z_k^L} = f&apos;(z_k^L) {\bf e}_k,
\quad \frac{\partial z_k^L}{\partial b^L_j} = \delta_{jk}\]

&lt;p&gt;This means&lt;/p&gt;

\[\frac{\partial {\bf a}^L}{\partial b^L_j} = f&apos;(z_j^L) {\bf e}_j\]

&lt;p&gt;Following the convention defined
&lt;a href=&quot;/2018/11/12/MatrixCalculus&quot;&gt;here&lt;/a&gt;, we’ll stack these vectors row by
row to get&lt;/p&gt;

\[\frac{\partial {\bf a}^L({\bf x}_i)}{\partial {\bf b}^L} = 
\begin{bmatrix}
f&apos;( z_1^L({\bf x}_i) )  &amp;amp; &amp;amp; 0 \\
  &amp;amp;   \ddots &amp;amp; \\
 0    &amp;amp;  &amp;amp;   f&apos;(z_{n_L}^L({\bf x}_i))
\end{bmatrix}\]

&lt;p&gt;In the end, with \(z_k^L({\bf x}_i) = ({\bf w}^L_k)^T \cdotp {\bf a}^{L-1}({\bf
x}_i) + b^L_k\), we
get&lt;/p&gt;

\[\frac{\partial c({\bf x}_i)}{\partial {\bf b}^L} = 
\begin{bmatrix}
f&apos;( z_1^L({\bf x}_i) )  &amp;amp; &amp;amp; 0 \\
  &amp;amp;   \ddots &amp;amp; \\
 0    &amp;amp;  &amp;amp;   f&apos;(z_{n_L}^L({\bf x}_i))
\end{bmatrix} \cdotp 
\frac{\partial c({\bf x}_i)}{\partial {\bf a}^L}\]

&lt;p&gt;Using the results in the post &lt;a href=&quot;/2018/11/23/loss-fct&quot;&gt;loss function&lt;/a&gt;, we can show
actual examples. For a least-square loss function, we have
\(\frac{\partial c({\bf x}_i)}{\partial {\bf a}^L} = ({\bf a}^L({\bf x}_i) -
{\bf y}_i)\). Whereas in the case of a multi-class classifier with a softmax
output layer (which is not actually covered by the above formula), we would have
\(\frac{\partial c({\bf x}_i)}{\partial {\bf a}^L} = a^L({\bf x}_i)_{y_i} {\bf e}_{y_i}\).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;For an interior layer $l$&lt;/strong&gt;, we need to write an induction between successive
layers. But first, let’s make a few observations. In the derivatives, the
variables ${\bf z}^l$ and ${\bf b}^l$ are inter-changeable. Indeed, since
\(\frac{\partial {\bf z}^l}{\partial {\bf b}^l} = I_{n_l}\), we have&lt;/p&gt;

\[\frac{\partial \cdotp}{\partial {\bf b}^l} = 
\frac{\partial {\bf z}^l}{\partial {\bf b}^l}
\frac{\partial \cdotp}{\partial {\bf z}^l} = \frac{\partial \cdotp}{\partial
{\bf z}^l}\]

&lt;p&gt;Next, the result for \(\frac{\partial {\bf a}^L}{\partial {\bf b}^L}\) 
derived above
can be generalized to any layer $l$, i.e.,&lt;/p&gt;

\[\frac{\partial {\bf a}^l({\bf x}_i)}{\partial {\bf b}^l} = 
\begin{bmatrix}
f&apos;( z_1^l({\bf x}_i) )  &amp;amp; &amp;amp; 0 \\
  &amp;amp;   \ddots &amp;amp; \\
 0    &amp;amp;  &amp;amp;   f&apos;(z_{n_l}^l({\bf x}_i))
\end{bmatrix}\]

&lt;p&gt;We now apply the chain-rule,&lt;/p&gt;

\[\begin{align*}
\frac{\partial c({\bf x}_i)}{\partial {\bf b}^l} &amp;amp; = \frac{\partial c({\bf x}_i)}{\partial {\bf z}^l} \\
 &amp;amp; = \frac{\partial {\bf a}^l}{\partial {\bf z}^l} 
 \frac{\partial {\bf z}^{l+1}}{\partial {\bf a}^l} 
\frac{\partial c({\bf x}_i)}{\partial {\bf z}^{l+1}} \\
 &amp;amp; = \frac{\partial {\bf a}^l}{\partial {\bf b}^l} 
 \frac{\partial {\bf z}^{l+1}}{\partial {\bf a}^l} 
\frac{\partial c({\bf x}_i)}{\partial {\bf b}^{l+1}} 
\end{align*}\]

&lt;p&gt;And in the end,&lt;/p&gt;

\[\frac{\partial c({\bf x}_i)}{\partial {\bf b}^l} =
\begin{bmatrix}
f&apos;( z_1^l({\bf x}_i) )  &amp;amp; &amp;amp; 0 \\
  &amp;amp;   \ddots &amp;amp; \\
 0    &amp;amp;  &amp;amp;   f&apos;(z_{n_l}^l({\bf x}_i))
\end{bmatrix} \cdotp
({\bf W}^{l+1})^T \cdotp
\frac{\partial c({\bf x}_i)}{\partial {\bf b}^{l+1}}\]

&lt;h2 id=&quot;derivatives-with-respect-to-bf-wl&quot;&gt;Derivatives with respect to ${\bf W}^l$&lt;/h2&gt;

&lt;p&gt;I am going to re-use the results in the previous section. In fact, this is yet
another application of the chain-rule,&lt;/p&gt;

\[\begin{align*}
\frac{\partial c({\bf x}_i)}{\partial {\bf w}_i^l} &amp;amp; = 
 \frac{\partial {\bf z}^l}{\partial {\bf w}_i^l}  
 \frac{\partial c({\bf x}_i)}{\partial {\bf z}^l}  \\
&amp;amp; = \begin{bmatrix}  \dots &amp;amp; 0 &amp;amp; {\bf {a}}^{l-1}({\bf x}_i) &amp;amp; 0 &amp;amp; \dots \end{bmatrix} \cdotp
 \frac{\partial c({\bf x}_i)}{\partial {\bf b}^l} \\
 &amp;amp; = {\bf a}^{l-1}({\bf x}_i) \frac{\partial c({\bf x}_i)}{\partial b^l_i} 
\end{align*}\]

&lt;p&gt;Since \({\bf W}^l = [{\bf w}_1^l,\dots,{\bf w}_{n_l}^l]^T\), we stack all
\(\frac{\partial c({\bf x}_i)}{\partial {\bf w}_i^l}\) in columns then take the transpose
to get&lt;/p&gt;

\[\frac{\partial c({\bf x}_i)}{\partial {\bf W}^l} =
\frac{\partial c({\bf x}_i)}{\partial {\bf b}^l} \cdotp ({\bf a}^{l-1}({\bf x}_i))^T\]

&lt;h2 id=&quot;comments&quot;&gt;Comments&lt;/h2&gt;

&lt;h4 id=&quot;dead-neurons&quot;&gt;Dead neurons&lt;/h4&gt;

&lt;p&gt;It is informative to investigate what can cause the gradient of the cost
functional to be zero. Note that this only applies to a single datapoint.&lt;/p&gt;

&lt;p&gt;(1) that will be the case if the next layer is dead (all gradients evaluate to
zero), b/c of the back-propagation recursion. So once you have a dead layer (all
neurons in that layer are dead), then you stop backpropagating past that layer.&lt;/p&gt;

&lt;p&gt;(2) that will also be the case if $f’(z^l_i) = 0$. With RELU, this will happen
with negative values of ${\bf z}^l$, and for sigmoid / tanh, this will happen with very large
values (either negative of positive).&lt;/p&gt;

&lt;p&gt;(3) if a column of ${\bf W}^{l+1}$ is zero.&lt;/p&gt;

&lt;h4 id=&quot;parallelism&quot;&gt;Parallelism&lt;/h4&gt;

&lt;p&gt;The backpropagation algorithm is inherently sequential, which makes its
parallelism challenging, besides the embarassing parallelism of the sum over the
data points. This is actually an advantage for batch algorithms, that require all
$N$ points, over stochastic algorithms like stochastic gradient which compute
the derivative for a single data point $i$.&lt;/p&gt;

&lt;h4 id=&quot;checkpointing&quot;&gt;Checkpointing&lt;/h4&gt;

&lt;p&gt;To compute the derivatives, you need to store all \({\bf
a}^l({\bf x}_i)\) that were computed during the forward propagation.
I’m wondering if memory storage could be an issue. In which case a checkpointing
algorithm would be a natural solution&lt;/p&gt;

&lt;h4 id=&quot;higher-order-derivatives&quot;&gt;Higher-order derivatives&lt;/h4&gt;

&lt;p&gt;The next step is to look at how you can compute the action of a given vector on
the Hessian of the loss function.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Statistical Hypothesis Tests</title>
   <link href="https://bcrestel.github.io/2018/11/12/StatTests/"/>
   <updated>2018-11-12T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2018/11/12/StatTests</id>
   <content type="html">&lt;p&gt;The goal of the post is to summarize a few important statistical hypothesis tests that come
back often in practice.&lt;/p&gt;

&lt;h2 id=&quot;t-tests&quot;&gt;t-tests&lt;/h2&gt;
&lt;p&gt;The first class of tests we are going to look at are
&lt;a href=&quot;https://en.wikipedia.org/wiki/Student%27s_t-test&quot;&gt;t-tests&lt;/a&gt;, i.e., tests that
involve a statistic following a &lt;a href=&quot;https://en.wikipedia.org/wiki/Student%27s_t-distribution&quot;&gt;Student’s t
distribution&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;one-sample-t-test&quot;&gt;One sample t-test&lt;/h4&gt;
&lt;p&gt;The simplest t-test, a &lt;a href=&quot;https://en.wikipedia.org/wiki/Student%27s_t-test#One-sample_t-test&quot;&gt;one sample
t-test&lt;/a&gt;,
can be used to check whether the &lt;strong&gt;sample mean&lt;/strong&gt; is equal to a certain value
when all samples
\(\{ X_i \}_i\) 
are drawn from distributions that have
the same mean $\mu$ and variance $\sigma^2$.  This is a variation of a z-test when the
variance of the population is unknown and needs to be estimated from the
population. Indeed, from the central limit theorem, we know the sample mean,
$\bar{X} = \frac1n \sum_{i=1}^n X_i$, tends (as the number of samples increase)
to a normal distribution, i.e., in the limit of a large number of samples,
\[ \frac{\bar{X} - \mu}{\sigma/\sqrt{n}} \sim \mathcal{N}(0,1) \]
Clearly we have $\mathbb{E}[\bar{X}] = \mu$, and if all samples are independent,
$Var[\bar{X}] = \sigma^2/n$.
However if we do not know $\sigma^2$ a priori, and use instead the sampling
variance 
\(s^2 = \frac1{n-1} \sum_{i=1}^n (X_i - \bar{X})^2\), the distribution becomes
\[ \frac{\bar{X} - \mu}{s/\sqrt{n}} \sim t(n-1) \]&lt;/p&gt;

&lt;p&gt;To apply a one-sample t-test to the parameters of a &lt;strong&gt;linear regression&lt;/strong&gt;, the
variance is not scaled by the square root of the number of samples. 
The standard error is calculated directly from the regression.
For an OLS, $Y = X.\beta +
\varepsilon$, with $\varepsilon \sim \mathcal{N}(0, \sigma^2 I)$, the parameters
are estimated by $b = (X^T \cdotp X)^{-1} (X^T \cdotp Y) = \beta + (X^T \cdotp
X)^{-1} X^T \varepsilon$. The variance of the estiamte $b$ is then $Var[b] =
\sigma^2 (X^T \cdotp X)^{-1}$. Since $\sigma^2$ is unknown, we use instead the
sampling variance $s^2 = \frac1{n-1} \sum_{i=1}^n (y_i - \hat{y}_i)^2$, where
$\hat{y}_i$ is the estimate for $y_i$.&lt;/p&gt;

&lt;h4 id=&quot;independent-two-sample-t-test&quot;&gt;Independent two sample t-test&lt;/h4&gt;
&lt;p&gt;We can use an &lt;a href=&quot;https://en.wikipedia.org/wiki/Student%27s_t-test#Independent_two-sample_t-test&quot;&gt;independent two sample
t-test&lt;/a&gt;
to compare the population mean of two populations, when the samples for each
population do not have a clear connection with each other. We’ll look at the
case of the dependent two-sample t-test afterward.
Again, we typically do not
know the variance of each population and must resort to the sampling variance,
which leads to a Student’s t distribution. The statistic we use here is
\[ \frac{ \bar{X}_1 - \bar{X}_2}{s_d} \sim t(df) \]as 
where $s_d$ is the standard deviation and $df$ is the number of degrees of
freedom. The correct values to use depend on the situation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If the two populations have the same variance&lt;/strong&gt;, we can use a &lt;a href=&quot;https://en.wikipedia.org/wiki/Pooled_variance&quot;&gt;pooled sample
variance&lt;/a&gt; $s_p$ to compute $s_d$, i.e., 
\[ s_d = s_p \sqrt{1/n_1 + 1/n_2}, \] 
where \(s_p^2 = (\sum_i (n_i-1) s_i^2) / (\sum_j (n_j-1))\), and 
\[ df = n_1 + n_2 - 2 \]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If the two populations have different variance&lt;/strong&gt; (and in the general case
different number of samples), then
\[ s_d = \sqrt{s_1^1/n_1 + s_2^2/n_2} . \]
The exact distribution in that case is a mess, but for all practical cases it
can be approximated by a Student’s t-test with degrees of freedom
\[ df = \frac{(s_1^1/n_1 + s_2^2/n_2)^2}{(s_1^2/n_1)^2/(n_1-1) +
(s_2^2/n_2)^2/(n_2-1)} \]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A dependent t-test for paired samples&lt;/strong&gt; is when two samples are related to
each other, for instance, if those are the same patients before and after a
treatment. In that case, the solution is to test the difference of the pairs in
a one-sample t-test.&lt;/p&gt;

&lt;h2 id=&quot;chi-square-tests&quot;&gt;Chi-square tests&lt;/h2&gt;

&lt;p&gt;There are a different flavours of &lt;a href=&quot;https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test&quot;&gt;chi-square
tests&lt;/a&gt;.
I’m here looking at the goodness-of-fit, i.e., test if a categorical population
is distributed according to a theoretical distribution (null hypothesis). In
that case, Pearson’s chi-square test is actually an approximation to the
&lt;a href=&quot;https://en.wikipedia.org/wiki/G-test&quot;&gt;G-test&lt;/a&gt;,
\[ G = 2 \sum_i O_i \ln (O_i/E_i) \sim \chi^2(df) \]
where $O_i$ is the observed count for category $i$, $E_i$ is the expected count
for category $i$ (under the null). The degrees of freedom is $df = $ number of
categories $-$ (number of parameters in the distribution + 1); for instance, the
number of parameters for a uniform distribution is 0, for a standard normal is
2,$\dots$&lt;/p&gt;

&lt;p&gt;How do we get to the chi-square test from a G-test? 
We use the expansion $\ln(1+u) \approx u - u^2/2$ when $|u|\ll 1$.
If we assume that $O_i$ and
$E_i$ are close to each other, then 
\[ O_i \ln(E_i/O_i) = O_i \ln \left( 1 + \frac{E_i-O_i}{O_i} \right)
\approx (E_i - O_i) - \frac{(E_i-O_i)^2}{2O_i} \]
Going back to the G statistic, and using the fact that $\sum_i E_i = \sum_i
O_i$, we have
\[ G = -2 \sum_i O_i \ln (E_i/O_i) 
\approx \sum_i \frac{(O_i-E_i)^2}{O_i} \]
which is the chi-square test.&lt;/p&gt;

&lt;h2 id=&quot;anova-test&quot;&gt;ANOVA test&lt;/h2&gt;

&lt;p&gt;I won’t go much in details. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Analysis_of_variance&quot;&gt;ANOVA
test&lt;/a&gt; can be seen as a
generalization of a two-sample t-test to the case of multiple populations.
There is also a whole chapter dedicated to it in Casella &amp;amp; Berger’s Statistical
Inference textbook.&lt;/p&gt;

&lt;h2 id=&quot;ljung-box-test&quot;&gt;Ljung-Box test&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;https://otexts.org/fpp2/residuals.html&quot;&gt;Ljung-Box&lt;/a&gt; test is a portmanteau
test to test the correlation of a time-series. It allows to test whether
the first $h$ auto-correlations of a time-series are like white noise (null) or
not (serial correlation). We define the statistic
\[ Q = n (n+2) \sum_{k=1}^h \frac{\hat{\rho}_k^2}{n-k} \sim \chi^2(h) \]
where $\hat{\rho}_k$ is the lag-k sample autocorrelation. Note that this is a
one-sided test.&lt;/p&gt;

&lt;h2 id=&quot;unit-root-test&quot;&gt;Unit root test&lt;/h2&gt;

&lt;p&gt;If a time-series has a unit root, it is, among other things, non-stationary.
For instance, we can test for the presence of a unit root to decide whether a time-series
needs to be differencied or not.
The Dickey-Fuller test is a popular test for unit roots.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Conventions for Matrix Calculus</title>
   <link href="https://bcrestel.github.io/2018/11/12/MatrixCalculus/"/>
   <updated>2018-11-12T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2018/11/12/MatrixCalculus</id>
   <content type="html">&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;p&gt;In optimization, you need to take derivatives. To do that in $\mathbb{R}^n$, you
need matrix calculus. The objective of this note is to summarize the important
definitions and conventions, explain them whenever possible, and show a few
examples.&lt;/p&gt;

&lt;h4 id=&quot;frechet-and-gateaux-derivatives&quot;&gt;Frechet and Gateaux derivatives&lt;/h4&gt;

&lt;p&gt;Let’s define a function $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$.  That
function is 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Fr%C3%A9chet_derivative&quot;&gt;Frechet differentiable&lt;/a&gt; 
at $x$ if
there exists a bounded (necessarily the case for an operator between finite dimensional
spaces) linear operator&lt;br /&gt;
$Df(x): \mathbb{R}^n \rightarrow \mathbb{R}^m$ 
such that
\[ \frac{| f(x+h) - f(x) - Df(x)h |}{| h |} \rightarrow 0 \] 
as $|h| \rightarrow 0$.&lt;/p&gt;

&lt;p&gt;A somehow weaker definition of differentiability is the &lt;a href=&quot;https://en.wikipedia.org/wiki/G%C3%A2teaux_derivative&quot;&gt;Gateaux
differentiability&lt;/a&gt;.  A
function $f$ is Gateaux differentiable at $x$ if, for any $v
\in \mathbb{R}^n$ the following limit exists
\[ lim_{\varepsilon \rightarrow 0} \frac{f(x + \varepsilon v) - f(x)} \varepsilon . \]
If that limit exists, we can calculate it as
\[ lim_{\varepsilon \rightarrow 0} \frac{f(x + \varepsilon v) -
f(x)}\varepsilon = \frac{d}{d\varepsilon} f(x + \varepsilon v)
|_{\varepsilon=0} . \]&lt;/p&gt;

&lt;p&gt;If $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ 
is Frechet differentiable, it is necessarily Gateaux differentiable, and
\(\frac{d}{d \varepsilon} f(x + \varepsilon v) |_{\varepsilon=0} = Df(x) v\).&lt;/p&gt;

&lt;h4 id=&quot;gradient-of-a-functional&quot;&gt;Gradient of a functional&lt;/h4&gt;

&lt;p&gt;Let’s call $H = \mathbb{R}^n$, which is a Hilbert space with inner product
$\langle \cdotp, \cdotp \rangle$.  For a functional $f: \mathbb{R}^n \rightarrow
\mathbb{R}$, the derivative $Df(x)$ is, by definition, an element of the dual
space $H^*$.
Applying &lt;a href=&quot;https://en.wikipedia.org/wiki/Riesz_representation_theorem&quot;&gt;Riesz representation
theorem&lt;/a&gt;, we know
there is an element $g_x \in H$ such that for any $v \in H$,
\(DF(x) v = \langle g_x, v \rangle\). That element $g_x$ is the gradient of
the functional $f$. This clearly defines the gradient of a functional, 
without having to agree on notations or conventions.&lt;/p&gt;

&lt;h4 id=&quot;general-case&quot;&gt;General case&lt;/h4&gt;

&lt;p&gt;What can we do for a function $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$?
First, we can’t apply Riesz representation theorem. Also, it is not clear how we
optimize that function $f$. We’d need to define a &lt;a href=&quot;https://en.wikipedia.org/wiki/Total_order#Orders_on_the_Cartesian_product_of_totally_ordered_sets&quot;&gt;total
order&lt;/a&gt;
on $\mathbb{R}^m$ that would coincide with the objective of optimization.  For
that reason, I see the definition of a gradient in that case as more of a
convention.  There are really two conventions, which are a transpose of each
other (see &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_calculus#Layout_conventions&quot;&gt;layout
conventions&lt;/a&gt;),
and I adopt the convention used in Nocedal &amp;amp; Wright’s Numerical Optimization
textbook (section A.2 Derivatives).
Linear maps between two finite-dimensional spaces can
all be described by the action of a matrix.
Nocedal &amp;amp; Wright call the Jacobian the matrix 
$J(x) \in \mathbb{R}^{m \times n}$ 
that verifies, for any $v \in \mathbb{R}^n$, $Df(x)v = J(x) \cdotp v$.
The gradient is defined to be the transpose,&lt;/p&gt;

\[\begin{align} 
J(x) &amp;amp; = \left[ \frac{\partial f_i}{\partial x_j} \right]_{ij} 
\in \mathbb{R}^{m \times n} \\
\nabla f(x) &amp;amp; = \left[ \frac{\partial f_j}{\partial x_i} \right]_{ij} = J(x)^T
= \begin{bmatrix}
\frac{\partial f_1}{\partial x_1} &amp;amp; \dots &amp;amp; \frac{\partial f_m}{\partial x_1} \\
\vdots &amp;amp; &amp;amp; \vdots \\
\frac{\partial f_1}{\partial x_n} &amp;amp; \dots &amp;amp; \frac{\partial f_m}{\partial x_n} 
\end{bmatrix}
\in \mathbb{R}^{n \times m} 
\end{align}\]

&lt;p&gt;One thing to be careful about with that notation, 
the chain-rule, as we know it, applies to the Jacobian, i.e., if $h(x) =
f(g(x))$, then&lt;/p&gt;

\[J_h(x) = J_f(g(x)) \cdotp J_g(x)\]

&lt;p&gt;and therefore in terms of the gradient, we get the transpose,&lt;/p&gt;

\[\nabla h(x) = \nabla g(x) \cdotp \nabla f(g(x))\]

&lt;p&gt;For instance, let’s assume $f: \mathbb{R}^m \rightarrow \mathbb{R}$ and $g: \mathbb{R}^n
\rightarrow \mathbb{R}^m$. Then, with $y_k = (g(x))_k$, for any $i=1,\dots,n$,&lt;/p&gt;

\[\frac{\partial h}{\partial x_i} = \sum_{k=1}^m \frac{\partial f}{\partial
y_k} \frac{\partial y_k}{\partial x_i} 
= \left [\frac{\partial (g(x))_i}{\partial x_i} \right]_i^T \cdotp \nabla f(g(x))\]

&lt;p&gt;Then putting all indices $i$ together (in rows), we get the expression above for the
gradient.&lt;/p&gt;

&lt;h4 id=&quot;derivative-with-respect-to-a-matrix&quot;&gt;Derivative with respect to a matrix&lt;/h4&gt;

&lt;p&gt;In that case also, this is just a convenient notation. For a function $f :
\mathbb{R}^{m \times n} \rightarrow \mathbb{R}$, we define&lt;/p&gt;

\[\frac{\partial f(M)}{\partial M} = \left[
\frac{\partial f(M)}{\partial m_{ij}} \right]_{ij}\]

&lt;h2 id=&quot;examples&quot;&gt;Examples&lt;/h2&gt;

&lt;h4 id=&quot;if-fx--ax--b&quot;&gt;If $f(x) = Ax + b$&lt;/h4&gt;

&lt;p&gt;We then have $f : \mathbb{R}^n \rightarrow \mathbb{R}^m$.
We can apply the definition of the Gateaux derivative,&lt;/p&gt;

\[f(x + \varepsilon v) = f(x) + \varepsilon A v\]

&lt;p&gt;We can conlude directly that&lt;/p&gt;

\[\nabla f(x) = A^T\]

&lt;h4 id=&quot;if-fx--frac12-xt-q-x-&quot;&gt;If $f(x) = \frac12 x^T Q x $&lt;/h4&gt;

&lt;p&gt;We then have $f : \mathbb{R}^n \rightarrow \mathbb{R}$.
We can apply the definition of the Gateaux derivative,&lt;/p&gt;

\[f(x + \varepsilon v) = f(x) + \frac12 \varepsilon 
\left( x^T Q v + v^T Q x \right) + 
\frac12 \varepsilon^2 v^T Q v\]

&lt;p&gt;We conlude that&lt;/p&gt;

\[\nabla f(x) = \frac12 ( Q + Q^T) x\]

&lt;p&gt;In the special case that $Q=Q^T$ (symmetric), we have&lt;/p&gt;

\[\nabla f(x) =  Q x\]

&lt;h4 id=&quot;if-fx--frac12--axb2&quot;&gt;If $f(x) = \frac12 | Ax+b|^2$&lt;/h4&gt;

&lt;p&gt;We then have $f : \mathbb{R}^n \rightarrow \mathbb{R}$.
Following the same approach, we get&lt;/p&gt;

\[f(x + \varepsilon v) = f(x) + 
\frac12 \varepsilon \left( (Ax+b)^T A v + (Av)^T(Ax+b) \right) + 
\frac12 \varepsilon^2 \|Av\|^2\]

&lt;p&gt;We can conlude that&lt;/p&gt;

\[\nabla f(x) = A^T \cdotp (Ax+b)\]

&lt;p&gt;Alternatively, we can use the chain-rule with $g(y) = \frac12 | y |^2$ and
$f(x) = g(Ax + b)$. 
Since $\nabla (Ax+b) = A^T$ and $\nabla g(y) = y$, we have&lt;/p&gt;

\[\nabla f(x) = A^T \cdotp (Ax+b)\]

&lt;h4 id=&quot;if-fa--axb&quot;&gt;If $f(A) = Ax+b$&lt;/h4&gt;

&lt;p&gt;In that case, I’m not sure it helps to talk about a gradient. However we can
still calculate the derivative (e.g., using the formula for the Gateaux
derivative), and we get&lt;/p&gt;

\[Df(A) M = M \cdotp x\]

&lt;h4 id=&quot;if-fa--frac12--axb2&quot;&gt;If $f(A) = \frac12 | Ax+b|^2$&lt;/h4&gt;

&lt;p&gt;Here we have $f: \mathbb{R}^{m \times n} \rightarrow \mathbb{R}$.
It’s tempting to use the chain-rule, but I couldn’t agree with myself on a
logical convention. And I think sadly this is the main conclusion of that small
post: it’s safer to always rely on the entry-wise derivative. In this case,
with $y = Ax + b \in \mathbb{R}^m$, we have&lt;/p&gt;

\[\begin{align*}
\frac{\partial f(A)}{\partial a_{ij}} &amp;amp; = 
\sum_k \frac{\partial g(Ax + b)}{\partial y_k} \frac{\partial y_k}{\partial
a_{ij}} \\
&amp;amp; = \left[ \frac{\partial y_1}{\partial a_{ij}}, \dots, \frac{\partial y_m}{\partial
a_{ij}} \right] \cdotp (Ax + b)
\end{align*}\]

&lt;p&gt;Let’s look at the partial derivatives for $y$, using the notation $\delta_{ik} =
1$ if $i=k$ and $0$ otherwise,&lt;/p&gt;

\[\frac{\partial y_k}{\partial a_{ij}} = x_j \delta_{ik}\]

&lt;p&gt;Such that&lt;/p&gt;

\[\frac{\partial f(A)}{\partial a_{ij}}  = (Ax+b)_i x_j\]

&lt;p&gt;And putting all indices back together,&lt;/p&gt;

\[\frac{\partial f(A)}{\partial A}  = (Ax+b) \cdotp x^T\]

</content>
 </entry>
 
 <entry>
   <title>Using Font Awesome on your webpage</title>
   <link href="https://bcrestel.github.io/2018/09/04/fontawesome/"/>
   <updated>2018-09-04T00:00:00-04:00</updated>
   <id>https://bcrestel.github.io/2018/09/04/fontawesome</id>
   <content type="html">&lt;p&gt;Font awesome is an easy way to include cool icons to your website, like the one
for Github, Linkedin, or just email (see side bar). To use, all you need to do
is add&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css&quot;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;the header of your html file. Then &lt;a href=&quot;https://www.w3schools.com/icons/fontawesome_icons_webapp.asp&quot;&gt;choose&lt;/a&gt; 
the icon you want to use. You can modify the size of the icons by adding the
appropriate &lt;a href=&quot;https://fontawesome.com/how-to-use/on-the-web/styling/sizing-icons&quot;&gt;option&lt;/a&gt;.
For instance, for a large Github icon, you would add&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;i class=&quot;fa fa-github fa-lg&quot;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
</content>
 </entry>
 
 <entry>
   <title>Market-cap weighted portfolios are self-rebalancing</title>
   <link href="https://bcrestel.github.io/2018/09/01/mcapweighted/"/>
   <updated>2018-09-01T00:00:00-04:00</updated>
   <id>https://bcrestel.github.io/2018/09/01/mcapweighted</id>
   <content type="html">&lt;p&gt;This is a very simple note about a proof that I regularly forget. It is often
said that a great advantage of market-cap weighted portfolios is that they are
self rebalancing. That is, when you choose the weights of each security in a
portfolio according to the proportion of their market-cap, the weights remain
consistent with the original definition (under some rather mild assumptions).&lt;/p&gt;

&lt;h4 id=&quot;return-of-a-portfolio&quot;&gt;Return of a portfolio&lt;/h4&gt;
&lt;p&gt;First, an even simpler result that we need in the rest of that note: The return
of a portfolio is simply the weighted average of the returns of the securities
in the portfolio. Let’s introduce some notation: let’s call $X_t$ the
\$-value of
the portfolio at time $t$, and $X_t = \sum_i X_{i,t}$ where $X_{i,t} = w_{i,t}
X_t$ is the \$-value of the $i^\text{th}$ security in the portfolio at time
$t$. 
The sum is taken over all securities in the portfolio, and we assume the weights
sum to 1.
Next the return of the portfolio (similarly for any security $i$) is defined by $r_t =
X_t/X_{t-1}-1$.
Then,
\(\begin{align} 
X_t &amp;amp; = \sum_i X_{i,t} = \sum_i (1+r_{i,t}) X_{i,t-1} = \sum_i (1+r_{i,t})
w_{i,t-1} X_{t-1} \notag \\ 
&amp;amp; = X_{t-1} \left( 1 + \sum_i w_{i,t-1} r_{i,t} \right),
\end{align}\)
since by definition $\sum_i w_{i,t} = 1$ at any time $t$.
Note that the same conclusion applies to all type of returns, including
continuously compounded returns
\(\bar{r}_{i,t}\), since $1+r_{i,t} = e^{\bar{r}_{i,t}}$.&lt;/p&gt;

&lt;h4 id=&quot;self-rebalancing&quot;&gt;Self-rebalancing&lt;/h4&gt;
&lt;p&gt;Let’s define $M_{i,t}$ as the market-cap of security $i$, and let $M_t =
\sum_i M_{i,t}$.
Now let’s assume that in the period $t-1$, the weights are calculated as a
proportion of their relative market-cap, i.e., $w_{i,t-1} = M_{i,t-1} /
M_{t-1}$. And let’s assume that from period $t-1$ to $t$, the market-cap of all
securities $i$ only vary through its price (number of shares remain constant, no
M$\&amp;amp;$A,…).
Then, the ratios of market-cap will vary from $t-1$ to $t$
as
\[ \frac{M_{i,t}/M_t}{M_{i,t-1}/M_{t-1}} =  \frac{M_{i,t}}{M_{i,t-1}} \frac{M_{t-1}}{M_t}
 = \frac{1+r_{i,t}}{1+r_t} . \]
On the other hand, the weights of the portfolios will vary as
\[ \frac{w_{i,t}}{w_{i,t-1}} = \frac{X_{i,t}/X_t}{X_{i,t-1}/X_{t-1}} 
= \frac{X_{i,t}}{X_{i,t-1}} \frac{X_{t-1}}{X_t}
 = \frac{1+r_{i,t}}{1+r_t} . \]&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Sample mean, sample variance</title>
   <link href="https://bcrestel.github.io/2018/05/11/estimator/"/>
   <updated>2018-05-11T00:00:00-04:00</updated>
   <id>https://bcrestel.github.io/2018/05/11/estimator</id>
   <content type="html">&lt;p&gt;Let $X_1,\ldots,X_n$ be a random sample from a population with mean $\mu$ and
variance $\sigma^2 &amp;lt; \infty$. Any function of this
random sample is called a statistic, and the probability distribution of a statistic is
called the sampling distribution of that statistic.
In this note, we look at two important statistics.&lt;/p&gt;

&lt;h4 id=&quot;sample-mean&quot;&gt;Sample mean&lt;/h4&gt;
&lt;p&gt;The sample mean is defined as \[ \bar{X} = \frac1n \sum_{i=1}^n X_i \]
The sample mean is an unbiased estimator of the mean of the distribution, i.e.,
\[ \mathbb{E}[\bar{X}] = \frac1n \sum_{i=1}^n \mathbb{E}[X_i] = \mu\]
And the variance of the sample mean is
\(\begin{align} 
Var[\bar{X}] &amp;amp; = \mathbb{E}[\left( \frac1n \sum_{i=1}^n (X_i-\mu) \right)^2 ]
\notag \\
&amp;amp; = \frac{1}{n^2} \left( \sum_{i=1}^n \mathbb{E}[ (X_i-\mu)^2] + \sum_{i j}
\mathbb{E}[(X_i-\mu)(X_j-\mu)] \right) \notag \\
Var[\bar{X}] &amp;amp; = \frac{\sigma^2}n
\end{align}\)&lt;/p&gt;

&lt;h4 id=&quot;sample-variance&quot;&gt;Sample variance&lt;/h4&gt;
&lt;p&gt;The sample variance is defined as \[ (S_n^2=) S^2 = \frac1{n-1} \sum_{i=1}^n (X_i -
\bar{X})^2 \]
The sample variance is an unbiased estimator of the variance of the distribution
the samples are taken from, i.e.,
\(\begin{align}
\mathbb{E}[S^2] &amp;amp; = \frac1{n-1} \sum_{i=1}^n \left( \mathbb{E}[X_i^2] - \frac2n
\sum_{j=1}^n \mathbb{E}[X_i X_j] + \mathbb{E}[\bar{X}^2] \right) \notag\\
&amp;amp; = \frac1{n-1} \sum_{i=1}^n \left( \mu^2 + \sigma^2 - \frac2n (\sigma^2 + n
\mu^2 ) + \frac1{n^2} (n(\mu^2+\sigma^2) + (n^2-n)\mu^2) \right) \notag\\
\mathbb{E}[S^2] &amp;amp; = \sigma^2
\end{align}\)
However, because of &lt;a href=&quot;https://en.wikipedia.org/wiki/Jensen%27s_inequality#Measure-theoretic_and_probabilistic_form&quot;&gt;Jensen’s
inequality&lt;/a&gt;,
\(\sqrt{S^2}\) is a biased estimator of the standard deviation of the distribution
we sampled from. Indeed, square root being strictly concave over its domain, we
have that \(\mathbb{E}[\sqrt{S^2}] &amp;lt; \sqrt{\mathbb{E}[S^2]} = \sigma\) (unless
$\sigma=0$).&lt;/p&gt;

&lt;p&gt;On the other hand, because square root is a continuous function over
$\mathbb{R}^+$, if $S^2$ is a consistent estimator (converges in probability),
$\sqrt{S^2}$ is also a consistent estimator (i.e., the bias disappears as the
number of samples grow). Using Chebychev’s inequality, we see that
\[ \mathbb{P}[|S_n^2 - \sigma^2| \geq \varepsilon] \leq
\frac{Var[S_n^2]}{\varepsilon^2}, \notag \]
that is $S^2_n$ is consistent if $Var[S_n^2]$ goes to zero as the number of
samples increase.&lt;/p&gt;

&lt;p&gt;Without a proof, one can show that with $\theta$ the fourth moment of the
underlying distribution, we have
\[ Var[S^2] = \frac1n (\theta - \frac{n-3}{n-1} \sigma^4) \]&lt;/p&gt;

&lt;h4 id=&quot;hypothesis-testing&quot;&gt;Hypothesis testing&lt;/h4&gt;
&lt;p&gt;A common test for a random sample is to test the value of its mean.
To do so, we can use the statistic
\[ \frac{\bar{X} - \mu}{\sigma / \sqrt{n}} \]
where $\mu$ is the value of the statistic we test for.
For hypothesis testing, we need to know the distribution of that statistic. In
the general case, we can only conclude asymptotically as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Central_limit_theorem&quot;&gt;central limit
theorem&lt;/a&gt; states that if the
underlying distribution has finite variance, 
\[ \frac{\bar{X} - \mu}{\sigma / \sqrt{n}} \rightarrow \mathcal{N}(0,1) \]
In practice, $\sigma$ is rarely known, and we want to replace it with $S$. We
know that $S$ is a consistent estimator of $\sigma$, and we can show that $\sigma /
S \rightarrow 1$. 
Then by &lt;a href=&quot;https://en.wikipedia.org/wiki/Slutsky%27s_theorem&quot;&gt;Slutsky’s theorem&lt;/a&gt;,
\[ \frac{\bar{X} - \mu}{\sigma / \sqrt{n}} \frac{\sigma}{S} \rightarrow \mathcal{N}(0,1) \]&lt;/p&gt;

&lt;p&gt;However this result is only valid asymptotically. Often in practice, people
assume the asymptotic regime is valid, and use a standard normal distribution
for that statistic; but there is no general rule to know when that approximation
is valid.&lt;/p&gt;

&lt;p&gt;On the other hand, if the random samples come from a &lt;em&gt;normal distribution&lt;/em&gt;
$\mathcal{N}(\mu,\sigma^2)$, we can say something more general. In that case,
\[ \frac{\bar{X} - \mu}{\sigma / \sqrt{n}} \sim \mathcal{N}(0,1) \]
and
\[ \frac{\bar{X} - \mu}{S / \sqrt{n}} \sim t_{n-1} \]
a Student’s t-distribution with $n-1$ degrees of freedoms.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Delta method and convergence of random variables</title>
   <link href="https://bcrestel.github.io/2018/05/04/deltameth/"/>
   <updated>2018-05-04T00:00:00-04:00</updated>
   <id>https://bcrestel.github.io/2018/05/04/deltameth</id>
   <content type="html">&lt;h4 id=&quot;delta-method&quot;&gt;Delta method&lt;/h4&gt;
&lt;p&gt;The Delta method can be used to approximate the variance of a function of a random
variable that converges to a normal random variable. 
Assuming \(\sqrt{n} (Y_n - \theta) \rightarrow
\mathcal{N}(0,\sigma^2)\) (distribution) with $\theta \in
\mathbb{R}$, then for a given function $g$ (with $g’(\theta) \neq 0$), we have
\[ \sqrt{n} (g(Y_n) - g(\theta)) \rightarrow \mathcal{N}(0, \sigma^2
[g’(\theta)]^2) \text{ (distribution)} \]&lt;/p&gt;

&lt;h4 id=&quot;convergence-in-distribution-and-convergence-in-probability&quot;&gt;Convergence in distribution and convergence in probability&lt;/h4&gt;
&lt;p&gt;In the proof of the Delta method, we use the result that 
\[ \sqrt{n} (Y_n - \theta) \rightarrow
\mathcal{N}(0,\sigma^2) \text{ (distribution)} \Rightarrow
Y_n \rightarrow \theta \text{ (probability),} \]
 i.e., for any $\varepsilon &amp;gt; 0$,
$\mathbb{P}[|Y_n-\theta| \geq \varepsilon] \rightarrow 0$  as $n$ grows.
Now, let us introduce $X_n = \sqrt{n}(Y_n - \theta)$. Then
\[ Y_n - \theta = \frac{1}{\sqrt{n}} X_n \]
$X_n$ converges to $\mathcal{N}(0,\sigma^2)$ in distribution and $1/\sqrt{n}$
converges to zero. 
Slutsky’s theorem tells us that
\[ X_n \rightarrow X \text{ (distr) and } Y_n \rightarrow a \in \mathbb{R}
\text{ (prob)} 
\Rightarrow X_n Y_n \rightarrow aX \text{ (distr)} \]
Therefore $Y_n - \theta \rightarrow 0$ (distr).&lt;/p&gt;

&lt;p&gt;Moreover, 
\[ X_n \rightarrow a \in \mathbb{R} \text{ (distr)}
\Rightarrow X_n \rightarrow a \text{ (prob)} \]
Indeed, $\mathbb{P}[|X_n-a| \geq \varepsilon] = 1-F_n(a+\varepsilon) +
F_n(a-\varepsilon)$. Convergence to a constant in distribution means $F_n(x)$
converges to the Heaviside function $H(x-a)$. Therefore $F_n(a+\varepsilon)
\rightarrow 1$ and $F_n(a-\varepsilon) \rightarrow 0$.&lt;/p&gt;

&lt;p&gt;Going back to our original proof, we have $Y_n - \theta \rightarrow 0$ (prob),
or equivalently $Y_n \rightarrow \theta$ (prob).&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Correlation, covariance, and standard deviation</title>
   <link href="https://bcrestel.github.io/2018/05/02/correlation/"/>
   <updated>2018-05-02T00:00:00-04:00</updated>
   <id>https://bcrestel.github.io/2018/05/02/correlation</id>
   <content type="html">&lt;p&gt;I want to illustrate how correlation and covariance relates to the standard
deviation of the random variables they involve.&lt;/p&gt;

&lt;p&gt;Let’s introduce two random variables $X_i$, $i=1,2$, and the centered random
variables $Y_i = X_i - \mathbb{E}[X_i]$. Then we have 
\(\mathbb{E}[Y_i] = 0\), \(Var[Y_i] = Var(X_i)\), \(Cov(Y_1, Y_2) =
Cov(X_1,X_2)\).
We then introduce two new random variables, $\tilde{X}_i$, with the same
expectation as $X_i$, but with a re-scaled standard deviation,&lt;/p&gt;

\[\begin{align}
\tilde{X}_i &amp;amp; = \mathbb{E}[X_i] + a_i Y_i \notag \\
 &amp;amp; = \mathbb{E}[X_i] + a_i (X_i - \mathbb{E}[X_i]) 
\end{align}\]

&lt;p&gt;with $a_i \in \mathbb{R}$. We can immediately verify that
$\mathbb{E}[\tilde{X}_i] = \mathbb{E}[X_i]$ and $Var[\tilde{X}_i] = a_i^2
Var[X_i]$.&lt;/p&gt;

&lt;p&gt;Then, the covariance of $\tilde{X}_1$ and $\tilde{X}_2$ gives&lt;/p&gt;

\[\begin{align}
Cov(\tilde{X}_1,\tilde{X}_2) &amp;amp; =
\mathbb{E}[(\tilde{X}_1-\mathbb{E}[\tilde{X}_1])(\tilde{X}_2-\mathbb{E}[\tilde{X}_2])]
\notag
\\ 
&amp;amp; = a_1a_2 \mathbb{E}[Y_1 Y_2] \notag \\
&amp;amp; = a_1a_2 Cov(X_1,X_2)
\end{align}\]

&lt;p&gt;Whereas the correlation between $\tilde{X}_1$ and $\tilde{X}_2$ is given by&lt;/p&gt;

\[\begin{align}
corr(\tilde{X}_1,\tilde{X}_2) &amp;amp; =
\frac{Cov(\tilde{X}_1,\tilde{X}_2)}{\sqrt{Var[\tilde{X}_1] Var[\tilde{X}_2]}}
\notag \\
&amp;amp; = \frac{Cov({X}_1,{X}_2)}{\sqrt{Var[{X}_1] Var[{X}_2]}} \notag \\
&amp;amp; = 
corr({X}_1,{X}_2)
\end{align}\]

&lt;p&gt;In words, the covariance scales with the standard deviation of each random
variable, whereas the correlation is oblivious to the standard deviations.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Playing with the Netflix dataset</title>
   <link href="https://bcrestel.github.io/2018/02/15/netflix-dataset/"/>
   <updated>2018-02-15T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2018/02/15/netflix-dataset</id>
   <content type="html">&lt;p&gt;I downloaded the Netflix dataset from
&lt;a href=&quot;https://www.kaggle.com/netflix-inc/netflix-prize-data&quot;&gt;Kaggle&lt;/a&gt;. The Netflix
dataset is a list of movie ratings entered by different customers. It was
provided by Netflix for the &lt;a href=&quot;https://en.wikipedia.org/wiki/Netflix_Prize&quot;&gt;Netflix
Prize&lt;/a&gt; competition.
The goal of the competition was to predict missing movie ratings.&lt;/p&gt;

&lt;p&gt;So far, I have been
&lt;a href=&quot;https://github.com/bcrestel/ML/datasets/netflix/read_netflix.py&quot;&gt;playing&lt;/a&gt; with
the data to format them in an interesting way. My approach is&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;create column for movieID by copying custID and removing all entries not
finishing by ‘:’, then extend movieID ‘ffill’.&lt;/li&gt;
  &lt;li&gt;create sparse matrix with movie ratings for each customer&lt;/li&gt;
  &lt;li&gt;create SparseDataFrame from sparse matrix&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;An interesting question is how do you reconcile ratings from different
customers. One approach is to normalize the ratings for each customer, by
substracting the mean rating and dividing by the standard deviation of the
ratings for that customer.
Also, as I don’t want to have to deal with pathological cases, I am going to
remove all customers with a single rating.&lt;/p&gt;

&lt;p&gt;Unfortunately, the SparseDateFrame object appears to be extremely slow for any
sort of operation (subtract, mean,…). I ended making all operations prior to
the transformation to sparse dataframe.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Terminology and notations in supervised learning</title>
   <link href="https://bcrestel.github.io/2018/02/04/defHTF/"/>
   <updated>2018-02-04T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2018/02/04/defHTF</id>
   <content type="html">&lt;h2 id=&quot;general&quot;&gt;General&lt;/h2&gt;
&lt;p&gt;The goal of supervised learning is to use a set of measures, called inputs, to
predict some outputs. Inputs and outputs are sometimes also called&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;inputs&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;outputs&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;predictors&lt;/td&gt;
      &lt;td&gt;responses&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;independent variables&lt;/td&gt;
      &lt;td&gt;dependent variables&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;features&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;categories-of-outputs&quot;&gt;Categories of outputs&lt;/h2&gt;
&lt;p&gt;Outputs can be of two types,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;quantitative&lt;/strong&gt;: outputs are numbers, with an ordering. Prediction with
 quantitative outputs is called &lt;strong&gt;regression&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;qualitative&lt;/strong&gt;/categorical/discrete: outputs are sets, with not necessarily
an order relationship. Prediction with qualitative outputs is called
&lt;strong&gt;classification&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;notations&quot;&gt;Notations&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Input variables are denoted by $X=\{X_j\}_j$ (vector).&lt;/li&gt;
  &lt;li&gt;Outputs are denoted by $Y$ (&lt;strong&gt;quantitative&lt;/strong&gt;) or $G$ (&lt;strong&gt;qualitative&lt;/strong&gt;).&lt;/li&gt;
  &lt;li&gt;Predicted or estimated quantities are marked with a hat.&lt;/li&gt;
  &lt;li&gt;$X,Y,G$ are generic variables. The actual observed values are denoted in
 lowercase; the $i^\text{th}$ observed input value is $x_i$ (potentially a
vector).&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Cookbook for Pandas</title>
   <link href="https://bcrestel.github.io/2018/01/23/pandas/"/>
   <updated>2018-01-23T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2018/01/23/pandas</id>
   <content type="html">&lt;p&gt;I end up looking for how to do things in pandas way too often. So I’ll summarize
some basic features that I need.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;datetime
    &lt;ul&gt;
      &lt;li&gt;converting to datetime format
        &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;df[&apos;time&apos;] = pd.to_datetime(df[&apos;time&apos;])
&lt;/code&gt;&lt;/pre&gt;
        &lt;p&gt;It can also be used on the index&lt;/p&gt;
        &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;df.index = pd.to_datetime(df.index)
&lt;/code&gt;&lt;/pre&gt;
      &lt;/li&gt;
      &lt;li&gt;Shift a timeindex by constant time
        &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;df.index = df.index - datetime.timedelta(hours=6)
&lt;/code&gt;&lt;/pre&gt;
      &lt;/li&gt;
      &lt;li&gt;Change sampling frequency; for instance, going from 1min data to 20min data
        &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;df.set_index(&apos;time&apos;, inplace=True)
df20 = df[&apos;open&apos;].resample(&apos;20min&apos;).first()
df20[&apos;close&apos;] = df[&apos;close&apos;].resample(&apos;20min&apos;).last()
df20[&apos;mean&apos;] = 0.5*(df[&apos;open&apos;].resample(&apos;20min&apos;).mean() + df[&apos;close&apos;].resample(&apos;20min&apos;).mean() )
df20[&apos;low&apos;] = df[&apos;low&apos;].resample(&apos;20min&apos;).min()
df20[&apos;high&apos;] = df[&apos;high&apos;].resample(&apos;20min&apos;).max()
&lt;/code&gt;&lt;/pre&gt;
        &lt;p&gt;Many different frequencies can be used with &lt;code&gt;resample&lt;/code&gt;, the main ones being&lt;/p&gt;
        &lt;pre&gt;&lt;code&gt;M   monthly
W   weekly
D   daily
H   hourly
&lt;/code&gt;&lt;/pre&gt;
        &lt;p&gt;A nice reference is available
&lt;a href=&quot;https://stackoverflow.com/questions/17001389/pandas-resample-documentation&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Normalize data (e.g, ‘volume’) by their daily maximum; assuming the index is every minute,
    &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;df[&apos;date&apos;] = df.index.date
df.volume / df.groupby(&apos;date&apos;).volume.transform(np.max)
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;Plot results (‘res’) of a groupby with timeindex, all on top of each other. The trick
here is to turn off the use of the index when plotting,
    &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;df.groupby(&apos;date&apos;).res.plot(use_index=False)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;take negation of a boolean index selection: &lt;code&gt;~&lt;/code&gt;.
For instance, the following example sets all entries in the ‘movie’ column to
NaN where they do not end with a colon ‘:’,
    &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;df.loc[~df[&apos;movie&apos;].str.endswith(&apos;:&apos;),&apos;movie&apos;] = np.NaN
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 
 <entry>
   <title>Tags page</title>
   <link href="https://bcrestel.github.io/2018/01/21/tags/"/>
   <updated>2018-01-21T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2018/01/21/tags</id>
   <content type="html">&lt;p&gt;I had some issue formatting a &lt;a href=&quot;https://bcrestel.github.io/tags&quot;&gt;page&lt;/a&gt; containing all tags used in the blog. All
the coding had already been done in a &lt;a href=&quot;/2018/01/11/adding-tag/&quot;&gt;blog post&lt;/a&gt;
that I followed. However, instead of having all tags written next to each other
on the same line, they were placed on top of each other, which would take a lot
more screen space. I didn’t really dig to the bottom of it, but it turns out
this can be fixed by defining the tag page as an html file, instead of a
markdown file (md).&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Gaussian measures in high dimension</title>
   <link href="https://bcrestel.github.io/2018/01/20/concentration_measure/"/>
   <updated>2018-01-20T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2018/01/20/concentration_measure</id>
   <content type="html">&lt;p&gt;High dimensional spaces can be counter-intuitive. &lt;a href=&quot;https://www.johndcook.com/blog/2011/09/01/multivariate-normal-shell/&quot;&gt;For instance&lt;/a&gt;, 
most of the mass
of a sphere in high dimension is concentrated in the outside region of the sphere.
To see this, remember that the volume of a
&lt;a href=&quot;https://en.wikipedia.org/wiki/N-sphere#Volume_and_surface_area&quot;&gt;n-sphere&lt;/a&gt; is
given by
\[ V_n(r) = \frac{\pi^{n/2}}{\Gamma(n/2+1)} r^n. \]
With this formula, we find that the percentage of mass of a unit n-sphere found
in-between two spheres of radii 0.99 and 1.00 (i.e., a shell of thickness 0.01)
is equal to $10\%$ in $\mathcal{R}^{10}$, $39\%$ in $\mathcal{R}^{50}$, and $63\%$
in $\mathcal{R}^{100}$.&lt;/p&gt;

&lt;p&gt;This example also helps explain why samples from a &lt;a href=&quot;https://en.wikipedia.org/wiki/Multivariate_normal_distribution&quot;&gt;multivariate Gaussian&lt;/a&gt;
in $\mathcal{R}^n$ tend to accumulate on a sphere of radius $\sqrt{n}$ when $n$
grows large.
The pdf of a Gaussian decreases exponentially fast as distance from the mean
grows, but the mass of the space becomes more and more concentrate. The result
is that the mode of a multivariate standard normal is at a distance
$\sqrt{n-1}$.&lt;/p&gt;

&lt;p&gt;For a multivariate standard normal ($\mu=0$, $\Sigma=I$) in $\mathcal{R}^n$, the $l_2-$norm of the
samples has a &lt;a href=&quot;https://en.wikipedia.org/wiki/Chi_distribution&quot;&gt;chi distribution&lt;/a&gt;
with parameter $n$, $\chi_n$; it has mode $\sqrt{n-1}$, and a mean that is
approximately equal to $\sqrt{n}$ as $n$ grows. This means the variance, given
by $n$ minus the square of the mean, is approximately equal to zero. 
Empirically, with 1,000 samples, I found the standard deviation of the samples
to remain around 0.7 for dimensions $n=$1 to 100,000. A more rigorous
explanation that almost all samples remain in a band of constant thickness
can be found
&lt;a href=&quot;https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/chap1-high-dim-space.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;side-note&quot;&gt;Side note&lt;/h3&gt;
&lt;p&gt;As a ‘fun’ little exercise, we can re-derive the distribution of the $l_2-$norm
of a multivariate standard normal. Assume $X \sim \mathcal{N}(0,I)$, then
\[ \mathcal{P}(|X| \leq \alpha) = \iint_{|x| \leq \alpha^2} (2\pi)^{-n/2}
\exp(-|x|^2/2) dx \]
We turn to spherical coordinates,
\[ x_1  = r \cos \theta_1 \]
\[ x_2  = r \sin \theta_1 \cos \theta_2 \]
\[ \vdots \]
\[ x_i = r \sin \theta_1 \ldots \sin \theta_{i-1} \cos \theta_i \]
\[ \vdots \]
\[ x_n = r \sin \theta_1 \ldots \ldots \sin \theta_{n-2} \sin \theta_n \]
And the Jacobian of the transformation is given 
&lt;a href=&quot;https://en.wikipedia.org/wiki/N-sphere#Spherical_coordinates&quot;&gt;by&lt;/a&gt;
\[ \left| \frac{\partial x}{\partial \theta} \right| = r^{n-1} \sin^{n-2}
\theta_1 \ldots \sin \theta_{n-2} \]
However the exact form of the Jacobian does not matter. Since the pdf of the
multivariate standard normal only depends on the radius, $\mathcal{P}(|X|\leq
\alpha)$ will be given by
\[ \mathcal{P}(|X|\leq \alpha) = S_n (2\pi)^{-n/2} \int_0^{\alpha} r^{n-1}
e^{-r^2/2} dr , \]
where $S_n$ is the part of the volume of unit n-sphere that does not depend on
the radius, i.e., all the integrals for the $\theta_i$’s. Again, we use the
formula for the volume of a unit n-sphere to get
\[ \frac{\pi^{n/2}}{\Gamma(n/2+1)} = S_n \int_0^1 r^{n-1} dr \]
Using the definition of the 
&lt;a href=&quot;https://en.wikipedia.org/wiki/Gamma_function&quot;&gt;gamma function&lt;/a&gt;
we re-write $S_n$ as
\[ S_n = \frac{n \pi^{n/2}}{n/2 \, \Gamma(n/2)} \]
and finally get
\[ \mathcal{P}(|X|\leq \alpha) = 
\frac{1}{2^{n/2-1} \Gamma(n/2)} \int_0^{\alpha} r^{n-1}
e^{-r^2/2} dr , \]
which corresponds to a chi distribution.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Hypothesis testing with linear regression</title>
   <link href="https://bcrestel.github.io/2018/01/15/hyp-testing/"/>
   <updated>2018-01-15T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2018/01/15/hyp-testing</id>
   <content type="html">
</content>
 </entry>
 
 <entry>
   <title>AIC in linear regression</title>
   <link href="https://bcrestel.github.io/2018/01/12/model-selection/"/>
   <updated>2018-01-12T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2018/01/12/model-selection</id>
   <content type="html">&lt;p&gt;Model selection is a central problem of statistical inference, whether you have to choose the independent variables to include in your linear regression, or compare between completely unrelated techniques.
In the context of linear regression, using $R^2$ to compare models would lead to over-fitting, and even the adjusted $R^2$ does not necessarily penalize larger models sufficiently.
An army of other methods exist (see &lt;a href=&quot;http://www.modelselection.org/model-selection.pdf&quot;&gt;here&lt;/a&gt;), among which 3 really stands out:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;cross-validation: it comes in many different flavours, but at its core, the idea is to leave out some of your training data to later test your model on. You can repeat the procedure leaving different distinct datasets, or not.
One obvious disadvantage of cross-validation is its potential computational cost, especially if you want to exhaust all possible splitting of the parameter space. 
However, in the case of linear regression, the cross-validation error can be
computed directly after solving the regression. 
Also, cross-validation is more restricted when applied to time-series data.&lt;/li&gt;
  &lt;li&gt;Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC): these two criteria do explicitly penalize models with large number of parameters.
    &lt;ul&gt;
      &lt;li&gt;AIC: It is generally given as
\[ AIC(\beta) = 2k - 2\ln(\mathcal{L}) \]
where $k$ is the number of parameters and $\mathcal{L}$ is the likelihood at the MLE.&lt;/li&gt;
      &lt;li&gt;BIC: the BIC criterion imposes a stronger penalty on the number of parameters of the model,
\[ BIC(\beta) = \ln(n) k - 2\ln(\mathcal{L}) \]
These two criteria are extremely popular; one disadvantage is their reliance on the Likelihood function which requires a specific distributional assumption.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Just as an exercise, let us derive the AIC in the case of linear regression. For the linear regression, assuming Gaussian noise $\varepsilon \sim \mathcal{N}(0, \Sigma)$, we have $ Y = X \cdotp \beta + \beta $.
The log-likelihood function is then given by
\[ \ln(\mathcal{L}) = -\frac12 \left( \ln( \det(2\pi \Sigma) ) \right) - \frac12 (Y - X \cdotp \beta)^T \Sigma^{-1} (Y - X \cdotp \beta) \]
In the case of iid normal noise, i.e., $\Sigma = \sigma^2 I$, we have
\[ \ln(\mathcal{L}) = -\frac12 \left( n \ln( 2\pi \sigma^2 ) \right) - \frac1{2\sigma^2} e^T e \]
where $e = Y - X \cdotp \beta$ and $n$ is the number of observations. 
Let us denote by $\hat{\beta}$ the MLE (and $\hat{e}=Y-X \cdotp \hat{\beta}$)
As we typically do not know the value of $\sigma^2$, we instead estimate it with the standard error $s^2 = \frac{e^Te}{n}$. This gives
\[ AIC = 2k+n + n \ln(2\pi) + n \ln \left( \frac{\hat{e}^T\hat{e}}{n} \right) = n \left[ 1 + 2\ln(2\pi) + \frac{2k}n + \ln \left( \frac{\hat{e}^T\hat{e}}{n} \right) \right]. \]
As a comparision, in Greene (2005) they use $2k/n + \ln(e^Te/n)$, which is, up to constant terms that do not vary in-between models, the same.
An additional reference for model selection in regression is &lt;a href=&quot;http://statweb.stanford.edu/~jtaylo/courses/stats203/notes/selection.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In a future post, I’d like to talk about &lt;a href=&quot;https://arxiv.org/pdf/1709.08221.pdf&quot;&gt;model averaging&lt;/a&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Fix some incorrect links</title>
   <link href="https://bcrestel.github.io/2018/01/11/fix-url/"/>
   <updated>2018-01-11T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2018/01/11/fix-url</id>
   <content type="html">&lt;p&gt;I had some issues with the links pointing to the main page in the sidebar. They would always point to the current page. I fixed this by changing &lt;code&gt;site.baseurl&lt;/code&gt; to &lt;code&gt;site.url&lt;/code&gt; in the definition of these two links (title and Home) in the file &lt;code&gt;_includes/sidebar.html&lt;/code&gt;.&lt;br /&gt;
Also, as I was looking for a solution to that problem, I found that all url’s in Jekyll should really be defined as &lt;code&gt;{ { site.baseurl } }{ { post.url } }&lt;/code&gt;, and the variable &lt;code&gt;baseurl&lt;/code&gt; should be left empty, instead of &lt;code&gt;/&lt;/code&gt;. I found &lt;a href=&quot;http://downtothewire.io/2015/08/15/configuring-jekyll-for-user-and-project-github-pages/&quot;&gt;this page&lt;/a&gt; to be pretty useful.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Adding tags on Github</title>
   <link href="https://bcrestel.github.io/2018/01/11/adding-tag/"/>
   <updated>2018-01-11T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2018/01/11/adding-tag</id>
   <content type="html">&lt;p&gt;As described in a previous post, I am following the instructions posted on &lt;a href=&quot;http://longqian.me/2017/02/09/github-jekyll-tag/&quot;&gt;that
blog post&lt;/a&gt;.&lt;br /&gt;
First, we add the &lt;code&gt;collecttags.html&lt;/code&gt; file on &lt;code&gt;_includes&lt;/code&gt;.&lt;br /&gt;
Next, we add that file in the header, &lt;code&gt;head.html&lt;/code&gt;.&lt;br /&gt;
After that step, we can display the tags in the front matter of each post. In
addition, a link was created to a webpage listing all posts using that tag.
Next, we need to define those pages. In the blog post linked above, the author
provides a Python script to generate these pages automatically. The addition of
the cloud of tags work just as described.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Getting started</title>
   <link href="https://bcrestel.github.io/2018/01/09/starter/"/>
   <updated>2018-01-09T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2018/01/09/starter</id>
   <content type="html">&lt;p&gt;For this blog, I am using the &lt;a href=&quot;https://github.com/poole/hyde&quot;&gt;poole/hyde&lt;/a&gt;
example. Unfortunately, this repo does not seem to be maintained anymore, and
the list of pull requests has grown steadily since 2015. The main modifications I
had to make to run this blog are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;in &lt;code&gt;_config.yml&lt;/code&gt;:
    &lt;ul&gt;
      &lt;li&gt;change the markdown to &lt;code&gt;kramdown&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;remove &lt;code&gt;relative_permalinks: true&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;add the gems &lt;code&gt;jekyll-paginate, jekyll-seo-tag, jekyll-sitemap, jekyll-gist&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;in the &lt;code&gt;_includes/head.html&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;replace the ` { { site.baseurl }} ` with a simple slash&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I also added Mathjax. I found online different approaches to do that. What seems
to work was to add the Mathjax script in the header of the html pages. Building
on the original organization, this meant adding the script to the file
&lt;code&gt;_includes/head.html&lt;/code&gt;. Following this discussion on
&lt;a href=&quot;https://github.com/github/pages-gem/issues/307&quot;&gt;github&lt;/a&gt;, I added the following
lines,&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-html&quot;&gt;&amp;lt;!-- Mathjax --&amp;gt;
&amp;lt;script type=&quot;text/x-mathjax-config&quot;&amp;gt; MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: &quot;all&quot; } } }); &amp;lt;/script&amp;gt;
&amp;lt;script type=&quot;text/x-mathjax-config&quot;&amp;gt;
 MathJax.Hub.Config({
   tex2jax: {
     inlineMath: [ [&apos;$&apos;,&apos;$&apos;], [&quot;\\(&quot;,&quot;\\)&quot;] ],
     displayMath: [ [&apos;$$&apos;,&apos;$$&apos;], [&quot;\\[&quot;,&quot;\\]&quot;] ],
     processEscapes: true
   }
 });
&amp;lt;/script&amp;gt;
&amp;lt;script
src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s check that it works inline $x^2 + y^2 = z^2$. It also works in displayMath
mode,
\[ \mathbb{P}(A|B) = \frac{\mathbb{P}(B|A) \mathbb{P}(A)}{\mathbb{P}(B)} \]&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>A first post</title>
   <link href="https://bcrestel.github.io/2018/01/08/first-post/"/>
   <updated>2018-01-08T00:00:00-05:00</updated>
   <id>https://bcrestel.github.io/2018/01/08/first-post</id>
   <content type="html">&lt;p&gt;If you want to serve this blog on your local machine, in the main directory,
type&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;jekyll serve
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are a few features that I would like to add to this blog.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;I would like to manage tags. This is handled in Jekyll with the gem
jekyll-tagging, but unfortunately this gem is not available on GitHub. However
it is possible to regenerate the tagging capacity in Jekyll, for instance
by following &lt;a href=&quot;http://longqian.me/2017/02/09/github-jekyll-tag/&quot;&gt;that link&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;I would also need to have a summary page listing all posts. This is described
in &lt;a href=&quot;http://joshualande.com/jekyll-github-pages-poole&quot;&gt;this&lt;/a&gt; blog post.&lt;/li&gt;
  &lt;li&gt;And of course, I will need support for
&lt;a href=&quot;https://github.com/github/pages-gem/issues/307&quot;&gt;MathJax&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;A few useful references for markdowns syntax:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A &lt;a href=&quot;https://gist.github.com/roachhd/779fa77e9b90fe945b0c&quot;&gt;cheat-sheet for markdowns&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;And another, &lt;a href=&quot;https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet&quot;&gt;better one&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
 </entry>
 

</feed>
