I"€<p>Notations are defined in that <a href="/2018/11/13/deeplearnDIY">post</a>.</p>

<p>A very popular activation function for classification with a deep net is the
softmax, which is defined as</p>

\[{\bf a}^l = [ \frac{\exp(a^{l-1}_1)}{\sum_{i=1}^{n_{l-1}} \exp(a^{l-1}_i)},
\dots,
\frac{\exp(a^{l-1}_{n_{l-1}})}{\sum_{i=1}^{n_{l-1}} \exp(a^{l-1}_i)} ]^T\]

<p>That is ${\bf a}^l = F^l({\bf a}^{l-1})$ with
$F^l(x) = [f_1(x), \dots, f_{n_l}(x)]^T$ where</p>

\[f_i(x) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}\]

<p>That type of activation function is different to what was covered in that
<a href="/2018/11/13/deeplearnDIY">post</a> for 2 reasons: (1) it does not have any
parameters to optimize, and (2) each coordinate depends on all other
coordinates. Letâ€™s see how we back-propagate the gradient in that case.</p>

<p><strong>For the output layer</strong>, which is the most common use-case for a softmax layer,
we need to modify the chain-rule to skip the output layer and start instead at
layer $L-1$,</p>

\[\frac{\partial c}{\partial {\bf b}^{L-1}} = 
\frac{\partial {\bf a}^{L-1}}{\partial {\bf b}^{L-1}}
\frac{\partial {\bf a}^{L}}{\partial {\bf a}^{L-1}}
\frac{\partial c}{\partial {\bf a}^{L}}\]

<p>The only term that was not covered previously is \(\frac{\partial {\bf
a}^{L}}{\partial {\bf a}^{L-1}}\) which is simply the gradient of $F^L$.</p>

<p><strong>For a hidden layer</strong>, which is definitely not the most common case for a
softmax activation function but could the case of another type of function, we
have</p>

\[\begin{align*}
\frac{\partial c}{\partial {\bf b}^{l-1}} &amp; = 
\frac{\partial {\bf a}^{l-1}}{\partial {\bf b}^{l-1}}
\frac{\partial {\bf a}^{l}}{\partial {\bf a}^{l-1}}
\frac{\partial {\bf z}^{l+1}}{\partial {\bf a}^{l}}
\frac{\partial c}{\partial {\bf z}^{l+1}} \\
&amp; = \frac{\partial {\bf a}^{l-1}}{\partial {\bf b}^{l-1}}
\frac{\partial {\bf a}^{l}}{\partial {\bf a}^{l-1}}
\frac{\partial {\bf z}^{l+1}}{\partial {\bf a}^{l}}
\frac{\partial c}{\partial {\bf b}^{l+1}} \\
\end{align*}\]

<p>Again, the only term we didnâ€™t before is \(\frac{\partial {\bf a}^{l}}{\partial
{\bf a}^{l-1}}\), which is the gradient of $F^l$.</p>
:ET