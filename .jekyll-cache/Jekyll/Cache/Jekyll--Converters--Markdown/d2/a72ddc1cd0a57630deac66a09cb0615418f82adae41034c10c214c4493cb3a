I"þ<h2 id="notes">Notes</h2>

<p>In optimization, you need to take derivatives. To do that in $\mathbb{R}^n$, you
need matrix calculus. The objective of this note is to summarize the important
definitions and conventions, explain them whenever possible, and show a few
examples.</p>

<h4 id="frechet-and-gateaux-derivatives">Frechet and Gateaux derivatives</h4>

<p>Letâ€™s define a function $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$.  That
function is 
<a href="https://en.wikipedia.org/wiki/Fr%C3%A9chet_derivative">Frechet differentiable</a> 
at $x$ if
there exists a bounded (necessarily the case for an operator between finite dimensional
spaces) linear operator<br />
$Df(x): \mathbb{R}^n \rightarrow \mathbb{R}^m$ 
such that
\[ \frac{| f(x+h) - f(x) - Df(x)h |}{| h |} \rightarrow 0 \] 
as $|h| \rightarrow 0$.</p>

<p>A somehow weaker definition of differentiability is the <a href="https://en.wikipedia.org/wiki/G%C3%A2teaux_derivative">Gateaux
differentiability</a>.  A
function $f$ is Gateaux differentiable at $x$ if, for any $v
\in \mathbb{R}^n$ the following limit exists
\[ lim_{\varepsilon \rightarrow 0} \frac{f(x + \varepsilon v) - f(x)} \varepsilon . \]
If that limit exists, we can calculate it as
\[ lim_{\varepsilon \rightarrow 0} \frac{f(x + \varepsilon v) -
f(x)}\varepsilon = \frac{d}{d\varepsilon} f(x + \varepsilon v)
|_{\varepsilon=0} . \]</p>

<p>If $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ 
is Frechet differentiable, it is necessarily Gateaux differentiable, and
\(\frac{d}{d \varepsilon} f(x + \varepsilon v) |_{\varepsilon=0} = Df(x) v\).</p>

<h4 id="gradient-of-a-functional">Gradient of a functional</h4>

<p>Letâ€™s call $H = \mathbb{R}^n$, which is a Hilbert space with inner product
$\langle \cdotp, \cdotp \rangle$.  For a functional $f: \mathbb{R}^n \rightarrow
\mathbb{R}$, the derivative $Df(x)$ is, by definition, an element of the dual
space $H^*$.
Applying <a href="https://en.wikipedia.org/wiki/Riesz_representation_theorem">Riesz representation
theorem</a>, we know
there is an element $g_x \in H$ such that for any $v \in H$,
\(DF(x) v = \langle g_x, v \rangle\). That element $g_x$ is the gradient of
the functional $f$. This clearly defines the gradient of a functional, 
without having to agree on notations or conventions.</p>

<h4 id="general-case">General case</h4>

<p>What can we do for a function $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$?
First, we canâ€™t apply Riesz representation theorem. Also, it is not clear how we
optimize that function $f$. Weâ€™d need to define a <a href="https://en.wikipedia.org/wiki/Total_order#Orders_on_the_Cartesian_product_of_totally_ordered_sets">total
order</a>
on $\mathbb{R}^m$ that would coincide with the objective of optimization.  For
that reason, I see the definition of a gradient in that case as more of a
convention.  There are really two conventions, which are a transpose of each
other (see <a href="https://en.wikipedia.org/wiki/Matrix_calculus#Layout_conventions">layout
conventions</a>),
and I adopt the convention used in Nocedal &amp; Wrightâ€™s Numerical Optimization
textbook (section A.2 Derivatives).
Linear maps between two finite-dimensional spaces can
all be described by the action of a matrix.
Nocedal &amp; Wright call the Jacobian the matrix 
$J(x) \in \mathbb{R}^{m \times n}$ 
that verifies, for any $v \in \mathbb{R}^n$, $Df(x)v = J(x) \cdotp v$.
The gradient is defined to be the transpose,</p>

\[\begin{align} 
J(x) &amp; = \left[ \frac{\partial f_i}{\partial x_j} \right]_{ij} 
\in \mathbb{R}^{m \times n} \\
\nabla f(x) &amp; = \left[ \frac{\partial f_j}{\partial x_i} \right]_{ij} = J(x)^T
= \begin{bmatrix}
\frac{\partial f_1}{\partial x_1} &amp; \dots &amp; \frac{\partial f_m}{\partial x_1} \\
\vdots &amp; &amp; \vdots \\
\frac{\partial f_1}{\partial x_n} &amp; \dots &amp; \frac{\partial f_m}{\partial x_n} 
\end{bmatrix}
\in \mathbb{R}^{n \times m} 
\end{align}\]

<p>One thing to be careful about with that notation, 
the chain-rule, as we know it, applies to the Jacobian, i.e., if $h(x) =
f(g(x))$, then</p>

\[J_h(x) = J_f(g(x)) \cdotp J_g(x)\]

<p>and therefore in terms of the gradient, we get the transpose,</p>

\[\nabla h(x) = \nabla g(x) \cdotp \nabla f(g(x))\]

<p>For instance, letâ€™s assume $f: \mathbb{R}^m \rightarrow \mathbb{R}$ and $g: \mathbb{R}^n
\rightarrow \mathbb{R}^m$. Then, with $y_k = (g(x))_k$, for any $i=1,\dots,n$,</p>

\[\frac{\partial h}{\partial x_i} = \sum_{k=1}^m \frac{\partial f}{\partial
y_k} \frac{\partial y_k}{\partial x_i} 
= \left [\frac{\partial (g(x))_i}{\partial x_i} \right]_i^T \cdotp \nabla f(g(x))\]

<p>Then putting all indices $i$ together (in rows), we get the expression above for the
gradient.</p>

<h4 id="derivative-with-respect-to-a-matrix">Derivative with respect to a matrix</h4>

<p>In that case also, this is just a convenient notation. For a function $f :
\mathbb{R}^{m \times n} \rightarrow \mathbb{R}$, we define</p>

\[\frac{\partial f(M)}{\partial M} = \left[
\frac{\partial f(M)}{\partial m_{ij}} \right]_{ij}\]

<h2 id="examples">Examples</h2>

<h4 id="if-fx--ax--b">If $f(x) = Ax + b$</h4>

<p>We then have $f : \mathbb{R}^n \rightarrow \mathbb{R}^m$.
We can apply the definition of the Gateaux derivative,</p>

\[f(x + \varepsilon v) = f(x) + \varepsilon A v\]

<p>We can conlude directly that</p>

\[\nabla f(x) = A^T\]

<h4 id="if-fx--frac12-xt-q-x-">If $f(x) = \frac12 x^T Q x $</h4>

<p>We then have $f : \mathbb{R}^n \rightarrow \mathbb{R}$.
We can apply the definition of the Gateaux derivative,</p>

\[f(x + \varepsilon v) = f(x) + \frac12 \varepsilon 
\left( x^T Q v + v^T Q x \right) + 
\frac12 \varepsilon^2 v^T Q v\]

<p>We conlude that</p>

\[\nabla f(x) = \frac12 ( Q + Q^T) x\]

<p>In the special case that $Q=Q^T$ (symmetric), we have</p>

\[\nabla f(x) =  Q x\]

<h4 id="if-fx--frac12--axb2">If $f(x) = \frac12 | Ax+b|^2$</h4>

<p>We then have $f : \mathbb{R}^n \rightarrow \mathbb{R}$.
Following the same approach, we get</p>

\[f(x + \varepsilon v) = f(x) + 
\frac12 \varepsilon \left( (Ax+b)^T A v + (Av)^T(Ax+b) \right) + 
\frac12 \varepsilon^2 \|Av\|^2\]

<p>We can conlude that</p>

\[\nabla f(x) = A^T \cdotp (Ax+b)\]

<p>Alternatively, we can use the chain-rule with $g(y) = \frac12 | y |^2$ and
$f(x) = g(Ax + b)$. 
Since $\nabla (Ax+b) = A^T$ and $\nabla g(y) = y$, we have</p>

\[\nabla f(x) = A^T \cdotp (Ax+b)\]

<h4 id="if-fa--axb">If $f(A) = Ax+b$</h4>

<p>In that case, Iâ€™m not sure it helps to talk about a gradient. However we can
still calculate the derivative (e.g., using the formula for the Gateaux
derivative), and we get</p>

\[Df(A) M = M \cdotp x\]

<h4 id="if-fa--frac12--axb2">If $f(A) = \frac12 | Ax+b|^2$</h4>

<p>Here we have $f: \mathbb{R}^{m \times n} \rightarrow \mathbb{R}$.
Itâ€™s tempting to use the chain-rule, but I couldnâ€™t agree with myself on a
logical convention. And I think sadly this is the main conclusion of that small
post: itâ€™s safer to always rely on the entry-wise derivative. In this case,
with $y = Ax + b \in \mathbb{R}^m$, we have</p>

\[\begin{align*}
\frac{\partial f(A)}{\partial a_{ij}} &amp; = 
\sum_k \frac{\partial g(Ax + b)}{\partial y_k} \frac{\partial y_k}{\partial
a_{ij}} \\
&amp; = \left[ \frac{\partial y_1}{\partial a_{ij}}, \dots, \frac{\partial y_m}{\partial
a_{ij}} \right] \cdotp (Ax + b)
\end{align*}\]

<p>Letâ€™s look at the partial derivatives for $y$, using the notation $\delta_{ik} =
1$ if $i=k$ and $0$ otherwise,</p>

\[\frac{\partial y_k}{\partial a_{ij}} = x_j \delta_{ik}\]

<p>Such that</p>

\[\frac{\partial f(A)}{\partial a_{ij}}  = (Ax+b)_i x_j\]

<p>And putting all indices back together,</p>

\[\frac{\partial f(A)}{\partial A}  = (Ax+b) \cdotp x^T\]

:ET